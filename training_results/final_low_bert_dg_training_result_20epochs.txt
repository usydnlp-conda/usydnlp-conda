 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/08/2021 14:23:59 - INFO - data_loader -   *** Example ***
10/08/2021 14:23:59 - INFO - data_loader -   guid: test-1
10/08/2021 14:23:59 - INFO - data_loader -   tokens: [CLS] how dare we go 0 - 2 feeder ##s bronze kids [SEP]
10/08/2021 14:23:59 - INFO - data_loader -   input_ids: 101 2129 8108 2057 2175 1014 1011 1016 21429 2015 4421 4268 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   intent_label: 3 (id = 3)
10/08/2021 14:23:59 - INFO - data_loader -   slot_labels: 0 4 4 5 6 4 0 0 4 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   ner_embeds: [ 0.14663459 -0.05262458 -0.00995038 -0.09029851  0.12263373 -0.14626154
  0.02043098 -0.1690678   0.03294891 -0.05268421 -0.3358649   0.27530029
 -0.07744435 -0.07584086 -0.02695106 -0.26804128  0.02627846 -0.10347202
  0.2013707  -0.01272027  0.16158915 -0.03340857  0.17287622 -0.18740179
  0.26615396  0.09241544 -0.11608424 -0.27987984 -0.28424925 -0.18137644
 -0.22812925  0.0249024   0.30128998  0.13045995  0.03118261  0.16935138
 -0.06230416  0.17561074  0.00367108 -0.07402565  0.04037194 -0.07837126
 -0.1561325   0.20836185  0.14040159  0.11099035  0.17083055 -0.35949063
  0.03112581 -0.18540204  0.04089637 -0.29396915  0.49076098 -0.05263726
 -0.16885227  0.11559261  0.25719538 -0.1559715  -0.06362429 -0.12877826
  0.06794425 -0.01322358  0.26562798  0.08987913  0.29401389 -0.37333494
 -0.01440139  0.30768135 -0.37712052 -0.108851    0.07931803  0.17231582
 -0.44193396  0.03251904  0.0195513  -0.03582918 -0.13726601 -0.28466982
 -0.33915862 -0.0583167   0.01170416  0.32957253  0.08330661  0.28316665
  0.00079225  0.41026247  0.02413866 -0.45413318 -0.1344302  -0.09248621
  0.51263183 -0.13745138 -0.11134322 -0.20985258  0.39278489 -0.1011213
  0.18303782 -0.12043855  0.26268336 -0.0461308   0.30422714  0.07902091
 -0.15427054  0.18763816 -0.00932948  0.02431204  0.30051631 -0.24693149
  0.11975729 -0.0125444   0.13902089 -0.25323379 -0.1082405   0.42633086
  0.19724542 -0.07689937 -0.1337288   0.27917713 -0.0091875   0.25963399
 -0.14592938 -0.05517824  0.0350663   0.20550938  0.11277337 -0.2699483
  0.01591763 -0.00071367  0.07296164  0.02445946  0.14451511 -0.04559916
 -0.17699969 -0.42008832  0.10587756  0.09828436  0.28259453 -0.0146672
 -0.11086253 -0.35100189  0.04879292 -0.0834544   0.16815855 -0.13103473
 -0.02294332  0.36864796  0.14379583  0.44768068 -0.23370998  0.152519
  0.04665052 -0.17252302  0.31976756 -0.16741526 -0.25466728 -0.26076594
  0.00063117 -0.05335071  0.15843306 -0.26611817  0.05734076 -0.08238135
  0.18893163 -0.20564257 -0.16225532 -0.20887086  0.08916163 -0.01871778
  0.12805638  0.26127747 -0.0709104  -0.27082589 -0.1627634   0.13914403
  0.12214889 -0.1563787  -0.24772033 -0.06986653  0.2272698  -0.05595371
 -0.00991442 -0.20372061  0.04004419 -0.04990653  0.43829471  0.11963772
 -0.12598085  0.24185398 -0.04867083 -0.08653479 -0.26793903  0.13082959
 -0.43375427 -0.13357146  0.19298363 -0.22029646  0.38383847 -0.07818928
 -0.0058459  -0.32991308 -0.08181709  0.14099021  0.09491039  0.21827479
 -0.00399596 -0.08103576  0.13853465  0.01428204  0.44172066  0.10182345
  0.14742956 -0.14591892  0.33454806  0.04376031  0.32970932  0.15365525
 -0.22210287 -0.16479632 -0.28103629  0.00301148  0.01037944  0.20718177
 -0.2371673   0.33999005 -0.13203178 -0.26044402 -0.22456646 -0.09428472
  0.12473079 -0.10682054  0.08118179  0.18629588 -0.10320079  0.10400119
 -0.36503249  0.0367523  -0.26162294 -0.13888274 -0.00607623  0.1842383
 -0.33304039  0.17386849 -0.0694743   0.03439113  0.08467512  0.23954627
  0.3646954  -0.02856011 -0.21535675  0.18716544  0.25370413 -0.18916315
  0.1572178  -0.06640452  0.13099778  0.01409951 -0.31194866 -0.14290154
 -0.1712914  -0.32875261 -0.09570646  0.28620249 -0.23103324  0.22911115
  0.02136808 -0.04183525  0.2437249  -0.1248548  -0.05797715  0.476778
 -0.21734898  0.23745687  0.26735842  0.27674222  0.07875728 -0.16760489
 -0.46723711 -0.08130792  0.09524607 -0.19139667  0.13001987  0.25027886
 -0.01569632  0.08894423 -0.10037478  0.12834264 -0.4296048   0.41507876
  0.2195386  -0.00381741 -0.146727    0.23849073  0.10347191  0.17253159
 -0.4031671   0.19611832  0.17422171  0.3443523  -0.18065456  0.04004083
  0.10621159 -0.19717942  0.25161341 -0.0291713   0.11438692  0.13997085
  0.00901562 -0.24855603 -0.01550403] [ 0.31645012  0.23681979  0.02408778 -0.10382108  0.05711119 -0.21169041
 -0.13343655 -0.11978559  0.10702279 -0.22732002  0.07549891  0.39060447
 -0.11202553  0.01705183 -0.15475994 -0.43785399 -0.10380839  0.35624763
  0.33269167 -0.07350143  0.41160715 -0.2369107  -0.22060162 -0.03197724
 -0.37522468  0.19381419  0.04750056 -0.13244776 -0.44677216  0.11769927
  0.08058778 -0.16975509  0.21654703  0.01641016 -0.17830199  0.14711328
 -0.00857483 -0.10036231 -0.1261739   0.22669169  0.50129974 -0.21256408
 -0.07905225  0.15172993  0.4305349  -0.16698378  0.14232323  0.09830435
 -0.14417355  0.18004134  0.08013424 -0.25265339  0.02507285  0.21474431
 -0.13102664 -0.21296036  0.10397134  0.17001291  0.34161538 -0.07959195
 -0.09309427 -0.08036674  0.12530391  0.12595838  0.24240577 -0.25848654
 -0.03840262 -0.02610202 -0.30724066  0.01428296  0.06805208 -0.01454212
 -0.11942805 -0.07236097 -0.18013084 -0.04557644 -0.17246944 -0.17236757
 -0.15458086  0.09514437  0.01946209 -0.02913421 -0.1651358   0.03722278
 -0.27895769 -0.32350242  0.27376473 -0.21656299  0.03649862 -0.30238959
  0.01986988  0.07263321 -0.29522726  0.1000448   0.16360004  0.03759313
 -0.05754812  0.1577951  -0.28328514 -0.24788456  0.05972687  0.38710335
  0.21939346  0.02502625  0.20167919 -0.14959206 -0.0613601  -0.09237491
  0.22869515  0.31612042  0.30236599 -0.40277198 -0.13804458  0.22532579
  0.04644506 -0.26478741 -0.01336281 -0.08502444 -0.22952536  0.00144277
 -0.09982636  0.35303584 -0.11365893  0.05206472 -0.30778828 -0.0186678
  0.3683435   0.08578552  0.37500313  0.4316445   0.18522869  0.33208132
  0.12994398  0.06649266  0.08646612 -0.04273693 -0.22065608 -0.06062969
 -0.17948057  0.11324365 -0.16578308  0.19471875 -0.00195597 -0.02321819
 -0.15182248 -0.05598313 -0.14000063 -0.18455699 -0.15635788  0.04169158
 -0.22201011  0.08022717 -0.08392134 -0.27490371 -0.14292738  0.30744305
  0.48855576 -0.34065509  0.29066813 -0.27869505  0.04413398 -0.10498763
  0.25054663 -0.06748611  0.28896183  0.14614451  0.36109751  0.08812876
 -0.08782216  0.22276562 -0.04276538 -0.07144949 -0.25736263  0.24658608
 -0.17556362  0.34520796  0.1659715  -0.26152098 -0.10575724  0.17606384
 -0.36855382  0.31804052 -0.01547988  0.05531151  0.26532477  0.30634004
 -0.00294577  0.02476965 -0.28229785  0.0346672  -0.23920681  0.05594238
 -0.26727352  0.23525093  0.26043746 -0.08234384  0.13093305  0.02278524
 -0.14112966 -0.17674607 -0.00372183  0.12161627  0.08111356 -0.38471416
 -0.01870947 -0.32387868  0.28520915 -0.1735421  -0.05342947 -0.15824924
  0.27459946  0.16143864  0.10928415  0.14540176 -0.03411173  0.15543765
  0.11610833 -0.23827273  0.17685643  0.06986243 -0.15838401 -0.10755204
 -0.09384421 -0.28642741 -0.18354462 -0.03380159  0.3755523   0.210922
 -0.39555234  0.26868188  0.43981475  0.3383925   0.0022592   0.02086912
 -0.24158528  0.12817918 -0.03951709 -0.13626215 -0.20241761 -0.09885595
 -0.32536629  0.31841376 -0.011809    0.11596157  0.24954747  0.02114008
 -0.04546936  0.16675743 -0.35752389 -0.08022007  0.41565332 -0.0674333
 -0.36089897  0.08534818 -0.16227688  0.13246828 -0.16108224  0.03373696
  0.02241682  0.02217177  0.0237834   0.53691578 -0.33637285 -0.19968693
 -0.02900627 -0.11760227  0.23112416  0.00647701 -0.20685247 -0.1596486
  0.04900163 -0.0010045   0.15875942  0.26580685 -0.13256335 -0.11070947
  0.30651838  0.02212265 -0.33302328  0.10156352  0.19215786  0.32005918
 -0.24826789 -0.2699734  -0.32378975  0.34928155 -0.0146952  -0.11752178
  0.06641773 -0.25753534 -0.19333531  0.41728577 -0.24997114  0.12036086
 -0.21419427  0.15606789  0.23889135  0.2745221  -0.18515272 -0.22563456
 -0.16935     0.18983887 -0.36086574 -0.21563514  0.11920392 -0.03731018
 -0.24917744 -0.37690786  0.17535953] [ 1.21792354e-01  1.71701923e-01  9.28176716e-02  3.44112664e-02
 -4.04712334e-02 -2.54264027e-01 -7.61395171e-02 -2.43886083e-01
  3.25200036e-02  1.05708443e-01  3.72521788e-01  4.06891018e-01
  3.02007794e-01 -2.35740039e-02 -2.98706591e-01  7.75372162e-02
 -1.33196339e-01 -3.54397714e-01 -8.65575820e-02  4.61489379e-01
  1.89431921e-01 -8.88486356e-02  4.63586003e-01 -2.48722970e-01
 -1.03644788e-01  1.01938613e-01  3.08432281e-01  1.54955164e-01
 -1.29156187e-01 -3.25068086e-01 -4.36671674e-01 -1.45223916e-01
  4.03746843e-01  4.06114280e-01  3.40808630e-02  1.85178578e-01
  3.59434634e-02  4.64595973e-01  3.75215083e-01 -1.89399898e-01
  3.00039738e-01 -1.24282867e-01 -2.35838577e-01 -3.84774715e-01
  4.82806623e-01 -4.20574993e-01 -2.13205125e-02 -4.86814082e-02
 -4.88884926e-01  2.86501665e-02  2.23509446e-01 -2.74271786e-01
 -9.37013552e-02 -2.79578626e-01 -3.20786774e-01 -2.61583596e-01
  3.50888431e-01  3.58721256e-01  1.06570467e-01 -1.21652614e-02
 -2.12426424e-01 -8.51812661e-02  2.97784150e-01 -2.00915575e-01
 -3.73645991e-01 -3.31618544e-03 -5.31684346e-02  7.88335577e-02
  2.06392229e-01 -8.42552930e-02 -2.99242556e-01 -1.55754209e-01
  6.79821670e-02  3.20360720e-01 -3.58014464e-01  3.72449964e-01
 -1.42184682e-02 -7.28049316e-03  6.66286498e-02  1.46651074e-01
 -1.09457180e-01 -3.12776744e-01 -1.77691162e-01 -1.13964275e-01
  4.03761148e-01 -1.98287830e-01 -8.71917158e-02 -2.05662847e-01
 -1.47222728e-01  1.55025750e-01  3.28515768e-01  3.42726819e-02
 -7.02096820e-02 -6.61948845e-02  3.98961067e-01  2.39254192e-01
 -2.79075056e-01  3.35891366e-01  1.15784019e-01  8.14297125e-02
  3.12855959e-01 -8.55740905e-02  8.22482035e-02  3.54312807e-02
  4.73016471e-01 -1.03433475e-01 -3.48960280e-01 -4.41966772e-01
 -1.60929888e-01  1.28236681e-01  9.37324241e-02 -4.45905179e-01
 -2.24989191e-01  4.06685658e-02 -1.38012975e-01 -4.32299376e-01
 -1.46443024e-01 -1.52588189e-01 -2.72427142e-01 -2.42283702e-01
  3.32304776e-01 -2.91969860e-03 -9.64877382e-02  1.02350384e-01
  7.84695894e-02  1.73824042e-01 -1.59363925e-01 -1.75720587e-01
 -1.91105261e-01  4.59781319e-01 -2.24008132e-03  1.83922589e-01
  1.47033796e-01  2.16758057e-01  2.67774880e-01 -3.91298026e-01
  2.01540999e-04 -3.45090255e-02  1.80761263e-01  1.62726164e-01
  1.36710024e-02  4.13417025e-03 -1.84116125e-01  8.99095610e-02
 -2.41826400e-01 -1.66670248e-01 -1.57895729e-01 -1.57549918e-01
 -3.28098595e-01  2.85860866e-01 -1.60906330e-01  2.27405474e-01
  8.99513289e-02 -1.61203042e-01  5.61550679e-03 -8.56929421e-02
  2.47875616e-01 -5.68460561e-02  1.04840219e-01  4.33346301e-01
 -8.24690461e-02 -4.91992235e-01 -2.44547203e-01  2.31135949e-01
  2.31275186e-01  3.97068933e-02  5.63766718e-01 -2.21680731e-01
  3.25221777e-01 -3.92969698e-01  9.98022035e-02  1.21072955e-01
 -9.42577794e-02  4.18985099e-01 -1.08999155e-01 -2.64246404e-01
  1.33463651e-01 -4.63373899e-01  1.32713079e-01 -2.52386332e-01
  9.25139617e-03  1.18884586e-01 -2.44147214e-03  3.45875919e-02
  2.93634403e-02  2.57038057e-01  9.96940359e-02 -3.31551135e-01
  1.58146515e-01 -2.31713146e-01  1.45203531e-01 -1.00259893e-01
 -2.23209590e-01  2.92554229e-01 -2.27789998e-01 -1.07975043e-01
  4.14370038e-02 -3.19908887e-01  3.75336669e-02  1.30740166e-01
 -5.92054566e-03  2.76437938e-01 -2.53319234e-01 -6.02751374e-02
 -1.06824771e-01 -6.97998628e-02  1.63218036e-01 -3.03648323e-01
 -2.91088261e-02 -2.53686666e-01 -3.25902760e-01  1.27031878e-01
  8.24536234e-02 -1.55437648e-01  8.76655430e-02  3.09565902e-01
  1.32955670e-01 -4.13661310e-03 -5.46287179e-01 -4.49238010e-02
 -3.44222635e-01  1.21715024e-01 -3.43544155e-01 -8.81644413e-02
  3.61567765e-01  1.13531284e-01  1.73401445e-01 -3.33700664e-02
  1.21632911e-01 -9.56284478e-02  1.65303916e-01 -5.75213991e-02
  5.91532178e-02 -2.15798616e-04  1.36264816e-01 -1.68062776e-01
 -8.40380613e-04 -2.41652764e-02 -8.98588970e-02  4.50640976e-01
  1.27012171e-02  1.54586464e-01  1.09918028e-01  3.27993572e-01
 -7.49246925e-02  4.05966967e-01 -1.98324353e-01  1.05989300e-01
 -6.18032143e-02 -4.06254530e-01 -1.35624064e-02 -5.67466095e-02
 -1.60735548e-01  1.67493060e-01 -1.18234664e-01 -1.87053755e-01
 -2.37661585e-01  1.19242333e-01  1.00548394e-01  3.67405266e-01
  6.44182324e-01  1.33120298e-01  3.51179279e-02 -2.22409949e-01
  2.11527139e-01  1.72435671e-01  2.00692981e-01  4.71674979e-01
  2.50788987e-01 -4.57798541e-01 -4.69608873e-01  6.13417625e-01
  9.63033438e-02  2.90439725e-01 -9.80913788e-02 -2.49806941e-02
  1.32012427e-01 -1.83126315e-01 -4.38448966e-01 -5.57753026e-01
 -5.30402511e-02  5.43790579e-01  2.74377018e-01 -7.46730044e-02
  2.16088280e-01  1.98789015e-01 -8.25360939e-02 -2.68025964e-01
 -1.59388557e-02 -5.01921356e-01  1.86366275e-01 -5.60372733e-02
 -6.82820082e-02  6.36150688e-02 -3.47789794e-01  2.48542968e-02
  2.56896108e-01  3.48623037e-01  1.83648571e-01  1.92874789e-01
  1.80124328e-01 -3.68094966e-02 -1.52325451e-01 -3.25481623e-01
 -3.62335704e-02  4.11888324e-02 -1.40705630e-01 -4.72733885e-01
  5.47193736e-02] [ 0.32129142  0.34275874  0.02520792 -0.17069589 -0.02509769 -0.24610339
 -0.14947852 -0.06770577  0.04775895 -0.12865323 -0.00587961  0.40081266
 -0.12070474  0.1627156  -0.27493748 -0.34065032 -0.09638485 -0.07585221
  0.26275095  0.14115892  0.56027579  0.09391703 -0.03157583 -0.06131345
 -0.30302879  0.29266     0.14633767 -0.29173338 -0.4218567  -0.04984074
 -0.01523861 -0.27980259  0.30902347 -0.07985619 -0.03113094  0.45960462
 -0.08167046 -0.2623812   0.32746357  0.11063384  0.3848201   0.09877135
 -0.10707947 -0.12795416  0.61733323 -0.32107729  0.07276133  0.06431024
 -0.0837815   0.29677981  0.15318242 -0.16234712 -0.3501088   0.20556155
 -0.14536348 -0.19678023  0.23888735  0.3629497   0.53913122  0.08683708
 -0.04211164 -0.16521768 -0.0845599  -0.1613178   0.53011513 -0.19315766
 -0.09448384  0.10476014 -0.43564886 -0.0467296   0.12307405 -0.14992613
 -0.24799709 -0.16738445  0.00878228 -0.15760876 -0.15982839 -0.1486866
  0.19890103 -0.02793303 -0.31631824 -0.08811046 -0.08718562 -0.28345934
 -0.19749096  0.18267643  0.3848488  -0.25893342  0.04576158 -0.13806039
  0.00485378  0.00142407 -0.2058866  -0.2954998   0.29266649 -0.05231676
 -0.16484924  0.19258292 -0.28769273  0.02640593  0.11269996  0.11946734
  0.01571279  0.21045668  0.32528332 -0.13634858  0.11618441 -0.14680246
  0.30696246  0.1324221   0.19762698 -0.5158124  -0.05959453  0.29647151
 -0.15397565 -0.1582606  -0.05768587 -0.01695118 -0.31660894  0.14543864
 -0.00211576  0.28766745  0.06578264 -0.03321915 -0.22112617  0.02292406
  0.08700944  0.3655214   0.03551405  0.56008869  0.23336633  0.39227021
  0.09454845 -0.01456762  0.38055325  0.17670546 -0.04823102 -0.34046331
  0.18031392 -0.05488857  0.07892235  0.06455727  0.03912613  0.08486982
 -0.12845126  0.09610099 -0.51143593 -0.17198026 -0.43713665  0.45956507
  0.11729788  0.23935741  0.34630436  0.21533783 -0.68809712  0.09467459
  0.15384085 -0.46076292  0.08141962 -0.09654498 -0.42688474 -0.10774308
 -0.12893569  0.19888964  0.19947931 -0.17388393  0.17395931  0.40384221
  0.1265118   0.26716408  0.01654734  0.11205247 -0.35283604  0.33794326
 -0.12522346  0.36724988 -0.09391516 -0.58248091 -0.21771401 -0.08072329
 -0.15599363  0.13244164  0.26231608  0.02695386  0.54205966  0.36848328
 -0.12109972 -0.21267968 -0.36622828 -0.24164866 -0.14064445  0.00577394
 -0.58479375  0.20800914  0.2576378  -0.28312299 -0.03562433 -0.02778082
 -0.0811988  -0.24241436 -0.04245684 -0.00544086 -0.009803   -0.12108323
 -0.20091893 -0.32479119  0.43964642  0.01045767  0.12925194 -0.09777942
 -0.2244464  -0.23295249  0.15205134  0.3222011  -0.00778939  0.04670145
 -0.15952466 -0.29401425 -0.42292076  0.37608504 -0.20026644  0.12884668
 -0.09257212 -0.40015435  0.13120306 -0.05446926  0.34483159  0.28026706
 -0.06562635 -0.06797577  0.15773442  0.4386214   0.06613781  0.27243027
 -0.54485857 -0.18453282  0.15764515 -0.28570381 -0.13737997  0.25276387
 -0.18072259  0.26315209  0.20603439  0.24877501  0.31992567  0.47946602
 -0.28665191  0.07400503 -0.17260076 -0.30247593  0.36552262 -0.09942875
 -0.22677104  0.08525125 -0.24987739 -0.1199422  -0.25224769  0.03803614
 -0.06518582  0.0033514   0.20556688  0.18485056 -0.07133536  0.18721342
  0.00163307  0.17257282  0.31492656  0.00552698  0.19321126  0.20635656
 -0.08405183  0.2177614   0.25604236  0.08820758 -0.19261166 -0.00139286
  0.14145331 -0.05124622 -0.35767561 -0.40606594  0.01588637  0.31500688
 -0.44471392 -0.06327396 -0.23408964  0.04175372 -0.27082005  0.02459745
 -0.09878814 -0.5250932  -0.29180118 -0.1578908  -0.24199872  0.08122445
 -0.46725142 -0.00557958  0.17929903  0.26785839 -0.187171   -0.22061442
  0.14176896 -0.19339901 -0.01615573 -0.24118501  0.29263994  0.23209135
 -0.31540474 -0.29112038  0.08200915] [ 2.69596398e-01  1.91759109e-01  7.45855123e-02 -8.29560757e-02
 -3.78013849e-02 -2.22420707e-01 -7.20924437e-02 -9.77678820e-02
  8.01148638e-02  8.08044150e-02 -2.03729391e-01  1.42736837e-01
 -1.17445193e-01  8.83530602e-02 -1.89039946e-01 -1.44379541e-01
  4.53715697e-02  2.00437322e-01  2.57499844e-01 -3.82643305e-02
  4.38777983e-01 -7.76957199e-02  5.55036869e-03  1.27830163e-01
 -1.23497814e-01  1.72979429e-01 -2.73898281e-02 -8.51162001e-02
 -3.20802927e-01 -2.22475175e-03  2.54869368e-03 -3.07188630e-01
  6.08860608e-03  1.93871036e-01  4.44726869e-02  1.38662323e-01
 -2.12539941e-01 -1.65046096e-01  5.66864163e-02 -1.32763341e-01
  3.60946059e-01 -3.44822496e-01 -5.09792447e-01  1.76090319e-02
  1.43796265e-01 -1.47524178e-01  1.10515803e-01 -1.36567667e-01
 -2.57102758e-01  6.41814545e-02  1.81436792e-01 -2.95867026e-01
  9.45026353e-02 -1.80575386e-01 -3.25196058e-01  6.98337480e-02
  2.21740097e-01  3.04359883e-01  2.76625633e-01  7.60802552e-02
  3.77791971e-01 -1.83525965e-01  3.18503613e-03 -5.04144747e-03
 -2.29752168e-01 -4.01022971e-01 -7.07295462e-02  4.51709121e-01
 -6.74286634e-02 -1.14996254e-01 -1.06970727e-01 -1.39018998e-01
 -3.55817843e-03 -1.67044163e-01 -4.62142751e-02 -7.99760669e-02
  2.30059009e-02 -6.03521056e-02 -2.50296533e-01 -7.41522089e-02
 -2.06949517e-01 -1.00759289e-03  6.69077933e-02  1.87983319e-01
 -8.34892467e-02 -1.01998910e-01  1.60741061e-01 -1.51447326e-01
  1.14653341e-01  1.04487702e-01  3.41225833e-01  1.69752054e-02
  4.41315435e-02  1.20973863e-01  4.20527697e-01  8.74084793e-03
  1.40714217e-02  9.44754705e-02  9.65630263e-02 -1.25024691e-01
  1.81932524e-01 -1.97963819e-01 -1.64068267e-01  1.10719532e-01
  2.85786003e-01 -1.50386512e-01 -6.03460521e-02 -3.03773165e-01
  2.14680135e-01  1.81502521e-01  5.67571297e-02 -1.87934414e-01
 -9.04868171e-02  4.90822159e-02 -1.83148265e-01 -9.27971378e-02
  1.20327607e-01  2.49341223e-02 -7.38233700e-02 -3.83092053e-02
 -2.32614130e-01  1.06850319e-01  2.89832562e-01  2.52187788e-01
  1.73658490e-01  9.89509970e-02  1.98941261e-01 -5.36561720e-02
  1.79709885e-02  1.57364950e-01  4.78193536e-03  4.05631661e-01
 -2.03738496e-01 -8.81424844e-02  9.90742892e-02  6.57372996e-02
  2.23578095e-01 -2.60956585e-01 -1.46646187e-01 -1.56827033e-01
  1.16384111e-01 -1.03699207e-01 -7.75325224e-02 -4.36494723e-02
 -3.89235556e-01  5.23779355e-02 -1.37442812e-01 -3.46143007e-01
 -2.95676500e-01  2.19095841e-01 -7.54843801e-02  2.56667882e-01
  5.82759874e-03  6.02615736e-02 -2.89973944e-01 -1.46622017e-01
  8.34360793e-02  1.48489714e-01  1.11555770e-01 -1.45713717e-01
 -2.52818465e-01 -2.12075695e-01 -1.79957941e-01  1.52505487e-01
 -1.52356595e-01  1.32661849e-01  3.59037191e-01  4.54879040e-03
  2.06848770e-01  1.98306590e-01 -4.69168619e-04 -2.21890613e-01
  4.26304489e-02  5.61082922e-03 -7.17740552e-03  2.25499570e-02
 -4.60744239e-02 -1.00851528e-01  3.75439078e-02  1.36874199e-01
  1.11879818e-01  1.11123726e-01 -7.74132609e-02 -3.03931963e-02
  4.51624364e-01  4.56292816e-02 -8.79436433e-02 -2.10551918e-01
 -3.96519274e-01 -4.53393906e-02 -1.18866004e-01 -1.54110109e-02
 -1.64960876e-01  4.48043793e-01 -1.93358168e-01 -2.09860116e-01
  1.07468769e-01  2.60477699e-02  2.10237429e-01 -1.50546104e-01
  4.77076769e-02 -3.51515375e-02 -3.43842745e-01 -5.14898859e-02
 -1.54375851e-01 -6.18461668e-02  3.23755980e-01 -2.52464950e-01
 -8.73073414e-02 -1.01149894e-01 -4.81258214e-01 -4.32510376e-01
  2.55202860e-01  4.50172611e-02  2.90537328e-01  1.74336150e-01
 -4.23079617e-02 -1.16322087e-02 -3.71555597e-01 -1.69404447e-01
 -8.18354487e-02  3.56375277e-01 -3.00571710e-01  3.17464173e-01
 -1.12117082e-01 -7.53164813e-02 -2.32147306e-01  1.52136624e-01
 -8.41358826e-02 -2.10176483e-01  1.69225946e-01  8.05071741e-02
  1.89560384e-01  2.68778086e-01 -1.94387108e-01 -2.07499728e-01
  3.70671064e-01 -3.14747036e-01 -2.49776363e-01  3.65313888e-01
 -1.28890947e-01  2.33948529e-01 -5.61595941e-03 -4.21459414e-02
  1.66597813e-01  4.41533297e-01 -1.30485222e-01  1.37113616e-01
  2.21991152e-01 -9.73499417e-02  3.58788252e-01 -4.45248872e-01
  3.36940944e-01  3.72039229e-01 -3.78200375e-02 -2.21976936e-01
 -2.28775829e-01 -6.02928288e-02 -9.20211989e-03 -1.57663837e-01
 -2.32389774e-02  3.91607016e-01  5.84651269e-02  2.01744482e-01
 -1.43581241e-01  6.04165718e-02  3.99592280e-01 -3.73861790e-02
  1.93691161e-02 -2.34815180e-01 -3.52356672e-01  3.91583830e-01
  5.16723357e-02  1.75000444e-01 -3.09564680e-01 -2.63427526e-01
 -2.14793477e-02  6.01779670e-02 -4.26586688e-01 -1.35944635e-01
  4.72167701e-01  2.79229194e-01  2.47289222e-02  4.57213111e-02
 -3.34514946e-01 -1.31633162e-01 -9.71250609e-02  1.06061414e-01
 -7.84177706e-02 -2.76234299e-01  2.20471472e-02 -6.46766573e-02
 -1.40880281e-02  6.84074834e-02 -1.59682184e-01 -7.02267215e-02
  2.11941823e-01  3.44715744e-01  1.42822787e-03 -1.66162550e-02
  2.01411709e-01 -2.09427908e-01 -2.45356336e-02 -9.55456644e-02
  1.30344570e-01  7.56725296e-02 -4.09503728e-01 -2.51056403e-01
  6.13081343e-02] [ 0.4795565   0.1805498   0.13075891 -0.06237778 -0.10000283 -0.13903876
  0.0653518  -0.31374249  0.1547047   0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ] [ 0.08702344  0.18458235  0.37562189 -0.01208748 -0.44048437 -0.12599725
 -0.00877414 -0.29612905  0.07801971  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ] [ 0.30658117  0.11490601  0.13285208  0.03179812 -0.0184346  -0.04639965
  0.00652742 -0.12284825  0.022705    0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ] [ 0.13725726 -0.08411375  0.13283202 -0.05049944 -0.06270097 -0.03976175
  0.04712682 -0.21629179  0.15849967  0.19789802  0.29917771 -0.41562298
 -0.27813095  0.37624815  0.00070632 -0.22391008  0.17507589 -0.15026779
  0.37688705 -0.08638491  0.15119883  0.61398095  0.24422123 -0.07763284
 -0.43382424 -0.26687148  0.25686097 -0.61813521 -0.02631504  0.09381697
  0.27491325 -0.08027732 -0.04776844 -0.15268967 -0.09704675  0.11016345
  0.00331933  0.09226041  0.19894879  0.43345556  0.2939828   0.17427211
 -0.01511596  0.27703997 -0.38575169  0.30519849  0.3871949   0.30099708
  0.11614652 -0.40382951 -0.05785611  0.17658447  0.27228341  0.0413342
 -0.44348621  0.25965747 -0.09529594  0.06042131 -0.14962345  0.19171786
  0.14417745 -0.09275478 -0.32167268 -0.37792474 -0.46977729 -0.50896287
 -0.03105223 -0.22275354 -0.28858826 -0.23556066 -0.4080677   0.39622742
 -0.04244985 -0.23216279 -0.01922166 -0.38239151  0.21332492  0.03237812
  0.29343703 -0.08492372 -0.01914045  0.15634584 -0.16758366 -0.2295393
  0.53357148  0.15027075 -0.41327044 -0.02946115  0.18027946 -0.1982251
  0.13857034 -0.13777256 -0.34712523 -0.15604933 -0.26341382  0.00908263
  0.43156156 -0.42802995 -0.04291266 -0.10167361 -0.05730546 -0.30774
  0.22017293  0.44112515  0.6395036  -0.00810865  0.09935226 -0.27528736
  0.2186396  -0.0446081  -0.28398472 -0.07203508  0.41714075 -0.11604772
  0.16438587 -0.20797506 -0.17566746 -0.2506299   0.12850042 -0.0008907
  0.07040917  0.31692377  0.49306947  0.35317123  0.0007317   0.16356748
 -0.21185002  0.31562591  0.11431498 -0.40370551  0.16962026  0.26263309
  0.00289708  0.41320091  0.11440121 -0.15242957  0.27729389  0.36347729
  0.0753918   0.24863915  0.53468716 -0.4492034  -0.57134825  0.47271293
 -0.46930864  0.15353066  0.06556755  0.30489916 -0.29397678 -0.09070148
 -0.01317201 -0.26230913  0.06945382 -0.07919732  0.23873553  0.00989845
 -0.24668775  0.25778827  0.09697059  0.08924693  0.02318965 -0.03254029
 -0.32614091  0.1183411   0.40294969 -0.05077374 -0.32458597  0.04394685
  0.39241117 -0.06186248 -0.06070341 -0.32964236 -0.31462345  0.22473606
 -0.46781754  0.07671089 -0.33199519  0.20458835  0.23304199 -0.32019144
  0.3550145  -0.09998508  0.25982168 -0.51351225  0.08759576 -0.10103939
  0.35800803  0.08007183 -0.13989976 -0.06097021  0.08239157  0.38855883
  0.50636184 -0.30682507 -0.34935522  0.00286579  0.24112737 -0.09757469
  0.48234689  0.11201829 -0.03483088  0.0086148   0.03718236  0.16086027
 -0.50091648  0.11070649  0.11764584 -0.00469694 -0.27691293  0.35355818
  0.19474538 -0.55002183 -0.41403794  0.1637073  -0.38991094 -0.03505617
 -0.34459525 -0.28583032  0.01815002 -0.1270884  -0.1669879   0.35768694
 -0.25795013 -0.37925491 -0.21371774  0.01397513 -0.0555716  -0.04388139
 -0.13516787  0.19788499  0.28547359 -0.32185724 -0.21646208  0.34070441
  0.26185623  0.03647481  0.1780252  -0.14166249  0.24406788 -0.18231942
  0.17257465  0.24240263 -0.02668856  0.04648128 -0.47689876  0.28245425
 -0.10446384  0.0644256   0.27179191  0.27966422  0.02973623 -0.41798019
 -0.0525245   0.13302465  0.12957156  0.39594379  0.14385284 -0.05942453
 -0.12710027  0.31552276  0.03729419 -0.21562797  0.17353016 -0.51500052
 -0.61404657 -0.4381057   0.04743306  0.27465329  0.30613154 -0.08322201
 -0.09680272  0.10589802  0.32906204 -0.06526046  0.06010919 -0.0673278
  0.10635953  0.16586635 -0.41190296  0.09106021 -0.10887404 -0.09153771
 -0.54688251 -0.14207008 -0.05963541 -0.06242859 -0.04508504  0.23137389
  0.07256412 -0.46708483  0.44395405 -0.36846474  0.32419744  0.19434559
  0.11038412  0.28146294  0.19379517  0.09715058  0.04694065  0.24668971
  0.19462302 -0.00443839  0.34439847 -0.09252615  0.06287297  0.49551693
  0.27744564 -0.31561071  0.05698482] [ 0.14539956  0.15284954  0.11732712 -0.00734479 -0.01622459 -0.19360994
  0.05985836 -0.23592828 -0.03114882  0.39579603  0.59835541 -0.83124596
 -0.5562619   0.7524963   0.00141265 -0.44782016  0.35015178 -0.30053559
  0.75377411 -0.17276981  0.30239767  1.2279619   0.48844245 -0.15526567
 -0.86764848 -0.53374296  0.51372194 -1.23627043 -0.05263007  0.18763393
  0.5498265  -0.16055463 -0.09553688 -0.30537933 -0.1940935   0.2203269
  0.00663866  0.18452083  0.39789757  0.86691111  0.58796561  0.34854421
 -0.03023191  0.55407995 -0.77150339  0.61039698  0.7743898   0.60199416
  0.23229304 -0.80765903 -0.11571221  0.35316893  0.54456681  0.08266839
 -0.88697243  0.51931494 -0.19059189  0.12084261 -0.29924691  0.38343573
  0.2883549  -0.18550956 -0.64334536 -0.75584948 -0.93955457 -1.01792574
 -0.06210445 -0.44550708 -0.57717651 -0.47112131 -0.81613541  0.79245484
 -0.08489971 -0.46432558 -0.03844332 -0.76478302  0.42664984  0.06475624
  0.58687407 -0.16984744 -0.0382809   0.31269169 -0.33516732 -0.45907861
  1.06714296  0.30054149 -0.82654089 -0.0589223   0.36055893 -0.39645019
  0.27714068 -0.27554512 -0.69425046 -0.31209865 -0.52682763  0.01816525
  0.86312312 -0.85605991 -0.08582532 -0.20334722 -0.11461093 -0.61548001
  0.44034585  0.88225031  1.2790072  -0.0162173   0.19870451 -0.55057472
  0.43727919 -0.08921621 -0.56796944 -0.14407016  0.8342815  -0.23209544
  0.32877174 -0.41595012 -0.35133493 -0.5012598   0.25700083 -0.0017814
  0.14081834  0.63384753  0.98613894  0.70634246  0.0014634   0.32713497
 -0.42370003  0.63125181  0.22862996 -0.80741102  0.33924052  0.52526617
  0.00579416  0.82640183  0.22880241 -0.30485913  0.55458778  0.72695458
  0.1507836   0.4972783   1.06937432 -0.8984068  -1.1426965   0.94542587
 -0.93861729  0.30706131  0.13113511  0.60979831 -0.58795357 -0.18140297
 -0.02634402 -0.52461827  0.13890764 -0.15839465  0.47747105  0.0197969
 -0.49337551  0.51557654  0.19394118  0.17849386  0.04637929 -0.06508058
 -0.65228182  0.23668221  0.80589938 -0.10154747 -0.64917195  0.0878937
  0.78482234 -0.12372497 -0.12140682 -0.65928471 -0.62924689  0.44947213
 -0.93563509  0.15342177 -0.66399038  0.40917671  0.46608397 -0.64038289
  0.71002901 -0.19997016  0.51964337 -1.02702451  0.17519152 -0.20207877
  0.71601605  0.16014366 -0.27979952 -0.12194042  0.16478314  0.77711767
  1.01272368 -0.61365014 -0.69871044  0.00573157  0.48225474 -0.19514938
  0.96469378  0.22403657 -0.06966177  0.0172296   0.07436471  0.32172054
 -1.00183296  0.22141297  0.23529167 -0.00939388 -0.55382586  0.70711637
  0.38949075 -1.10004365 -0.82807589  0.3274146  -0.77982187 -0.07011233
 -0.68919051 -0.57166064  0.03630003 -0.2541768  -0.33397579  0.71537387
 -0.51590025 -0.75850981 -0.42743549  0.02795027 -0.11114321 -0.08776278
 -0.27033573  0.39576998  0.57094717 -0.64371449 -0.43292415  0.68140882
  0.52371246  0.07294963  0.3560504  -0.28332499  0.48813576 -0.36463884
  0.34514931  0.48480526 -0.05337713  0.09296256 -0.95379752  0.5649085
 -0.20892768  0.12885119  0.54358381  0.55932844  0.05947246 -0.83596039
 -0.10504901  0.2660493   0.25914311  0.79188758  0.28770569 -0.11884906
 -0.25420055  0.63104552  0.07458839 -0.43125594  0.34706032 -1.03000104
 -1.22809315 -0.8762114   0.09486613  0.54930657  0.61226308 -0.16644402
 -0.19360544  0.21179605  0.65812409 -0.13052091  0.12021838 -0.13465561
  0.21271905  0.33173269 -0.82380593  0.18212043 -0.21774808 -0.18307541
 -1.09376502 -0.28414017 -0.11927082 -0.12485718 -0.09017007  0.46274778
  0.14512824 -0.93416965  0.8879081  -0.73692948  0.64839488  0.38869119
  0.22076824  0.56292588  0.38759035  0.19430116  0.09388129  0.49337941
  0.38924605 -0.00887678  0.68879694 -0.18505229  0.12574594  0.99103385
  0.55489129 -0.63122141  0.11396965] [ 0.0978981   0.01783093  0.05027778 -0.14408033  0.13288936 -0.14443889
  0.07469472  0.03034516 -0.06471247 -0.15286922  0.11267807  0.2293372
 -0.26542705  0.13366187  0.04092865  0.37100086 -0.10599201 -0.30404541
 -0.49771667  0.28613973 -0.5554862   0.02143512  0.03488499  0.10015943
 -0.00232789  0.66660607 -0.21503855  0.12561619  0.26148483 -0.11905316
 -0.06594202  0.08652379 -0.20505428  0.17803317  0.45499635 -0.12265726
  0.53997689 -0.44353235 -0.18785878 -0.16538987 -0.31131643  0.24553877
 -0.2430961   0.22160135 -0.06976528  0.19342439  0.07373687 -0.44760519
  0.33325338  0.29229864  0.30060273 -0.29366666  0.35834444  0.03847735
 -0.28993928 -0.04195101 -0.05793238 -0.11676014 -0.04236681 -0.21269216
 -0.06716813 -0.35614917  0.04405568  0.17273371  0.4469623  -0.13074483
 -0.09566131 -0.40008157  0.18059961 -0.7123493   0.34082288  0.49733508
 -0.08396833 -0.23098266 -0.25211647 -0.0265602   0.2614702  -0.30672628
 -0.07392909  0.21971838 -0.65880138  0.05129592  0.04885766  0.09348635
 -0.11813313  0.15963589  0.29635945 -0.11897612  0.12616532 -0.12725122
 -0.01683596 -0.22924967  0.15747738 -0.47936881 -0.35660058 -0.30168551
  0.488525   -0.06723613  0.18778726 -0.42746931  0.15547316 -0.71088064
  0.2934655  -0.51007348  0.167037    0.12780297 -0.15009981  0.07156654
 -0.11955497  0.4046779  -0.42070279  0.27839765  0.56772286  0.29432893
  0.44948247  0.38968363  0.6485033   0.03694484  0.46740982 -0.49257278
  0.09620359 -0.39707559 -0.25151449 -0.23042046 -0.37765682  0.11089069
 -0.36786467  0.56705242 -0.03823859  0.15948889 -0.28109717 -0.3851392
 -0.15943483  0.10727303 -0.0450397  -0.17614882 -0.18977743 -0.21970016
 -0.23258488  0.15544309 -0.16462456 -0.2089193  -0.5277788  -0.53386521
  0.19475067  0.01923611 -0.13348837  0.35472444  0.02996855  0.2167024
 -0.22921209  0.09819613  0.55717003 -0.21456073  0.19085337  0.2863802
  0.14694285 -0.23350805  0.15320973  0.09031536 -0.13217555 -0.38161564
  0.43314585  0.00465568 -0.12114217  0.39186811  0.22677538 -0.24730301
  0.25663239  0.04673701  0.08719243  0.01624326 -0.24889724 -0.22090381
 -0.05904419 -0.0831335  -0.59033495  0.18020472  0.39785495  0.0405685
  0.07458658 -0.29943874 -0.01694294  0.11456117  0.09019414  0.32686734
  0.0675974   0.21975519 -0.34298003 -0.14795929 -0.25062224 -0.20149159
 -0.1285758  -0.16725364 -0.18651694 -0.30109659  0.31447828 -0.05496696
  0.43632498  0.13863817 -0.14198817 -0.05433459 -0.02173719  0.1197678
  0.04033393  0.26666665  0.1945204  -0.10329008  0.25231338 -0.11325161
 -0.15671328  0.14611265 -0.31876901  0.29314929 -0.29495791 -0.45679215
  0.1111379   0.37385756  0.21727364 -0.64442319  0.30826575  0.16147387
 -0.50456309  0.11389183  0.07192434  0.05269609  0.40301979 -0.28698766
 -0.03162227 -0.28836489 -0.14267424  0.39455518  0.29675034  0.33228239
 -0.04872128 -0.21754535 -0.35264158 -0.31700945  0.14606099  0.02457966
 -0.19854425  0.38516647 -0.11399285 -0.17241368  0.39310306 -0.48425937
 -0.38476282  0.3391479   0.02503851  0.05031592 -0.1646452   0.36957774
 -0.02775983  0.19695003 -0.20174566 -0.52748328 -0.19198567 -0.40657264
  0.51577663 -0.41611657  0.37526962  0.06360652  0.03527089  0.3197186
 -0.32548842 -0.0651332   0.04846462  0.17600997  0.04905157  0.00505366
 -0.14774697 -0.20983689 -0.0766654   0.35657462 -0.1340798  -0.50466192
  0.32419461  0.02462816 -0.27638125  0.04313075  0.28970757  0.20448315
  0.27136564  0.27330178  0.28196019 -0.168115   -0.30521712 -0.02905159
 -0.10994692  0.12255888 -0.06139188  0.25843009 -0.08472705 -0.376932
  0.448291    0.08825782 -0.19742529 -0.13183616 -0.27942851 -0.4214687
 -0.43062347 -0.00927636  0.09114671 -0.12799481  0.05055178  0.54697067
  0.04275419 -0.06240009  0.28663015] [ 4.16959852e-01  4.70334828e-01  1.30235240e-01 -9.87012386e-02
 -1.61589980e-02 -4.00925457e-01 -1.94210619e-01 -2.85851181e-01
  9.88782048e-02 -6.50947094e-02 -3.31301950e-02  3.48044097e-01
 -7.33295009e-02  2.10728068e-02 -2.07522407e-01  1.91047922e-01
  1.22413687e-01  5.62063372e-03  5.21371067e-01 -4.05533426e-03
  3.44237447e-01 -1.87540893e-02 -5.90793863e-02 -4.35210556e-01
 -5.52363634e-01  5.72056510e-02 -1.53152540e-01 -2.13377312e-01
 -3.63019645e-01  3.70914131e-01  7.59203685e-03 -1.34108841e-01
  4.38207090e-01  4.35608268e-01  3.64546567e-01 -1.63548850e-02
  1.85392752e-01 -9.73207504e-02  1.16956107e-01  2.81179577e-01
  1.82226300e-01 -2.48137742e-01 -2.68088579e-01  3.82861257e-01
  4.43311445e-02 -2.40556411e-02  1.99132189e-01 -4.07733917e-01
 -1.60095140e-01  1.47689849e-01  9.82762203e-02 -6.66364804e-02
  1.06266543e-01  1.11868970e-01 -2.02551842e-01 -2.09942579e-01
 -3.13621163e-02  1.42645448e-01  1.50794968e-01 -1.91261232e-01
  2.47332692e-01 -8.53240862e-02 -8.02288298e-03  3.77471070e-03
  5.41099347e-02 -2.18042672e-01 -3.93098980e-01  4.34802562e-01
  2.02354923e-01 -1.17975824e-01 -2.67976765e-02 -3.28577757e-02
 -1.72158733e-01 -3.57320786e-01 -3.95335704e-01 -1.06003210e-01
 -3.68844301e-01 -1.26202837e-01  3.26519310e-01 -9.63570103e-02
 -3.87268126e-01  7.60012716e-02 -1.51632562e-01  3.74109931e-02
 -1.68846846e-01 -4.23047304e-01  5.56434035e-01 -2.64342576e-01
 -3.50948334e-01 -1.88997522e-01 -9.93732139e-02 -8.53688344e-02
 -1.45351931e-01 -5.22544742e-01 -1.11575294e-02 -2.05126524e-01
  3.33836079e-02  3.04927766e-01  3.25280696e-01  2.10312605e-01
  3.61351460e-01  5.44163764e-01 -3.16010207e-01 -1.66704461e-01
 -2.21391842e-01 -3.15935798e-02 -4.11216795e-01 -6.00474119e-01
  4.71896470e-01  3.66874129e-01  1.53858408e-01  1.51549041e-01
 -2.53780991e-01  4.00370896e-01  9.68489349e-02  1.66299760e-01
 -2.85789341e-01 -3.83986682e-02 -2.38401085e-01  2.93825883e-02
 -1.03073254e-01  3.66191536e-01 -1.96942121e-01  2.39743590e-01
  1.99805692e-01  3.00910860e-01  2.63787489e-02  2.72590704e-02
  1.38867810e-01  1.66544452e-01  2.49896601e-01 -2.76550148e-02
  4.51925904e-01 -8.06802809e-02 -2.21689329e-01 -2.00756818e-01
  1.59075052e-01 -4.19044793e-01 -2.58606691e-02  4.06134845e-04
  1.41502898e-02 -1.71605960e-01  5.20586632e-02  3.71708721e-01
 -2.98400193e-01  1.88751847e-01 -2.06870660e-01 -1.81840509e-01
 -2.08676115e-01  1.52675599e-01 -1.99452534e-01  8.93244892e-02
  2.88513124e-01 -2.63475031e-01 -3.28928649e-01 -4.22651954e-02
  1.26030192e-01 -4.96258080e-01  9.28621218e-02  2.00932994e-02
 -2.60032833e-01 -1.62482634e-01  4.07439232e-01 -9.42604542e-02
 -5.29549837e-01  9.46943238e-02  3.14039856e-01  4.88696620e-02
  1.18388250e-01 -3.20630133e-01 -6.66333269e-03 -4.01535332e-01
 -1.28975838e-01  4.76904601e-01  1.04345204e-02  2.32845068e-01
  1.16885982e-01 -3.23339283e-01  3.04378927e-01  2.81558316e-02
  3.32918495e-01  1.08405374e-01  2.12294310e-01 -2.76519414e-02
  6.13922358e-01  4.07808945e-02 -3.64578098e-01 -5.31470537e-01
 -1.78524300e-01  5.58551587e-02 -9.52764787e-03  8.85526016e-02
 -4.19316083e-01 -1.94234215e-02  2.50195920e-01 -5.41165173e-02
  1.46807745e-01  9.07770172e-03  1.58198550e-01  5.42495362e-02
  3.77792507e-01  2.43625179e-01 -3.38445127e-01 -1.68740928e-01
 -3.38088095e-01 -5.48435412e-02 -7.67025873e-02 -1.69910997e-01
 -1.30231068e-01  8.75751227e-02 -1.04524963e-01  1.00460477e-01
 -1.38068482e-01  2.99491554e-01  9.94744971e-02 -1.34747341e-01
  9.80186015e-02 -1.99868172e-01 -5.76205671e-01  3.47168386e-01
  2.05535367e-01  2.80020148e-01 -4.17977899e-01  3.30600515e-02
 -1.06454991e-01 -2.14617580e-01 -1.57049477e-01 -1.20070152e-01
  2.64764518e-01 -1.77326538e-02  6.51822537e-02  8.58592093e-02
 -3.91419172e-01  2.14156471e-02 -6.95468485e-02 -9.63311121e-02
  2.15616539e-01 -2.55790025e-01 -3.89193594e-01  2.63024509e-01
  1.96685672e-01  3.89115125e-01 -4.06976417e-03  1.19265340e-01
 -4.40811850e-02 -4.39922921e-02  1.14668198e-01 -3.53143722e-01
 -4.65403311e-02 -2.56810814e-01  3.60860944e-01 -5.25257066e-02
 -8.64340365e-02 -1.96654931e-01  1.23224601e-01 -1.47967771e-01
  1.19660504e-01  1.62660889e-02  2.31315531e-02 -3.26530069e-01
  3.28583457e-02  4.93397176e-01  2.19572961e-01 -4.05126112e-03
  3.41708846e-02  1.68836683e-01 -1.55329779e-02 -1.95950910e-01
  8.61623958e-02 -1.49894357e-01  6.28337413e-02  4.68520783e-02
 -1.55178323e-01  2.12353528e-01  1.20956935e-01  1.61230937e-01
 -6.05303288e-01 -9.56170857e-02 -3.12793255e-01 -8.59709680e-02
  1.87703267e-01 -3.52839716e-02 -2.78133929e-01  1.24644503e-01
  3.55439261e-02  4.62416746e-02  2.23216057e-01 -6.16782531e-02
 -3.36378306e-01 -3.24398696e-01 -1.84533000e-01  2.64820546e-01
  8.70983675e-02 -4.62117195e-02 -4.27224636e-01 -9.72464755e-02
  4.29119110e-01  2.36186236e-01 -2.05660220e-02  1.69577837e-01
  5.70382476e-02  3.95817488e-01 -3.96005623e-03  1.81853071e-01
 -1.46695048e-01  4.55200255e-01 -7.89807066e-02 -3.94342206e-02
  2.03648150e-01] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  3.23701560e-01  1.23859271e-01  3.17038149e-01
 -1.99837953e-01  1.45933881e-01  1.97479472e-01  1.11766860e-01
 -2.97712028e-01  2.12801516e-01 -8.30430072e-03 -1.36353567e-01
  5.88244051e-02 -1.61227629e-01  3.60989422e-01  2.74212211e-01
 -1.38430834e-01 -5.86703680e-02 -4.27209228e-01 -4.38389741e-02
 -1.70130521e-01 -2.79001564e-01 -5.08604087e-02 -2.22016856e-01
  3.46666753e-01 -1.82083368e-01 -1.79737844e-02 -7.58341700e-02
 -2.81926543e-01 -8.88730120e-03 -9.31498557e-02 -1.63259923e-01
  2.63657838e-01  2.68971026e-01 -1.65585026e-01 -1.38222620e-01
 -1.05521366e-01 -2.16932997e-01  1.53523952e-01 -6.68047294e-02
 -1.14297360e-01 -1.98990464e-01  9.14934743e-03  2.44847938e-01
 -1.28512876e-02  1.28243208e-01 -1.80381879e-01  1.76793709e-01
  1.34143546e-01 -9.09292847e-02  1.11920275e-02 -1.59795374e-01
  4.03871059e-01 -2.08633363e-01  2.61400253e-01 -1.77513242e-01
  2.64400899e-01 -3.74767929e-01 -1.41106667e-02  7.07195699e-02
 -2.59310529e-02 -1.52463704e-01 -2.09277481e-01  2.42659152e-01
 -4.37896281e-01 -2.74111658e-01  3.23784024e-01 -1.12194913e-02
 -7.78998435e-03 -2.63834894e-01 -1.86299264e-01 -1.20873287e-01
  2.04141185e-01  1.60952255e-01  3.01728785e-01  2.21852630e-01
  1.73017696e-01  3.71879220e-01  2.80739516e-01 -1.18493669e-01
  1.09062918e-01 -2.55998373e-01  3.87480080e-01  4.10263650e-02
  2.02835858e-01 -3.17999512e-01  3.78140330e-01  2.57947147e-01
 -4.50112194e-01  1.04353130e-01  1.40699465e-03  7.67216682e-02
 -1.11133285e-01  1.40065238e-01 -3.31422426e-02  6.38056397e-02
 -1.78506866e-01  1.02249458e-01 -1.90868706e-01 -2.67677069e-01
  8.56479257e-02  1.85055599e-01  2.22840309e-01 -1.93136290e-01
 -1.12496831e-01  2.01004639e-01 -3.48373577e-02 -1.59934118e-01
  2.03856632e-01  2.03415796e-01  1.91151500e-02 -2.14736730e-01
 -2.01674789e-01 -1.03147745e-01  2.00410113e-01  6.62044063e-02
 -5.02013564e-02 -1.48162618e-01 -1.72178164e-01 -1.56178564e-01
  2.05861077e-01 -2.44715880e-03 -4.20740098e-02  1.99603319e-01
 -3.03595811e-01 -2.62882337e-02  5.65342568e-02  1.27887994e-01
  1.71164036e-01 -1.49366796e-01 -6.41742051e-02  1.59788847e-01
 -1.77765921e-01  1.73824430e-01 -1.28364572e-02  3.04782897e-01
  8.32331404e-02  4.28962141e-01  1.86484933e-01  3.57698888e-01
  1.70930754e-02  1.14737764e-01  1.83813170e-01  7.07985610e-02
  1.53863117e-01  1.45394146e-01  6.10771216e-02 -7.36576989e-02
  5.53056449e-02  1.07640862e-01  8.00179467e-02  3.23191434e-02
  3.09991091e-01  3.02178174e-01 -9.09306332e-02  4.91853952e-02
 -1.28031313e-01 -2.54896469e-02  2.34120145e-01 -1.74658343e-01
 -2.32459098e-01  6.18232414e-02 -2.38484278e-01 -6.28195284e-03
  1.03620939e-01  2.82353729e-01 -6.52055070e-02  7.75345117e-02
 -1.90856859e-01 -1.25437349e-01  1.37492150e-01 -1.64269745e-01
  2.51677603e-01  2.33259335e-01  2.17757478e-01  2.26037607e-01
  4.26998675e-01 -1.67967141e-01  2.59857118e-01  7.12665543e-02
  1.49024948e-01 -4.46223438e-01  1.28380001e-01  1.54407173e-01
 -2.38353878e-01 -4.55187351e-01 -1.71284303e-02 -2.03437835e-01
  4.44028795e-01  1.49454489e-01  2.29066014e-01  5.37483320e-02
  1.16622731e-01  3.27921122e-01  8.48986357e-02  2.98905432e-01
 -1.16589181e-01  3.75571474e-02  2.16130674e-01  5.97562492e-02
  1.57818839e-01 -3.79134119e-01  1.97058737e-01  8.41842405e-03
 -6.73631504e-02  3.26564431e-01  1.02038927e-01  9.84950215e-02
 -4.74490523e-02 -3.58012021e-01 -8.46094415e-02 -2.53597409e-01
  1.31108731e-01 -1.36945480e-02 -2.58189082e-01 -4.04197037e-01
 -9.93263200e-02  7.22196922e-02 -3.21413815e-01  1.38105333e-01
  5.27646877e-02 -3.74778695e-02 -3.09642814e-02  8.79964158e-02
  9.22843367e-02  6.59269169e-02 -4.25867379e-01 -1.13668293e-01
 -2.31259083e-03  1.71499252e-02  4.65329364e-02  2.83751369e-01
 -9.33092088e-06  2.11278170e-01 -1.37060642e-01  4.17382754e-02
 -7.07651116e-03  1.57808796e-01 -6.90133646e-02  2.53847003e-01
 -2.96002537e-01  1.61520377e-01  9.76996198e-02 -2.18553003e-02
 -1.24721631e-01  1.75630823e-01 -6.57364130e-02 -2.27706671e-01
 -1.50046155e-01 -4.29430157e-01 -8.62716325e-03 -3.48888189e-01
  8.30511972e-02  1.19602770e-01 -1.06453761e-01 -3.39551032e-01
  5.58682233e-02 -1.50723115e-01 -8.26126337e-02 -1.29831970e-01
  9.88700613e-02  4.37329859e-01 -1.03127375e-01  2.38669097e-01
 -8.08308348e-02  3.22031081e-02 -4.68674600e-02 -1.37694642e-01
 -3.00011188e-01 -2.28143096e-01  1.68973625e-01 -1.38916388e-01
 -2.30189320e-02  7.20473751e-03  2.29728416e-01 -5.10682501e-02
 -2.43143052e-01  7.35338554e-02 -9.28391144e-02  1.04980737e-01
 -1.35518312e-01 -3.77203338e-02 -6.18087426e-02  3.51022571e-01
 -1.79783516e-02  3.15492228e-02 -5.28395176e-01 -6.82753976e-03
 -7.57957473e-02  5.80136776e-01  7.35457381e-03  2.11273469e-02
  1.24203205e-01 -1.12393416e-01  3.32168788e-01 -2.33854130e-01
 -1.09314382e-01  1.09809563e-01  6.32748082e-02 -3.05012822e-01
 -1.37086868e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/08/2021 14:23:59 - INFO - data_loader -   *** Example ***
10/08/2021 14:23:59 - INFO - data_loader -   guid: test-2
10/08/2021 14:23:59 - INFO - data_loader -   tokens: [CLS] g ##j re ##tar ##d [SEP]
10/08/2021 14:23:59 - INFO - data_loader -   input_ids: 101 1043 3501 2128 7559 2094 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   intent_label: 2 (id = 2)
10/08/2021 14:23:59 - INFO - data_loader -   slot_labels: 0 6 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   ner_embeds: [ 0.14663459 -0.05262458 -0.00995038 -0.09029851  0.12263373 -0.14626154
  0.02043098 -0.1690678   0.03294891 -0.05268421 -0.3358649   0.27530029
 -0.07744435 -0.07584086 -0.02695106 -0.26804128  0.02627846 -0.10347202
  0.2013707  -0.01272027  0.16158915 -0.03340857  0.17287622 -0.18740179
  0.26615396  0.09241544 -0.11608424 -0.27987984 -0.28424925 -0.18137644
 -0.22812925  0.0249024   0.30128998  0.13045995  0.03118261  0.16935138
 -0.06230416  0.17561074  0.00367108 -0.07402565  0.04037194 -0.07837126
 -0.1561325   0.20836185  0.14040159  0.11099035  0.17083055 -0.35949063
  0.03112581 -0.18540204  0.04089637 -0.29396915  0.49076098 -0.05263726
 -0.16885227  0.11559261  0.25719538 -0.1559715  -0.06362429 -0.12877826
  0.06794425 -0.01322358  0.26562798  0.08987913  0.29401389 -0.37333494
 -0.01440139  0.30768135 -0.37712052 -0.108851    0.07931803  0.17231582
 -0.44193396  0.03251904  0.0195513  -0.03582918 -0.13726601 -0.28466982
 -0.33915862 -0.0583167   0.01170416  0.32957253  0.08330661  0.28316665
  0.00079225  0.41026247  0.02413866 -0.45413318 -0.1344302  -0.09248621
  0.51263183 -0.13745138 -0.11134322 -0.20985258  0.39278489 -0.1011213
  0.18303782 -0.12043855  0.26268336 -0.0461308   0.30422714  0.07902091
 -0.15427054  0.18763816 -0.00932948  0.02431204  0.30051631 -0.24693149
  0.11975729 -0.0125444   0.13902089 -0.25323379 -0.1082405   0.42633086
  0.19724542 -0.07689937 -0.1337288   0.27917713 -0.0091875   0.25963399
 -0.14592938 -0.05517824  0.0350663   0.20550938  0.11277337 -0.2699483
  0.01591763 -0.00071367  0.07296164  0.02445946  0.14451511 -0.04559916
 -0.17699969 -0.42008832  0.10587756  0.09828436  0.28259453 -0.0146672
 -0.11086253 -0.35100189  0.04879292 -0.0834544   0.16815855 -0.13103473
 -0.02294332  0.36864796  0.14379583  0.44768068 -0.23370998  0.152519
  0.04665052 -0.17252302  0.31976756 -0.16741526 -0.25466728 -0.26076594
  0.00063117 -0.05335071  0.15843306 -0.26611817  0.05734076 -0.08238135
  0.18893163 -0.20564257 -0.16225532 -0.20887086  0.08916163 -0.01871778
  0.12805638  0.26127747 -0.0709104  -0.27082589 -0.1627634   0.13914403
  0.12214889 -0.1563787  -0.24772033 -0.06986653  0.2272698  -0.05595371
 -0.00991442 -0.20372061  0.04004419 -0.04990653  0.43829471  0.11963772
 -0.12598085  0.24185398 -0.04867083 -0.08653479 -0.26793903  0.13082959
 -0.43375427 -0.13357146  0.19298363 -0.22029646  0.38383847 -0.07818928
 -0.0058459  -0.32991308 -0.08181709  0.14099021  0.09491039  0.21827479
 -0.00399596 -0.08103576  0.13853465  0.01428204  0.44172066  0.10182345
  0.14742956 -0.14591892  0.33454806  0.04376031  0.32970932  0.15365525
 -0.22210287 -0.16479632 -0.28103629  0.00301148  0.01037944  0.20718177
 -0.2371673   0.33999005 -0.13203178 -0.26044402 -0.22456646 -0.09428472
  0.12473079 -0.10682054  0.08118179  0.18629588 -0.10320079  0.10400119
 -0.36503249  0.0367523  -0.26162294 -0.13888274 -0.00607623  0.1842383
 -0.33304039  0.17386849 -0.0694743   0.03439113  0.08467512  0.23954627
  0.3646954  -0.02856011 -0.21535675  0.18716544  0.25370413 -0.18916315
  0.1572178  -0.06640452  0.13099778  0.01409951 -0.31194866 -0.14290154
 -0.1712914  -0.32875261 -0.09570646  0.28620249 -0.23103324  0.22911115
  0.02136808 -0.04183525  0.2437249  -0.1248548  -0.05797715  0.476778
 -0.21734898  0.23745687  0.26735842  0.27674222  0.07875728 -0.16760489
 -0.46723711 -0.08130792  0.09524607 -0.19139667  0.13001987  0.25027886
 -0.01569632  0.08894423 -0.10037478  0.12834264 -0.4296048   0.41507876
  0.2195386  -0.00381741 -0.146727    0.23849073  0.10347191  0.17253159
 -0.4031671   0.19611832  0.17422171  0.3443523  -0.18065456  0.04004083
  0.10621159 -0.19717942  0.25161341 -0.0291713   0.11438692  0.13997085
  0.00901562 -0.24855603 -0.01550403] [ 1.15728311e-01  1.77597135e-01  9.78665426e-02  9.35021695e-03
 -1.35918409e-02 -2.26849690e-01  1.18382862e-02 -2.56954581e-01
 -5.13902195e-02  1.40946638e-03 -4.94093373e-02  1.18371047e-01
 -1.84925720e-01 -8.44093263e-02  4.49686438e-01  1.53657928e-01
  3.77811283e-01  1.40856951e-01  3.34679395e-01 -9.27605778e-02
  1.90596506e-01  1.37804642e-01  5.74968576e-01  3.06187868e-01
 -4.57290672e-02  5.10981083e-01 -1.37720723e-02  5.54463901e-02
 -2.09474489e-01 -2.24890895e-02  2.06912577e-01 -2.23312065e-01
  5.12205213e-02 -1.03564046e-01 -1.52333099e-02  1.14219628e-01
  1.89124681e-02 -1.34228885e-01  6.21379502e-02  5.52005887e-01
  2.78982203e-02 -3.81498821e-02 -9.20988172e-02 -2.05262497e-01
 -2.39167690e-01 -1.24010563e-01  3.38460952e-01  3.79649550e-01
 -8.00761059e-02 -1.65345937e-01  4.62901331e-02  1.34307310e-01
 -8.93177018e-02  1.36742458e-01  1.43562376e-01 -6.95254356e-02
 -6.75170794e-02 -3.24381553e-02 -1.17000729e-01  8.63842219e-02
  2.15452582e-01  1.70528680e-01  7.62154302e-03 -1.47381678e-01
 -7.75146410e-02  2.16678649e-01 -2.80700177e-01 -1.26496598e-01
  2.57435739e-01 -1.58114862e-02 -6.33684173e-02 -7.26924837e-02
  2.93953329e-01 -1.93264723e-01  2.35854328e-01  1.19563662e-01
 -5.40720105e-01 -1.57818317e-01 -6.54052049e-02  9.80154648e-02
 -1.77620277e-01 -1.29614875e-01 -2.48950690e-01 -1.30409403e-02
  2.78265953e-01  2.55372394e-02 -3.24787587e-01  2.46266142e-01
 -1.97395161e-01 -2.58974373e-01 -1.57134071e-01 -3.94944660e-02
 -8.38296935e-02  1.11087754e-01 -1.73248723e-01  2.38484859e-01
  5.44604547e-02 -8.48035216e-02  1.72307596e-01  3.70252311e-01
  5.18996678e-02  1.40496552e-01  1.08579565e-02  1.19191870e-01
 -2.88739540e-02 -2.12167010e-01  2.73477286e-02  3.28472704e-01
  3.01707238e-01  1.78139716e-01  1.26514703e-01  1.21987700e-01
 -1.14315830e-01  1.33378685e-01 -4.32196558e-01 -2.75271535e-01
  2.89359033e-01 -1.18777893e-01 -3.54972094e-01 -2.25475833e-01
 -8.98689255e-02  3.21235180e-01 -3.19600292e-02  1.03462711e-01
  4.30066399e-02  2.28780925e-01 -3.01137537e-01 -7.97446296e-02
 -9.82604325e-02  2.90591538e-01 -2.50160128e-01  1.21152498e-01
  1.18469104e-01 -2.46003997e-02 -9.17403400e-02  1.14537127e-01
  1.66852936e-01  8.64968225e-02 -6.54468015e-02  1.56751454e-01
 -1.49401287e-02  2.96123683e-01  1.05242789e-01  3.95021260e-01
 -4.76788431e-02 -2.36750498e-01 -4.21418529e-03 -8.94557536e-02
  1.48035347e-01 -1.03222214e-01 -1.31954610e-01 -3.14615995e-01
  5.66686653e-02  4.71622720e-02  1.61225602e-01 -1.51893124e-01
  1.83890298e-01  4.07511085e-01 -4.62750584e-01 -2.47511163e-01
 -3.98265183e-01  3.08057163e-02 -3.76644254e-01 -2.44004309e-01
 -2.44423419e-01  2.52762828e-02  2.32896671e-01 -2.16780063e-02
  3.83463889e-01 -2.86124289e-01  1.37446731e-01  6.12318458e-04
 -1.45013243e-01 -2.04386488e-01  1.65950224e-01 -1.99269503e-01
  3.60510081e-01 -6.13480091e-01  5.50649285e-01 -9.35417265e-02
  2.51767367e-01  1.61028966e-01 -3.07173014e-01 -6.73479065e-02
 -1.69519454e-01 -3.18256095e-02 -1.32433221e-01  2.54277498e-01
  5.03651381e-01  4.45105314e-01  2.06946626e-01 -4.53757048e-02
 -2.73640126e-01  5.06410003e-01  9.46833938e-02  1.16605759e-02
 -3.17809790e-01  6.68818802e-02 -1.61849141e-01  2.90378839e-01
  1.27863884e-01 -3.14603567e-01  2.81753410e-02  5.67365959e-02
  5.47064580e-02 -2.79468179e-01 -1.19964376e-01  1.97410226e-01
 -2.23409176e-01 -8.02040324e-02  5.03005423e-02 -2.28323326e-01
 -2.30373871e-02 -5.19477248e-01 -7.32984692e-02 -1.01873595e-02
 -5.41328967e-01  5.84599137e-01  9.59441289e-02  2.64189895e-02
  2.92774558e-01  7.80309439e-02  8.62343535e-02 -3.26920420e-01
  1.40340894e-01 -2.93459654e-01 -4.22757119e-01 -1.47550657e-01
  4.16889995e-01 -4.34097610e-02 -2.18695514e-02  3.57480980e-02
 -2.98597813e-01  1.41072050e-01 -2.38297045e-01 -6.92617148e-02
  1.08876713e-01  4.47213620e-01  1.98089182e-01  1.34139091e-01
 -1.75462496e-02 -1.16427600e-01 -1.31776497e-01 -9.73361358e-02
 -3.87905538e-02  1.69774994e-01  4.51519132e-01  1.86718136e-01
  2.87562758e-01 -1.89130574e-01  2.72975057e-01  8.57977420e-02
  1.84940398e-01  5.80099737e-03 -1.62797704e-01  1.15748771e-01
  3.16530019e-01 -4.02720749e-01 -7.97474608e-02  1.11601152e-01
  1.96475759e-02 -5.01249805e-02 -1.29948482e-01 -3.08780730e-01
  3.00753415e-01  2.46568143e-01  2.57944852e-01  1.34216264e-01
  2.24234127e-02  6.11880943e-02  1.52428761e-01  2.01945394e-01
  2.18464211e-01  1.60142019e-01 -6.85774535e-02 -1.16614670e-01
  1.42051578e-01  1.45031154e-01  1.51764125e-01 -4.22869325e-01
 -3.18583280e-01  1.21847227e-01 -4.10386622e-01  1.36003628e-01
  8.28140751e-02 -3.32349271e-01  1.86447904e-01 -1.76716357e-01
 -8.24048743e-02  1.52571335e-01  2.37396602e-02 -9.23199754e-04
  4.72339034e-01 -1.37484595e-01 -3.40662390e-01  5.84405899e-01
 -1.19777091e-01 -4.52686399e-01  1.66595295e-01  1.68234721e-01
 -1.97954420e-02  1.31061271e-01 -2.19053715e-01  3.62627119e-01
 -4.42025244e-01  4.31505293e-01  5.12025952e-02 -5.25085777e-02
  5.98412037e-01] [ 1.70408890e-01  1.44198924e-01  9.75735113e-02 -4.20796610e-02
  9.23748128e-04 -2.41188765e-01  3.76508720e-02 -1.96202636e-01
 -2.92958114e-02  2.81893276e-03 -9.88186747e-02  2.36742094e-01
 -3.69851440e-01 -1.68818653e-01  8.99372876e-01  3.07315856e-01
  7.55622566e-01  2.81713903e-01  6.69358790e-01 -1.85521156e-01
  3.81193012e-01  2.75609285e-01  1.14993715e+00  6.12375736e-01
 -9.14581344e-02  1.02196217e+00 -2.75441445e-02  1.10892780e-01
 -4.18948978e-01 -4.49781790e-02  4.13825154e-01 -4.46624130e-01
  1.02441043e-01 -2.07128093e-01 -3.04666199e-02  2.28439257e-01
  3.78249362e-02 -2.68457770e-01  1.24275900e-01  1.10401177e+00
  5.57964407e-02 -7.62997642e-02 -1.84197634e-01 -4.10524994e-01
 -4.78335381e-01 -2.48021126e-01  6.76921904e-01  7.59299099e-01
 -1.60152212e-01 -3.30691874e-01  9.25802663e-02  2.68614620e-01
 -1.78635404e-01  2.73484915e-01  2.87124753e-01 -1.39050871e-01
 -1.35034159e-01 -6.48763105e-02 -2.34001458e-01  1.72768444e-01
  4.30905163e-01  3.41057360e-01  1.52430860e-02 -2.94763356e-01
 -1.55029282e-01  4.33357298e-01 -5.61400354e-01 -2.52993196e-01
  5.14871478e-01 -3.16229723e-02 -1.26736835e-01 -1.45384967e-01
  5.87906659e-01 -3.86529446e-01  4.71708655e-01  2.39127323e-01
 -1.08144021e+00 -3.15636635e-01 -1.30810410e-01  1.96030930e-01
 -3.55240554e-01 -2.59229749e-01 -4.97901380e-01 -2.60818806e-02
  5.56531906e-01  5.10744788e-02 -6.49575174e-01  4.92532283e-01
 -3.94790322e-01 -5.17948747e-01 -3.14268142e-01 -7.89889321e-02
 -1.67659387e-01  2.22175509e-01 -3.46497446e-01  4.76969719e-01
  1.08920909e-01 -1.69607043e-01  3.44615191e-01  7.40504622e-01
  1.03799336e-01  2.80993104e-01  2.17159130e-02  2.38383740e-01
 -5.77479079e-02 -4.24334019e-01  5.46954572e-02  6.56945407e-01
  6.03414476e-01  3.56279433e-01  2.53029406e-01  2.43975401e-01
 -2.28631660e-01  2.66757369e-01 -8.64393115e-01 -5.50543070e-01
  5.78718066e-01 -2.37555787e-01 -7.09944189e-01 -4.50951666e-01
 -1.79737851e-01  6.42470360e-01 -6.39200583e-02  2.06925422e-01
  8.60132799e-02  4.57561851e-01 -6.02275074e-01 -1.59489259e-01
 -1.96520865e-01  5.81183076e-01 -5.00320256e-01  2.42304996e-01
  2.36938208e-01 -4.92007993e-02 -1.83480680e-01  2.29074255e-01
  3.33705872e-01  1.72993645e-01 -1.30893603e-01  3.13502908e-01
 -2.98802573e-02  5.92247367e-01  2.10485578e-01  7.90042520e-01
 -9.53576863e-02 -4.73500997e-01 -8.42837058e-03 -1.78911507e-01
  2.96070695e-01 -2.06444427e-01 -2.63909221e-01 -6.29231989e-01
  1.13337331e-01  9.43245441e-02  3.22451204e-01 -3.03786248e-01
  3.67780596e-01  8.15022171e-01 -9.25501168e-01 -4.95022327e-01
 -7.96530366e-01  6.16114326e-02 -7.53288507e-01 -4.88008618e-01
 -4.88846838e-01  5.05525656e-02  4.65793341e-01 -4.33560126e-02
  7.66927779e-01 -5.72248578e-01  2.74893463e-01  1.22463692e-03
 -2.90026486e-01 -4.08772975e-01  3.31900448e-01 -3.98539007e-01
  7.21020162e-01 -1.22696018e+00  1.10129857e+00 -1.87083453e-01
  5.03534734e-01  3.22057933e-01 -6.14346027e-01 -1.34695813e-01
 -3.39038908e-01 -6.36512190e-02 -2.64866441e-01  5.08554995e-01
  1.00730276e+00  8.90210629e-01  4.13893253e-01 -9.07514095e-02
 -5.47280252e-01  1.01282001e+00  1.89366788e-01  2.33211517e-02
 -6.35619581e-01  1.33763760e-01 -3.23698282e-01  5.80757678e-01
  2.55727768e-01 -6.29207134e-01  5.63506819e-02  1.13473192e-01
  1.09412916e-01 -5.58936357e-01 -2.39928752e-01  3.94820452e-01
 -4.46818352e-01 -1.60408065e-01  1.00601085e-01 -4.56646651e-01
 -4.60747741e-02 -1.03895450e+00 -1.46596938e-01 -2.03747191e-02
 -1.08265793e+00  1.16919827e+00  1.91888258e-01  5.28379790e-02
  5.85549116e-01  1.56061888e-01  1.72468707e-01 -6.53840840e-01
  2.80681789e-01 -5.86919308e-01 -8.45514238e-01 -2.95101315e-01
  8.33779991e-01 -8.68195221e-02 -4.37391028e-02  7.14961961e-02
 -5.97195625e-01  2.82144099e-01 -4.76594090e-01 -1.38523430e-01
  2.17753425e-01  8.94427240e-01  3.96178365e-01  2.68278182e-01
 -3.50924991e-02 -2.32855201e-01 -2.63552994e-01 -1.94672272e-01
 -7.75811076e-02  3.39549989e-01  9.03038263e-01  3.73436272e-01
  5.75125515e-01 -3.78261149e-01  5.45950115e-01  1.71595484e-01
  3.69880795e-01  1.16019947e-02 -3.25595409e-01  2.31497541e-01
  6.33060038e-01 -8.05441499e-01 -1.59494922e-01  2.23202303e-01
  3.92951518e-02 -1.00249961e-01 -2.59896964e-01 -6.17561460e-01
  6.01506829e-01  4.93136287e-01  5.15889704e-01  2.68432528e-01
  4.48468253e-02  1.22376189e-01  3.04857522e-01  4.03890789e-01
  4.36928421e-01  3.20284039e-01 -1.37154907e-01 -2.33229339e-01
  2.84103155e-01  2.90062308e-01  3.03528249e-01 -8.45738649e-01
 -6.37166560e-01  2.43694454e-01 -8.20773244e-01  2.72007257e-01
  1.65628150e-01 -6.64698541e-01  3.72895807e-01 -3.53432715e-01
 -1.64809749e-01  3.05142671e-01  4.74793203e-02 -1.84639951e-03
  9.44678068e-01 -2.74969190e-01 -6.81324780e-01  1.16881180e+00
 -2.39554182e-01 -9.05372798e-01  3.33190590e-01  3.36469442e-01
 -3.95908840e-02  2.62122542e-01 -4.38107431e-01  7.25254238e-01
 -8.84050488e-01  8.63010585e-01  1.02405190e-01 -1.05017155e-01
  1.19682407e+00] [ 0.16360979  0.1795129   0.16120473 -0.00198991 -0.07130799 -0.20982495
  0.07380834 -0.14492798  0.00642991 -0.21154545 -0.22631861  0.06008144
 -0.12986203 -0.31814745 -0.22124118 -0.13794801  0.20369573 -0.27594209
  0.18182343 -0.14437899 -0.03500564  0.11376508  0.30255896 -0.24528915
  0.13069499  0.21221241 -0.16237302 -0.18709511 -0.13197549 -0.08420475
 -0.4825947   0.24997365  0.09100696  0.04506109 -0.03977383  0.17381804
 -0.16772971  0.37654877  0.10504672  0.17095646 -0.07275156  0.02848515
 -0.00743797 -0.0729891   0.06223325 -0.07832859  0.07787851 -0.31602117
 -0.11101677 -0.30994791 -0.03409236  0.15154618  0.32150829  0.17514074
  0.14693658 -0.08992268  0.14557299 -0.10805468 -0.24037085 -0.01084208
 -0.42901623  0.07918528  0.03691786 -0.03015061 -0.06451776 -0.37968171
 -0.06249082 -0.02077553 -0.24341947 -0.10314665 -0.02890908 -0.30179584
  0.29088378  0.38927007  0.11858358  0.23560971  0.18471342  0.11691689
  0.23688774 -0.35126948 -0.01474785 -0.18145707  0.13524492 -0.04292936
  0.38949808  0.3252854  -0.27741945  0.34283072 -0.24345921 -0.13242739
  0.01272195 -0.02866835  0.32580835 -0.06273761  0.52419287 -0.04481154
 -0.35054383 -0.25388536  0.189786   -0.22719781  0.15896536  0.13935295
 -0.15597202  0.26440692 -0.20755407  0.01804159  0.08387475 -0.00344159
  0.44464919  0.03293199  0.08971426 -0.14758687  0.05056982  0.11590884
 -0.26884285 -0.1253404  -0.02860777  0.09005532  0.02696362 -0.07427597
  0.00915706 -0.08427375  0.18359305 -0.12138417  0.27221474  0.08940724
  0.22159582  0.04479841  0.34613267 -0.00217501  0.0562367  -0.1780899
 -0.08804107  0.28657371  0.44544744 -0.49013847  0.09091762 -0.01353613
  0.28641993 -0.0900179  -0.12740028 -0.28933817  0.1034326  -0.0872153
  0.30488893 -0.30301842  0.08757377 -0.03417611 -0.14110012  0.18713289
 -0.36280489 -0.25918555 -0.11171734 -0.18669835  0.0383372  -0.44443268
  0.04440751 -0.32296515 -0.23854087  0.42957959  0.12648916 -0.1911972
 -0.11099473 -0.15701349  0.4479734  -0.0880049   0.16652235 -0.13106294
  0.10547107 -0.24512546  0.10940223 -0.01913531  0.08333457 -0.09987427
  0.0357654  -0.11525851 -0.13140814  0.2332945   0.12658586 -0.24638136
 -0.07021611  0.15842457  0.04532009 -0.2966443  -0.21115203 -0.15443805
  0.18776372 -0.14338338 -0.41242382 -0.35219449  0.30265188  0.11375996
 -0.07446325  0.40213302 -0.44951454  0.05497991  0.20292242  0.2284746
 -0.07971102 -0.2466311   0.26455975 -0.11781365 -0.28395134 -0.0987025
 -0.12532336 -0.15580979  0.16606769 -0.07631826  0.08824456 -0.40034905
  0.01970167 -0.00501613 -0.07410132  0.24170785 -0.01973761  0.20123664
  0.07834967  0.31936881 -0.19147611 -0.01531443  0.10407995  0.42192504
 -0.44610548  0.11181364 -0.4689374  -0.10927644 -0.35030499  0.01725727
  0.0202181   0.25608268 -0.02376555  0.19453654  0.03723262  0.38715371
  0.14590117  0.2155768  -0.18933438  0.23930219 -0.30325782 -0.0671481
 -0.12572032 -0.15391725 -0.32974824  0.09062891  0.16583955  0.44625014
  0.19811636  0.00631013  0.06586231  0.1721404   0.20585325  0.16219538
 -0.21264823  0.01818033 -0.00423951  0.25991115 -0.25865677 -0.25009802
  0.12255074 -0.04507878  0.32379743  0.11941424  0.25509763  0.44031754
  0.07058754  0.11399724  0.07183474  0.27992055 -0.44054934 -0.40732613
  0.01897169  0.46428016  0.00610021 -0.07305834 -0.0655682  -0.1941466
 -0.0776193  -0.35067666  0.02435097 -0.12636994 -0.27847332 -0.45441824
 -0.18533428 -0.03784398  0.13601738 -0.24842317  0.0595407   0.13053709
 -0.30562291 -0.28269649  0.06526452  0.28783157  0.28331134  0.30845502
 -0.13197875 -0.00477505  0.11109827 -0.26825145  0.18879157  0.13052925
  0.56891239 -0.20204102 -0.11831819 -0.22432251 -0.29666209  0.45066938
  0.10701344  0.20111175  0.11382008] [ 0.16399461  0.06058935  0.05119022 -0.03451009  0.14651701 -0.198174
  0.00187353 -0.26797312 -0.04779137 -0.4230909  -0.45263723  0.12016287
 -0.25972405 -0.6362949  -0.44248235 -0.27589601  0.40739146 -0.55188417
  0.36364686 -0.28875798 -0.07001129  0.22753015  0.60511792 -0.49057829
  0.26138997  0.42442483 -0.32474604 -0.37419021 -0.26395097 -0.1684095
 -0.9651894   0.49994731  0.18201391  0.09012219 -0.07954765  0.34763607
 -0.33545941  0.75309753  0.21009344  0.34191293 -0.14550312  0.0569703
 -0.01487595 -0.1459782   0.1244665  -0.15665717  0.15575702 -0.63204235
 -0.22203355 -0.61989582 -0.06818472  0.30309236  0.64301658  0.35028148
  0.29387316 -0.17984536  0.29114598 -0.21610935 -0.48074171 -0.02168415
 -0.85803246  0.15837057  0.07383572 -0.06030123 -0.12903552 -0.75936341
 -0.12498165 -0.04155107 -0.48683894 -0.2062933  -0.05781817 -0.60359168
  0.58176756  0.77854013  0.23716716  0.47121942  0.36942685  0.23383379
  0.47377548 -0.70253897 -0.0294957  -0.36291414  0.27048984 -0.08585872
  0.77899617  0.65057081 -0.5548389   0.68566144 -0.48691842 -0.26485479
  0.02544389 -0.0573367   0.65161669 -0.12547523  1.04838574 -0.08962307
 -0.70108765 -0.50777072  0.379572   -0.45439562  0.31793073  0.27870589
 -0.31194404  0.52881384 -0.41510814  0.03608319  0.16774949 -0.00688317
  0.88929838  0.06586399  0.17942852 -0.29517373  0.10113964  0.23181768
 -0.53768569 -0.2506808  -0.05721554  0.18011065  0.05392723 -0.14855194
  0.01831411 -0.1685475   0.3671861  -0.24276833  0.54442948  0.17881447
  0.44319165  0.08959682  0.69226533 -0.00435002  0.11247339 -0.3561798
 -0.17608213  0.57314742  0.89089489 -0.98027694  0.18183523 -0.02707225
  0.57283986 -0.1800358  -0.25480056 -0.57867634  0.20686519 -0.17443061
  0.60977787 -0.60603684  0.17514755 -0.06835222 -0.28220025  0.37426579
 -0.72560978 -0.51837111 -0.22343467 -0.37339669  0.07667441 -0.88886535
  0.08881503 -0.64593029 -0.47708175  0.85915917  0.25297832 -0.3823944
 -0.22198945 -0.31402698  0.8959468  -0.1760098   0.33304471 -0.26212588
  0.21094213 -0.49025092  0.21880446 -0.03827062  0.16666913 -0.19974853
  0.0715308  -0.23051701 -0.26281628  0.466589    0.25317171 -0.49276271
 -0.14043222  0.31684914  0.09064019 -0.5932886  -0.42230406 -0.3088761
  0.37552744 -0.28676677 -0.82484764 -0.70438898  0.60530376  0.22751993
 -0.1489265   0.80426604 -0.89902908  0.10995981  0.40584484  0.4569492
 -0.15942204 -0.4932622   0.52911949 -0.23562731 -0.56790268 -0.197405
 -0.25064671 -0.31161958  0.33213538 -0.15263651  0.17648913 -0.8006981
  0.03940335 -0.01003226 -0.14820264  0.48341569 -0.03947523  0.40247327
  0.15669934  0.63873762 -0.38295221 -0.03062887  0.20815989  0.84385008
 -0.89221096  0.22362728 -0.93787479 -0.21855287 -0.70060998  0.03451455
  0.0404362   0.51216537 -0.0475311   0.38907307  0.07446525  0.77430743
  0.29180235  0.4311536  -0.37866876  0.47860438 -0.60651565 -0.13429619
 -0.25144064 -0.30783451 -0.65949649  0.18125783  0.33167911  0.89250028
  0.39623272  0.01262026  0.13172463  0.34428081  0.41170651  0.32439077
 -0.42529646  0.03636065 -0.00847903  0.5198223  -0.51731354 -0.50019604
  0.24510148 -0.09015756  0.64759487  0.23882848  0.51019526  0.88063508
  0.14117508  0.22799449  0.14366947  0.5598411  -0.88109869 -0.81465226
  0.03794337  0.92856032  0.01220042 -0.14611669 -0.1311364  -0.38829321
 -0.1552386  -0.70135331  0.04870194 -0.25273988 -0.55694664 -0.90883648
 -0.37066856 -0.07568796  0.27203476 -0.49684635  0.1190814   0.26107419
 -0.61124581 -0.56539297  0.13052903  0.57566315  0.56662267  0.61691004
 -0.2639575  -0.00955009  0.22219655 -0.5365029   0.37758315  0.26105851
  1.13782477 -0.40408203 -0.23663639 -0.44864503 -0.59332418  0.90133876
  0.21402688  0.4022235   0.22764017] [ 0.22164738  0.16209549  0.16796629  0.04259393  0.00421151 -0.10542212
  0.00639763 -0.21556678  0.10225053 -0.63463636 -0.67895584  0.18024431
 -0.38958608 -0.95444235 -0.66372353 -0.41384402  0.61108719 -0.82782626
  0.5454703  -0.43313697 -0.10501693  0.34129523  0.90767688 -0.73586744
  0.39208496  0.63663724 -0.48711906 -0.56128532 -0.39592646 -0.25261424
 -1.4477841   0.74992096  0.27302087  0.13518328 -0.11932148  0.52145411
 -0.50318912  1.1296463   0.31514016  0.51286939 -0.21825468  0.08545545
 -0.02231392 -0.2189673   0.18669975 -0.23498576  0.23363554 -0.94806352
 -0.33305032 -0.92984372 -0.10227708  0.45463854  0.96452487  0.52542222
  0.44080974 -0.26976804  0.43671897 -0.32416403 -0.72111256 -0.03252623
 -1.2870487   0.23755585  0.11075357 -0.09045184 -0.19355328 -1.13904512
 -0.18747247 -0.0623266  -0.73025841 -0.30943995 -0.08672725 -0.90538752
  0.87265134  1.1678102   0.35575075  0.70682913  0.55414027  0.35075068
  0.71066321 -1.05380845 -0.04424356 -0.54437122  0.40573476 -0.12878808
  1.16849425  0.97585621 -0.83225834  1.02849215 -0.73037763 -0.39728218
  0.03816584 -0.08600505  0.97742504 -0.18821284  1.57257861 -0.13443461
 -1.05163148 -0.76165608  0.56935801 -0.68159343  0.47689609  0.41805884
 -0.46791606  0.79322076 -0.62266222  0.05412478  0.25162424 -0.01032476
  1.33394757  0.09879598  0.26914278 -0.4427606   0.15170946  0.34772652
 -0.80652854 -0.37602121 -0.08582331  0.27016597  0.08089085 -0.22282791
  0.02747117 -0.25282124  0.55077915 -0.3641525   0.81664422  0.26822171
  0.66478747  0.13439523  1.038398   -0.00652503  0.16871009 -0.53426971
 -0.2641232   0.85972112  1.33634233 -1.47041541  0.27275285 -0.04060838
  0.85925978 -0.2700537  -0.38220084 -0.86801451  0.31029779 -0.26164591
  0.9146668  -0.90905526  0.26272132 -0.10252833 -0.42330037  0.56139868
 -1.08841467 -0.77755666 -0.33515201 -0.56009504  0.11501161 -1.33329803
  0.13322254 -0.96889544 -0.71562262  1.28873876  0.37946749 -0.5735916
 -0.33298418 -0.47104047  1.3439202  -0.26401471  0.49956706 -0.39318882
  0.3164132  -0.73537637  0.3282067  -0.05740592  0.2500037  -0.2996228
  0.10729621 -0.34577552 -0.39422442  0.69988351  0.37975757 -0.73914407
 -0.21064834  0.47527371  0.13596028 -0.8899329  -0.6334561  -0.46331415
  0.56329116 -0.43015015 -1.23727146 -1.05658346  0.90795565  0.34127989
 -0.22338974  1.20639905 -1.34854361  0.16493972  0.60876726  0.68542381
 -0.23913306 -0.7398933   0.79367924 -0.35344096 -0.85185403 -0.29610749
 -0.37597007 -0.46742937  0.49820307 -0.22895477  0.26473369 -1.20104715
  0.05910502 -0.01504838 -0.22230396  0.72512354 -0.05921284  0.60370991
  0.23504902  0.95810643 -0.57442832 -0.0459433   0.31223984  1.26577511
 -1.33831644  0.33544093 -1.40681219 -0.32782931 -1.05091497  0.05177182
  0.06065431  0.76824805 -0.07129665  0.58360961  0.11169787  1.16146114
  0.43770352  0.64673039 -0.56800313  0.71790656 -0.90977347 -0.20144429
 -0.37716097 -0.46175176 -0.98924473  0.27188674  0.49751866  1.33875042
  0.59434909  0.0189304   0.19758694  0.51642121  0.61755976  0.48658615
 -0.63794468  0.05454098 -0.01271854  0.77973345 -0.77597031 -0.75029406
  0.36765222 -0.13523634  0.9713923   0.35824272  0.76529288  1.32095262
  0.21176261  0.34199173  0.21550421  0.83976164 -1.32164803 -1.2219784
  0.05691506  1.39284047  0.01830063 -0.21917503 -0.1967046  -0.58243981
 -0.2328579  -1.05202997  0.07305291 -0.37910981 -0.83541995 -1.36325473
 -0.55600284 -0.11353194  0.40805215 -0.74526952  0.1786221   0.39161128
 -0.91686872 -0.84808946  0.19579355  0.86349472  0.84993401  0.92536506
 -0.39593625 -0.01432514  0.33329482 -0.80475435  0.56637472  0.39158776
  1.70673716 -0.60612305 -0.35495458 -0.67296754 -0.88998628  1.35200813
  0.32104032  0.60333525  0.34146025] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  3.23701560e-01  1.23859271e-01  3.17038149e-01
 -1.99837953e-01  1.45933881e-01  1.97479472e-01  1.11766860e-01
 -2.97712028e-01  2.12801516e-01 -8.30430072e-03 -1.36353567e-01
  5.88244051e-02 -1.61227629e-01  3.60989422e-01  2.74212211e-01
 -1.38430834e-01 -5.86703680e-02 -4.27209228e-01 -4.38389741e-02
 -1.70130521e-01 -2.79001564e-01 -5.08604087e-02 -2.22016856e-01
  3.46666753e-01 -1.82083368e-01 -1.79737844e-02 -7.58341700e-02
 -2.81926543e-01 -8.88730120e-03 -9.31498557e-02 -1.63259923e-01
  2.63657838e-01  2.68971026e-01 -1.65585026e-01 -1.38222620e-01
 -1.05521366e-01 -2.16932997e-01  1.53523952e-01 -6.68047294e-02
 -1.14297360e-01 -1.98990464e-01  9.14934743e-03  2.44847938e-01
 -1.28512876e-02  1.28243208e-01 -1.80381879e-01  1.76793709e-01
  1.34143546e-01 -9.09292847e-02  1.11920275e-02 -1.59795374e-01
  4.03871059e-01 -2.08633363e-01  2.61400253e-01 -1.77513242e-01
  2.64400899e-01 -3.74767929e-01 -1.41106667e-02  7.07195699e-02
 -2.59310529e-02 -1.52463704e-01 -2.09277481e-01  2.42659152e-01
 -4.37896281e-01 -2.74111658e-01  3.23784024e-01 -1.12194913e-02
 -7.78998435e-03 -2.63834894e-01 -1.86299264e-01 -1.20873287e-01
  2.04141185e-01  1.60952255e-01  3.01728785e-01  2.21852630e-01
  1.73017696e-01  3.71879220e-01  2.80739516e-01 -1.18493669e-01
  1.09062918e-01 -2.55998373e-01  3.87480080e-01  4.10263650e-02
  2.02835858e-01 -3.17999512e-01  3.78140330e-01  2.57947147e-01
 -4.50112194e-01  1.04353130e-01  1.40699465e-03  7.67216682e-02
 -1.11133285e-01  1.40065238e-01 -3.31422426e-02  6.38056397e-02
 -1.78506866e-01  1.02249458e-01 -1.90868706e-01 -2.67677069e-01
  8.56479257e-02  1.85055599e-01  2.22840309e-01 -1.93136290e-01
 -1.12496831e-01  2.01004639e-01 -3.48373577e-02 -1.59934118e-01
  2.03856632e-01  2.03415796e-01  1.91151500e-02 -2.14736730e-01
 -2.01674789e-01 -1.03147745e-01  2.00410113e-01  6.62044063e-02
 -5.02013564e-02 -1.48162618e-01 -1.72178164e-01 -1.56178564e-01
  2.05861077e-01 -2.44715880e-03 -4.20740098e-02  1.99603319e-01
 -3.03595811e-01 -2.62882337e-02  5.65342568e-02  1.27887994e-01
  1.71164036e-01 -1.49366796e-01 -6.41742051e-02  1.59788847e-01
 -1.77765921e-01  1.73824430e-01 -1.28364572e-02  3.04782897e-01
  8.32331404e-02  4.28962141e-01  1.86484933e-01  3.57698888e-01
  1.70930754e-02  1.14737764e-01  1.83813170e-01  7.07985610e-02
  1.53863117e-01  1.45394146e-01  6.10771216e-02 -7.36576989e-02
  5.53056449e-02  1.07640862e-01  8.00179467e-02  3.23191434e-02
  3.09991091e-01  3.02178174e-01 -9.09306332e-02  4.91853952e-02
 -1.28031313e-01 -2.54896469e-02  2.34120145e-01 -1.74658343e-01
 -2.32459098e-01  6.18232414e-02 -2.38484278e-01 -6.28195284e-03
  1.03620939e-01  2.82353729e-01 -6.52055070e-02  7.75345117e-02
 -1.90856859e-01 -1.25437349e-01  1.37492150e-01 -1.64269745e-01
  2.51677603e-01  2.33259335e-01  2.17757478e-01  2.26037607e-01
  4.26998675e-01 -1.67967141e-01  2.59857118e-01  7.12665543e-02
  1.49024948e-01 -4.46223438e-01  1.28380001e-01  1.54407173e-01
 -2.38353878e-01 -4.55187351e-01 -1.71284303e-02 -2.03437835e-01
  4.44028795e-01  1.49454489e-01  2.29066014e-01  5.37483320e-02
  1.16622731e-01  3.27921122e-01  8.48986357e-02  2.98905432e-01
 -1.16589181e-01  3.75571474e-02  2.16130674e-01  5.97562492e-02
  1.57818839e-01 -3.79134119e-01  1.97058737e-01  8.41842405e-03
 -6.73631504e-02  3.26564431e-01  1.02038927e-01  9.84950215e-02
 -4.74490523e-02 -3.58012021e-01 -8.46094415e-02 -2.53597409e-01
  1.31108731e-01 -1.36945480e-02 -2.58189082e-01 -4.04197037e-01
 -9.93263200e-02  7.22196922e-02 -3.21413815e-01  1.38105333e-01
  5.27646877e-02 -3.74778695e-02 -3.09642814e-02  8.79964158e-02
  9.22843367e-02  6.59269169e-02 -4.25867379e-01 -1.13668293e-01
 -2.31259083e-03  1.71499252e-02  4.65329364e-02  2.83751369e-01
 -9.33092088e-06  2.11278170e-01 -1.37060642e-01  4.17382754e-02
 -7.07651116e-03  1.57808796e-01 -6.90133646e-02  2.53847003e-01
 -2.96002537e-01  1.61520377e-01  9.76996198e-02 -2.18553003e-02
 -1.24721631e-01  1.75630823e-01 -6.57364130e-02 -2.27706671e-01
 -1.50046155e-01 -4.29430157e-01 -8.62716325e-03 -3.48888189e-01
  8.30511972e-02  1.19602770e-01 -1.06453761e-01 -3.39551032e-01
  5.58682233e-02 -1.50723115e-01 -8.26126337e-02 -1.29831970e-01
  9.88700613e-02  4.37329859e-01 -1.03127375e-01  2.38669097e-01
 -8.08308348e-02  3.22031081e-02 -4.68674600e-02 -1.37694642e-01
 -3.00011188e-01 -2.28143096e-01  1.68973625e-01 -1.38916388e-01
 -2.30189320e-02  7.20473751e-03  2.29728416e-01 -5.10682501e-02
 -2.43143052e-01  7.35338554e-02 -9.28391144e-02  1.04980737e-01
 -1.35518312e-01 -3.77203338e-02 -6.18087426e-02  3.51022571e-01
 -1.79783516e-02  3.15492228e-02 -5.28395176e-01 -6.82753976e-03
 -7.57957473e-02  5.80136776e-01  7.35457381e-03  2.11273469e-02
  1.24203205e-01 -1.12393416e-01  3.32168788e-01 -2.33854130e-01
 -1.09314382e-01  1.09809563e-01  6.32748082e-02 -3.05012822e-01
 -1.37086868e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/08/2021 14:23:59 - INFO - data_loader -   *** Example ***
10/08/2021 14:23:59 - INFO - data_loader -   guid: test-3
10/08/2021 14:23:59 - INFO - data_loader -   tokens: [CLS] yu ##p ign ##ite to ks no ##ob act [SEP]
10/08/2021 14:23:59 - INFO - data_loader -   input_ids: 101 9805 2361 16270 4221 2000 29535 2053 16429 2552 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:23:59 - INFO - data_loader -   intent_label: 2 (id = 2)
10/08/2021 14:23:59 - INFO - data_loader -   slot_labels: 0 4 0 3 0 4 4 7 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:24:00 - INFO - data_loader -   ner_embeds: [ 0.14663459 -0.05262458 -0.00995038 -0.09029851  0.12263373 -0.14626154
  0.02043098 -0.1690678   0.03294891 -0.05268421 -0.3358649   0.27530029
 -0.07744435 -0.07584086 -0.02695106 -0.26804128  0.02627846 -0.10347202
  0.2013707  -0.01272027  0.16158915 -0.03340857  0.17287622 -0.18740179
  0.26615396  0.09241544 -0.11608424 -0.27987984 -0.28424925 -0.18137644
 -0.22812925  0.0249024   0.30128998  0.13045995  0.03118261  0.16935138
 -0.06230416  0.17561074  0.00367108 -0.07402565  0.04037194 -0.07837126
 -0.1561325   0.20836185  0.14040159  0.11099035  0.17083055 -0.35949063
  0.03112581 -0.18540204  0.04089637 -0.29396915  0.49076098 -0.05263726
 -0.16885227  0.11559261  0.25719538 -0.1559715  -0.06362429 -0.12877826
  0.06794425 -0.01322358  0.26562798  0.08987913  0.29401389 -0.37333494
 -0.01440139  0.30768135 -0.37712052 -0.108851    0.07931803  0.17231582
 -0.44193396  0.03251904  0.0195513  -0.03582918 -0.13726601 -0.28466982
 -0.33915862 -0.0583167   0.01170416  0.32957253  0.08330661  0.28316665
  0.00079225  0.41026247  0.02413866 -0.45413318 -0.1344302  -0.09248621
  0.51263183 -0.13745138 -0.11134322 -0.20985258  0.39278489 -0.1011213
  0.18303782 -0.12043855  0.26268336 -0.0461308   0.30422714  0.07902091
 -0.15427054  0.18763816 -0.00932948  0.02431204  0.30051631 -0.24693149
  0.11975729 -0.0125444   0.13902089 -0.25323379 -0.1082405   0.42633086
  0.19724542 -0.07689937 -0.1337288   0.27917713 -0.0091875   0.25963399
 -0.14592938 -0.05517824  0.0350663   0.20550938  0.11277337 -0.2699483
  0.01591763 -0.00071367  0.07296164  0.02445946  0.14451511 -0.04559916
 -0.17699969 -0.42008832  0.10587756  0.09828436  0.28259453 -0.0146672
 -0.11086253 -0.35100189  0.04879292 -0.0834544   0.16815855 -0.13103473
 -0.02294332  0.36864796  0.14379583  0.44768068 -0.23370998  0.152519
  0.04665052 -0.17252302  0.31976756 -0.16741526 -0.25466728 -0.26076594
  0.00063117 -0.05335071  0.15843306 -0.26611817  0.05734076 -0.08238135
  0.18893163 -0.20564257 -0.16225532 -0.20887086  0.08916163 -0.01871778
  0.12805638  0.26127747 -0.0709104  -0.27082589 -0.1627634   0.13914403
  0.12214889 -0.1563787  -0.24772033 -0.06986653  0.2272698  -0.05595371
 -0.00991442 -0.20372061  0.04004419 -0.04990653  0.43829471  0.11963772
 -0.12598085  0.24185398 -0.04867083 -0.08653479 -0.26793903  0.13082959
 -0.43375427 -0.13357146  0.19298363 -0.22029646  0.38383847 -0.07818928
 -0.0058459  -0.32991308 -0.08181709  0.14099021  0.09491039  0.21827479
 -0.00399596 -0.08103576  0.13853465  0.01428204  0.44172066  0.10182345
  0.14742956 -0.14591892  0.33454806  0.04376031  0.32970932  0.15365525
 -0.22210287 -0.16479632 -0.28103629  0.00301148  0.01037944  0.20718177
 -0.2371673   0.33999005 -0.13203178 -0.26044402 -0.22456646 -0.09428472
  0.12473079 -0.10682054  0.08118179  0.18629588 -0.10320079  0.10400119
 -0.36503249  0.0367523  -0.26162294 -0.13888274 -0.00607623  0.1842383
 -0.33304039  0.17386849 -0.0694743   0.03439113  0.08467512  0.23954627
  0.3646954  -0.02856011 -0.21535675  0.18716544  0.25370413 -0.18916315
  0.1572178  -0.06640452  0.13099778  0.01409951 -0.31194866 -0.14290154
 -0.1712914  -0.32875261 -0.09570646  0.28620249 -0.23103324  0.22911115
  0.02136808 -0.04183525  0.2437249  -0.1248548  -0.05797715  0.476778
 -0.21734898  0.23745687  0.26735842  0.27674222  0.07875728 -0.16760489
 -0.46723711 -0.08130792  0.09524607 -0.19139667  0.13001987  0.25027886
 -0.01569632  0.08894423 -0.10037478  0.12834264 -0.4296048   0.41507876
  0.2195386  -0.00381741 -0.146727    0.23849073  0.10347191  0.17253159
 -0.4031671   0.19611832  0.17422171  0.3443523  -0.18065456  0.04004083
  0.10621159 -0.19717942  0.25161341 -0.0291713   0.11438692  0.13997085
  0.00901562 -0.24855603 -0.01550403] [ 2.09180534e-01  7.40154535e-02 -2.39711106e-02 -5.58062755e-02
  1.13997512e-01 -2.58368045e-01 -2.97725946e-02 -2.38652557e-01
  3.64222527e-02 -2.11944401e-01 -6.02492318e-02  5.89102320e-02
  2.63783574e-01 -1.47969678e-01  1.79679707e-01  5.65867238e-02
  1.06636457e-01  2.38424629e-01  1.76569551e-01 -8.93405452e-02
 -2.49815062e-01 -1.63611487e-01 -1.56339467e-01 -1.49016440e-01
 -7.84551576e-02 -1.26657695e-01 -6.44370839e-02  8.79683048e-02
  1.62070140e-01 -1.57595828e-01 -2.69628644e-01 -1.34388790e-01
  5.16721047e-02  5.26213422e-02 -9.38221291e-02 -2.96230704e-01
  1.79995000e-01  3.82483184e-01  2.63344020e-01 -3.48576099e-01
  1.05128422e-01 -1.54467463e-01  1.80137321e-01 -4.55754906e-01
  4.26490754e-01 -1.97721533e-02 -6.80136904e-02 -8.47884417e-02
 -2.18972906e-01 -2.28695422e-01  5.45909286e-01  3.73399198e-01
  1.23903014e-01  8.35707486e-02  3.30196470e-01  6.51841015e-02
 -8.10010508e-02  6.08191639e-02 -6.33550644e-01  4.08920608e-02
  2.11312816e-01 -9.27347466e-02  5.83169013e-02  2.07469594e-02
 -1.38078526e-01 -2.49516159e-01  4.41928953e-02  4.33727354e-02
 -1.62622333e-01 -1.23008400e-01  2.55701959e-01  4.47957180e-02
  2.92192370e-01  6.25797212e-02  7.26020411e-02 -1.16119601e-01
  1.63991582e-02 -1.08454609e-02  2.47195005e-01 -2.16674525e-02
  7.27909058e-02  8.36641900e-03 -2.94330101e-02 -1.63789779e-01
 -2.05960467e-01  2.92041022e-02 -1.23351060e-01 -7.65902773e-02
  3.38675141e-01  2.55516469e-01  1.99083015e-01 -1.94510624e-01
 -3.54763269e-01 -4.69212011e-02 -2.74663955e-01  2.38557324e-01
  9.71684232e-02 -1.58273458e-01 -9.92544368e-02  1.76097795e-01
 -2.20565647e-01  5.66653848e-01 -2.65360981e-01  3.00420672e-02
  1.79220349e-01 -1.88911363e-01 -3.75014961e-01 -1.33663923e-01
 -1.72382683e-01 -1.25884056e-01 -1.85093790e-01 -7.07463846e-02
  2.38912925e-02  1.78065360e-01 -1.03194790e-03 -1.11848101e-01
  4.96230051e-02  1.48191929e-01  6.23138398e-02 -3.34635638e-02
  1.15039892e-01 -2.52595127e-01  3.12554628e-01 -8.08448344e-02
  6.45468682e-02 -1.32501662e-01 -1.88524380e-01  1.30412072e-01
 -3.34003508e-01  1.43865481e-01 -1.64917812e-01  2.71062583e-01
 -2.11563885e-01 -1.17206752e-01 -6.01374745e-01 -3.79095823e-01
  2.33625188e-01 -9.49392691e-02  5.36085725e-01  2.57810615e-02
 -2.22991094e-01  2.73210645e-01 -1.98287711e-01  3.40457916e-01
 -1.20994356e-02  2.48375252e-01  1.31117254e-01 -2.39922941e-01
 -1.48461357e-01  1.24466941e-01 -4.92807440e-02  3.27681452e-01
  4.89343517e-02  2.72454083e-01 -5.46936840e-02 -1.52019858e-01
  2.13133290e-01  4.46368486e-01 -3.73324484e-01  3.92895797e-03
  3.61864477e-01  3.20961952e-01  2.61801090e-02 -1.12521060e-01
  1.60140261e-01 -3.51285160e-01  3.19863856e-02  2.06082702e-01
  8.84231031e-02  1.75278485e-01 -3.57989193e-04  2.54418582e-01
 -1.07391000e-01 -3.22090909e-02 -7.38021806e-02 -8.78330916e-02
  9.73861367e-02 -6.51456416e-02 -2.33184382e-01  1.17855638e-01
 -2.86073744e-01 -2.88615644e-01  2.32277155e-01 -2.08455816e-01
 -2.22954303e-01  2.15848058e-01  1.81728676e-01  1.57369673e-01
 -3.87443841e-01 -3.20024133e-01  4.51865532e-02 -1.25856757e-01
 -2.45386779e-01  2.91980773e-01 -4.00056452e-01  6.46462962e-02
 -7.62524875e-03 -1.06175289e-01 -2.88784921e-01 -1.58925727e-01
 -1.17889363e-02 -7.96867013e-02 -1.71787754e-01 -1.83430225e-01
  4.66834754e-01 -7.22562894e-02 -1.60228625e-01  2.93144464e-01
  1.21575311e-01 -4.58581060e-01  3.81794006e-01  2.08971590e-01
 -8.59564245e-02 -1.16030186e-01 -1.77883789e-01 -2.77272224e-01
 -2.31310111e-02  4.58534181e-01 -1.71748981e-01  2.23216247e-02
  4.34440039e-02  4.79586989e-01 -7.69597292e-02 -3.23805101e-02
 -3.42565954e-01 -1.98099017e-01  1.54494017e-01 -6.33683324e-01
 -3.70393135e-02 -3.34016606e-02 -2.23616317e-01  2.16172859e-01
  2.12041680e-02 -3.45547885e-01  3.61323625e-01  2.13434547e-01
  4.32368666e-01 -3.21162939e-02 -2.52521873e-01  2.48177841e-01
  3.70616317e-02  2.64370114e-01  5.07590435e-02  1.12490036e-01
  4.32418346e-01  1.18109807e-01  3.98368001e-01  3.03572603e-02
  3.82600054e-02  1.32180797e-02 -2.08697513e-01  4.29195426e-02
  8.11164305e-02 -7.82399848e-02 -3.41860890e-01 -1.72011912e-01
 -2.45577201e-01  2.80784577e-01 -3.54854435e-01  2.06923246e-01
  4.85302418e-01  3.66126031e-01  4.90014665e-02  1.51881993e-01
 -4.62888591e-02  1.40542895e-01 -4.37343866e-02  2.17056617e-01
  1.28554508e-01 -7.09914342e-02  9.62339062e-03 -3.40324976e-02
  4.38277632e-01  1.96391135e-01  3.97289619e-02  1.04933761e-01
  3.91868085e-01 -1.49333432e-01 -2.53140152e-01 -7.15237483e-02
 -3.80183637e-01 -3.10172699e-02  2.03453571e-01 -8.75254571e-02
  2.91964978e-01 -2.50745356e-01 -2.40878999e-01  6.77874088e-02
 -8.53921250e-02  5.14078140e-01  2.83203453e-01 -4.29050848e-02
 -3.44381817e-02  4.20112342e-01 -1.79593876e-01  6.78459629e-02
  1.86118223e-02 -3.09283167e-01 -5.63365370e-02  4.45288159e-02
 -1.62341073e-01  3.37131381e-01  9.46482420e-02  2.54906043e-02
 -1.52106099e-02 -2.28996817e-02  1.60486132e-01 -1.89236522e-01
  4.52176481e-02] [ 2.19021142e-01  1.63645595e-01  7.23623037e-02 -4.61666845e-02
  6.29020631e-02 -1.86333537e-01 -5.79470061e-02 -4.47601408e-01
  1.38459101e-01 -4.23888803e-01 -1.20498464e-01  1.17820464e-01
  5.27567148e-01 -2.95939356e-01  3.59359413e-01  1.13173448e-01
  2.13272914e-01  4.76849258e-01  3.53139102e-01 -1.78681090e-01
 -4.99630123e-01 -3.27222973e-01 -3.12678933e-01 -2.98032880e-01
 -1.56910315e-01 -2.53315389e-01 -1.28874168e-01  1.75936610e-01
  3.24140280e-01 -3.15191656e-01 -5.39257288e-01 -2.68777579e-01
  1.03344209e-01  1.05242684e-01 -1.87644258e-01 -5.92461407e-01
  3.59990001e-01  7.64966369e-01  5.26688039e-01 -6.97152197e-01
  2.10256845e-01 -3.08934927e-01  3.60274643e-01 -9.11509812e-01
  8.52981508e-01 -3.95443067e-02 -1.36027381e-01 -1.69576883e-01
 -4.37945813e-01 -4.57390845e-01  1.09181857e+00  7.46798396e-01
  2.47806028e-01  1.67141497e-01  6.60392940e-01  1.30368203e-01
 -1.62002102e-01  1.21638328e-01 -1.26710129e+00  8.17841217e-02
  4.22625631e-01 -1.85469493e-01  1.16633803e-01  4.14939187e-02
 -2.76157051e-01 -4.99032319e-01  8.83857906e-02  8.67454708e-02
 -3.25244665e-01 -2.46016800e-01  5.11403918e-01  8.95914361e-02
  5.84384739e-01  1.25159442e-01  1.45204082e-01 -2.32239202e-01
  3.27983163e-02 -2.16909219e-02  4.94390011e-01 -4.33349051e-02
  1.45581812e-01  1.67328380e-02 -5.88660203e-02 -3.27579558e-01
 -4.11920935e-01  5.84082045e-02 -2.46702120e-01 -1.53180555e-01
  6.77350283e-01  5.11032939e-01  3.98166031e-01 -3.89021248e-01
 -7.09526539e-01 -9.38424021e-02 -5.49327910e-01  4.77114648e-01
  1.94336846e-01 -3.16546917e-01 -1.98508874e-01  3.52195591e-01
 -4.41131294e-01  1.13330770e+00 -5.30721962e-01  6.00841343e-02
  3.58440697e-01 -3.77822727e-01 -7.50029922e-01 -2.67327845e-01
 -3.44765365e-01 -2.51768112e-01 -3.70187581e-01 -1.41492769e-01
  4.77825850e-02  3.56130719e-01 -2.06389581e-03 -2.23696202e-01
  9.92460102e-02  2.96383858e-01  1.24627680e-01 -6.69271275e-02
  2.30079785e-01 -5.05190253e-01  6.25109255e-01 -1.61689669e-01
  1.29093736e-01 -2.65003324e-01 -3.77048761e-01  2.60824144e-01
 -6.68007016e-01  2.87730962e-01 -3.29835624e-01  5.42125165e-01
 -4.23127770e-01 -2.34413505e-01 -1.20274949e+00 -7.58191645e-01
  4.67250377e-01 -1.89878538e-01  1.07217145e+00  5.15621230e-02
 -4.45982188e-01  5.46421289e-01 -3.96575421e-01  6.80915833e-01
 -2.41988711e-02  4.96750504e-01  2.62234509e-01 -4.79845881e-01
 -2.96922714e-01  2.48933882e-01 -9.85614881e-02  6.55362904e-01
  9.78687033e-02  5.44908166e-01 -1.09387368e-01 -3.04039717e-01
  4.26266581e-01  8.92736971e-01 -7.46648967e-01  7.85791595e-03
  7.23728955e-01  6.41923904e-01  5.23602180e-02 -2.25042120e-01
  3.20280522e-01 -7.02570319e-01  6.39727712e-02  4.12165403e-01
  1.76846206e-01  3.50556970e-01 -7.15978385e-04  5.08837163e-01
 -2.14782000e-01 -6.44181818e-02 -1.47604361e-01 -1.75666183e-01
  1.94772273e-01 -1.30291283e-01 -4.66368765e-01  2.35711277e-01
 -5.72147489e-01 -5.77231288e-01  4.64554310e-01 -4.16911632e-01
 -4.45908606e-01  4.31696117e-01  3.63457352e-01  3.14739347e-01
 -7.74887681e-01 -6.40048265e-01  9.03731063e-02 -2.51713514e-01
 -4.90773559e-01  5.83961546e-01 -8.00112903e-01  1.29292592e-01
 -1.52504975e-02 -2.12350577e-01 -5.77569842e-01 -3.17851454e-01
 -2.35778727e-02 -1.59373403e-01 -3.43575507e-01 -3.66860449e-01
  9.33669508e-01 -1.44512579e-01 -3.20457250e-01  5.86288929e-01
  2.43150622e-01 -9.17162120e-01  7.63588011e-01  4.17943180e-01
 -1.71912849e-01 -2.32060373e-01 -3.55767578e-01 -5.54544449e-01
 -4.62620221e-02  9.17068362e-01 -3.43497962e-01  4.46432494e-02
  8.68880078e-02  9.59173977e-01 -1.53919458e-01 -6.47610202e-02
 -6.85131907e-01 -3.96198034e-01  3.08988035e-01 -1.26736665e+00
 -7.40786269e-02 -6.68033212e-02 -4.47232634e-01  4.32345718e-01
  4.24083360e-02 -6.91095769e-01  7.22647250e-01  4.26869094e-01
  8.64737332e-01 -6.42325878e-02 -5.05043745e-01  4.96355683e-01
  7.41232634e-02  5.28740227e-01  1.01518087e-01  2.24980071e-01
  8.64836693e-01  2.36219615e-01  7.96736002e-01  6.07145205e-02
  7.65200108e-02  2.64361594e-02 -4.17395025e-01  8.58390853e-02
  1.62232861e-01 -1.56479970e-01 -6.83721781e-01 -3.44023824e-01
 -4.91154402e-01  5.61569154e-01 -7.09708869e-01  4.13846493e-01
  9.70604837e-01  7.32252061e-01  9.80029330e-02  3.03763986e-01
 -9.25777182e-02  2.81085789e-01 -8.74687731e-02  4.34113234e-01
  2.57109016e-01 -1.41982868e-01  1.92467812e-02 -6.80649951e-02
  8.76555264e-01  3.92782271e-01  7.94579238e-02  2.09867522e-01
  7.83736169e-01 -2.98666865e-01 -5.06280303e-01 -1.43047497e-01
 -7.60367274e-01 -6.20345399e-02  4.06907141e-01 -1.75050914e-01
  5.83929956e-01 -5.01490712e-01 -4.81757998e-01  1.35574818e-01
 -1.70784250e-01  1.02815628e+00  5.66406906e-01 -8.58101696e-02
 -6.88763633e-02  8.40224683e-01 -3.59187752e-01  1.35691926e-01
  3.72236446e-02 -6.18566334e-01 -1.12673074e-01  8.90576318e-02
 -3.24682146e-01  6.74262762e-01  1.89296484e-01  5.09812087e-02
 -3.04212198e-02 -4.57993634e-02  3.20972264e-01 -3.78473043e-01
  9.04352963e-02] [ 0.27047098 -0.00681241  0.04974469 -0.11996648  0.04411501 -0.17894775
 -0.02466077 -0.24340613  0.01943829 -0.04261136 -0.45356843 -0.13768335
  0.20624381 -0.21163219  0.06060405 -0.21011662 -0.16594294  0.34949994
  0.563586   -0.26189432 -0.086059    0.10001717 -0.10798296  0.04608048
 -0.06239658  0.37381864  0.0128103  -0.00280807 -0.03681417  0.18677008
  0.08801906  0.23458683  0.22499441  0.1761319  -0.25862628  0.5505724
 -0.31189108  0.11327094  0.19685453  0.12114907  0.02264112  0.26382354
  0.22890382  0.45110106  0.03178238  0.15534537  0.01044034  0.02940553
 -0.13008749  0.29800642 -0.37898248  0.21850537  0.39331579  0.05594549
  0.28292969  0.12296107 -0.1979202   0.05291541 -0.07171446 -0.03503162
  0.25422826  0.083243   -0.07749072 -0.01622716 -0.21319655 -0.33420527
  0.0072517   0.00437443 -0.05939798 -0.13924104  0.20523061 -0.16709346
 -0.06133088 -0.06517712  0.27954865 -0.01390786 -0.48025337 -0.0615512
 -0.25263339  0.05325951 -0.15393507 -0.01717967 -0.29972708 -0.3639912
  0.06168244  0.30470586 -0.07688128  0.3583115  -0.04896438 -0.03772349
 -0.11630802 -0.20830357 -0.29346001 -0.06692058  0.43540457  0.05865675
 -0.48205912  0.07198117  0.22527379 -0.16238075  0.50444138 -0.15532644
  0.02701689  0.40370595 -0.03337459 -0.10658488  0.23246779  0.23693907
  0.20943217 -0.10762436 -0.13853915 -0.15263064 -0.4440951  -0.0812629
  0.07514271  0.33333477 -0.1065881  -0.14125842  0.08252963 -0.01120972
  0.0020931   0.12530613  0.27961177  0.05699923  0.13926674  0.14369422
  0.05819476 -0.30147594  0.38113227 -0.03596937 -0.14048076 -0.44769934
  0.08734386  0.04695213  0.36357719  0.03395608 -0.02099694  0.12742434
  0.29844627 -0.14102493  0.01588937 -0.34253439  0.20262936 -0.07335018
  0.0580093  -0.29570583  0.18564914  0.38584113 -0.52281046 -0.07909267
 -0.12973043  0.16181286  0.16907062 -0.22255929 -0.15858319 -0.05938847
 -0.14815001 -0.2639465  -0.36418039 -0.04404463  0.23337227 -0.36294654
 -0.34635004  0.02999883  0.24072123  0.45784092  0.27065971  0.05055077
 -0.10418568 -0.13253205  0.19836725  0.36507633 -0.03887331  0.12796764
 -0.13578449 -0.51977932 -0.0699842  -0.02316374  0.21488661 -0.03179355
 -0.14007057 -0.11613645 -0.19741245  0.20067291  0.21946476  0.0697389
  0.36723155 -0.04145005 -0.38031459 -0.01338432 -0.14904068  0.26042476
 -0.44828847  0.41310498 -0.1673166   0.11702412  0.21464321  0.11685522
 -0.26871064  0.08758944 -0.04116738 -0.42751437 -0.09425637  0.13434727
  0.15617041 -0.12435462  0.12503891 -0.30201992  0.21300741 -0.05111772
  0.48230752  0.06948351  0.5062269   0.17526257  0.17488039  0.24894756
  0.24621463  0.0856149  -0.36564916  0.35681131  0.20435391 -0.0645981
  0.00451269  0.10908748 -0.14633061 -0.36211124  0.25073242 -0.22037224
  0.06436504  0.07641152  0.03121307 -0.16398336 -0.14730777  0.12469143
  0.25204074  0.11850432 -0.19507062  0.02517843 -0.31655771 -0.11864967
  0.01549433 -0.01373643 -0.13154966  0.09696529 -0.17569064  0.14343794
  0.13636684 -0.15656418  0.14775236 -0.09373885  0.05133307  0.34459218
 -0.12718602  0.27463743 -0.16860817 -0.52997309 -0.36392221 -0.12348921
 -0.13762562 -0.09899822  0.50060165  0.29053468 -0.11703569  0.41420773
  0.45229816  0.09187539 -0.0389514   0.03853106 -0.03208177 -0.4054057
  0.18645704 -0.05426712  0.10480112 -0.16562966 -0.02919251 -0.16158272
 -0.0870893   0.06996383  0.29820997 -0.55035162 -0.2058593  -0.24241212
  0.02129937  0.05579277 -0.04021941  0.33622694 -0.4097285  -0.17036428
 -0.30829698 -0.21250969 -0.25619659  0.14043078  0.13488914  0.42736396
 -0.10026434  0.07554407  0.16876361  0.14893106 -0.16868573  0.0689842
  0.02401385 -0.11580696  0.21099938  0.11816037 -0.31891257  0.14455628
 -0.27692816  0.10385042 -0.03856251] [ 0.1097418  -0.02649348 -0.02142039 -0.10511508 -0.0092895  -0.19745882
  0.00365825 -0.23398751 -0.14191425 -0.08522271 -0.90713686 -0.27536669
  0.41248763 -0.42326438  0.12120809 -0.42023325 -0.33188587  0.69899988
  1.12717199 -0.52378863 -0.17211799  0.20003434 -0.21596593  0.09216096
 -0.12479316  0.74763727  0.0256206  -0.00561613 -0.07362834  0.37354016
  0.17603812  0.46917367  0.44998881  0.35226381 -0.51725256  1.10114479
 -0.62378216  0.22654188  0.39370906  0.24229814  0.04528224  0.52764708
  0.45780763  0.90220213  0.06356476  0.31069073  0.02088068  0.05881106
 -0.26017499  0.59601283 -0.75796497  0.43701074  0.78663158  0.11189098
  0.56585938  0.24592215 -0.39584041  0.10583081 -0.14342892 -0.07006325
  0.50845653  0.16648601 -0.15498143 -0.03245432 -0.42639309 -0.66841054
  0.0145034   0.00874885 -0.11879595 -0.27848208  0.41046122 -0.33418691
 -0.12266176 -0.13035424  0.55909729 -0.02781572 -0.96050674 -0.1231024
 -0.50526679  0.10651902 -0.30787015 -0.03435934 -0.59945416 -0.7279824
  0.12336489  0.60941172 -0.15376256  0.71662301 -0.09792875 -0.07544699
 -0.23261604 -0.41660714 -0.58692002 -0.13384116  0.87080914  0.1173135
 -0.96411824  0.14396234  0.45054758 -0.32476151  1.00888276 -0.31065288
  0.05403377  0.80741191 -0.06674919 -0.21316977  0.46493557  0.47387815
  0.41886434 -0.21524872 -0.2770783  -0.30526128 -0.88819021 -0.1625258
  0.15028542  0.66666955 -0.21317619 -0.28251684  0.16505927 -0.02241943
  0.0041862   0.25061226  0.55922353  0.11399847  0.27853349  0.28738844
  0.11638951 -0.60295188  0.76226455 -0.07193874 -0.28096151 -0.89539868
  0.17468773  0.09390426  0.72715437  0.06791217 -0.04199388  0.25484869
  0.59689254 -0.28204986  0.03177873 -0.68506879  0.40525872 -0.14670037
  0.1160186  -0.59141165  0.37129828  0.77168226 -1.04562092 -0.15818533
 -0.25946087  0.32362571  0.33814123 -0.44511858 -0.31716639 -0.11877695
 -0.29630002 -0.52789301 -0.72836077 -0.08808925  0.46674454 -0.72589308
 -0.69270009  0.05999766  0.48144245  0.91568184  0.54131943  0.10110155
 -0.20837136 -0.26506409  0.39673451  0.73015267 -0.07774663  0.25593528
 -0.27156898 -1.03955865 -0.1399684  -0.04632748  0.42977321 -0.06358711
 -0.28014114 -0.23227291 -0.39482489  0.40134582  0.43892953  0.1394778
  0.7344631  -0.0829001  -0.76062918 -0.02676864 -0.29808137  0.52084953
 -0.89657694  0.82620996 -0.3346332   0.23404823  0.42928642  0.23371044
 -0.53742129  0.17517887 -0.08233476 -0.85502875 -0.18851274  0.26869455
  0.31234083 -0.24870925  0.25007781 -0.60403985  0.42601481 -0.10223544
  0.96461505  0.13896702  1.01245379  0.35052514  0.34976077  0.49789512
  0.49242926  0.17122981 -0.73129833  0.71362263  0.40870783 -0.1291962
  0.00902539  0.21817496 -0.29266122 -0.72422248  0.50146484 -0.44074449
  0.12873009  0.15282303  0.06242615 -0.32796672 -0.29461554  0.24938287
  0.50408149  0.23700863 -0.39014125  0.05035685 -0.63311541 -0.23729934
  0.03098866 -0.02747286 -0.26309931  0.19393058 -0.35138127  0.28687587
  0.27273369 -0.31312835  0.29550472 -0.18747771  0.10266613  0.68918437
 -0.25437203  0.54927486 -0.33721635 -1.05994618 -0.72784442 -0.24697842
 -0.27525124 -0.19799644  1.0012033   0.58106935 -0.23407137  0.82841545
  0.90459633  0.18375078 -0.07790281  0.07706212 -0.06416354 -0.8108114
  0.37291408 -0.10853423  0.20960224 -0.33125931 -0.05838501 -0.32316545
 -0.1741786   0.13992766  0.59641993 -1.10070324 -0.41171861 -0.48482424
  0.04259874  0.11158554 -0.08043882  0.67245388 -0.81945699 -0.34072855
 -0.61659396 -0.42501938 -0.51239318  0.28086156  0.26977828  0.85472792
 -0.20052868  0.15108815  0.33752722  0.29786211 -0.33737147  0.13796841
  0.0480277  -0.23161392  0.42199877  0.23632075 -0.63782513  0.28911257
 -0.55385631  0.20770085 -0.07712503] [ 0.27799532  0.25141633  0.11683662  0.01045998  0.04270918 -0.07840008
 -0.08661126 -0.30682501  0.16490841  0.02555094 -0.13575479  0.32688904
 -0.519445   -0.10821156 -0.43097278 -0.05755315  0.22519168  0.47043017
 -0.28354457 -0.2022935   0.20617445 -0.15743843 -0.1737003  -0.05788008
 -0.35445282 -0.32998368 -0.0158402  -0.3085244  -0.39521837  0.12753962
 -0.01279158 -0.13140525 -0.04447837  0.14626881  0.04911159 -0.01563388
 -0.26139665  0.25486675  0.18570656  0.27999958  0.25928175  0.18646635
 -0.00791868 -0.13148066  0.28878918 -0.26063478 -0.26858261 -0.26949781
 -0.09185616  0.09660495 -0.08577233 -0.41320345  0.21336047 -0.17624238
  0.24904053  0.34445345 -0.07830569  0.22659293 -0.01544242  0.02761223
  0.25364593 -0.35124528 -0.11335762  0.09054859  0.16680574 -0.52625924
 -0.22063255  0.37695584  0.0040031  -0.27684498  0.16247831  0.13938706
  0.05848957 -0.47303283  0.23719464  0.40597725 -0.25386378 -0.25006297
 -0.12228137  0.05890016 -0.14700137 -0.12860212  0.31221643 -0.13754584
  0.0278972  -0.1043361   0.19208544  0.0412639   0.05564766 -0.05818065
  0.38614306 -0.2048746  -0.09804989 -0.06578492  0.35500073  0.32333136
 -0.37541142  0.18429562  0.29368618 -0.24214494 -0.08452868 -0.22081737
 -0.26714805  0.15530306  0.15593772 -0.53683805  0.48763141 -0.06017432
  0.09472972  0.40297121  0.12790687 -0.13813145 -0.21725644  0.25329965
  0.09485205 -0.1511814   0.35470012  0.21395588 -0.11324929 -0.21061563
  0.11925761  0.18024807 -0.06180359 -0.16974662 -0.38699973 -0.28519058
  0.06950223 -0.19469474  0.38605946  0.13360785 -0.25855064 -0.02757506
 -0.44584936  0.50776291  0.04649964  0.28806749  0.02220185 -0.28208414
  0.09459565  0.17651227 -0.09987175  0.20630959 -0.14544502 -0.12734084
  0.0982879  -0.18239231  0.28705597  0.20794427 -0.19333661 -0.11558868
 -0.16222861  0.09128024  0.03955181 -0.26136962  0.04663865 -0.07006066
 -0.26420611  0.00117237 -0.0489503  -0.32539558 -0.1834006  -0.58891785
 -0.18658446  0.06991247  0.11807947  0.30356169  0.39531651 -0.135426
  0.17936398  0.17389902 -0.07654449 -0.03869094  0.04515459  0.12867047
 -0.05859521  0.06460779 -0.36737317  0.1016633  -0.04938795  0.13031884
 -0.13575646  0.37472793 -0.1433834   0.11216974  0.05497717  0.08764794
 -0.05866836 -0.31427765 -0.39812782 -0.2641131  -0.00967651  0.32737991
  0.1197385   0.34801438 -0.17538948 -0.05464144  0.27252045  0.10889056
  0.11334931  0.00386548 -0.01925213 -0.07028659 -0.19039409 -0.17353053
 -0.14583755 -0.18634582  0.54943734 -0.40730494  0.08963879 -0.02452429
 -0.08389649  0.09707978 -0.06811598  0.31860575  0.28885245 -0.0677591
 -0.22438747 -0.1088862  -0.3123529  -0.13328142 -0.15157714 -0.1722268
 -0.45672455 -0.06904202  0.09193715 -0.25130507  0.16957362  0.35332543
  0.50476062  0.31074288  0.06928249 -0.38184902  0.01605753  0.36370033
 -0.49950218 -0.28535503  0.38855094  0.12354925 -0.03175391  0.07486554
 -0.0787084   0.14710765 -0.30070412 -0.16798885  0.14602426  0.26133662
 -0.21115075  0.37410417 -0.18411259 -0.3909961   0.26289105 -0.3592253
  0.50706661  0.29527697  0.1496135  -0.15069875  0.07895356  0.00365734
 -0.07716664 -0.15338774  0.18465595  0.33834171  0.1958503   0.27414823
 -0.34883249 -0.18297705  0.34229639 -0.09437913 -0.04141122  0.10278723
 -0.32974076  0.47070354  0.08016025  0.31131795 -0.25365278 -0.21848004
  0.34006101  0.31919485 -0.28864244 -0.3752012  -0.04583651 -0.02088586
  0.26239806 -0.10389732 -0.38900056 -0.01673897 -0.043525    0.31832871
 -0.26383826 -0.23732629  0.05899206  0.1823989  -0.23964885 -0.03906048
 -0.39088961  0.05769115  0.12747958  0.40053871 -0.02970261 -0.02838266
 -0.10575941 -0.45824659  0.22261088  0.16231887 -0.19405562  0.10854702
 -0.1549864  -0.38926539  0.40103558] [ 0.11285562  0.07224368  0.05592576  0.09028678  0.03394749 -0.05047422
  0.21182679 -0.12933064 -0.05022215 -0.08785181  0.22671817  0.18506968
 -0.34845716 -0.0810918  -0.2501339   0.18888637 -0.08688726 -0.02150076
 -0.00143363 -0.17686152  0.22867724  0.2445491  -0.35166973 -0.18900304
 -0.09431417  0.09212504 -0.18501766 -0.00090199  0.05729333  0.3498047
 -0.01025065  0.29659012  0.59068799 -0.49259168  0.06498761 -0.27337092
 -0.16174389 -0.00234325 -0.34206167  0.08162647  0.03804906 -0.52324653
  0.35252365 -0.23672991 -0.42820746 -0.22506711  0.33678731  0.06468168
 -0.19715761 -0.02417593 -0.24362908 -0.14423329 -0.31842518 -0.05961119
 -0.21538508  0.04693483  0.44471428  0.51126087  0.08907517  0.38333446
  0.44195938 -0.1485526   0.32181039 -0.01375653 -0.0074072  -0.28244346
  0.06890702 -0.33994535  0.42324331  0.04466215 -0.2880677  -0.2447322
 -0.05692013  0.56987607 -0.20863459  0.4832164   0.10646578  0.0426756
  0.2814137  -0.2181088  -0.00196209 -0.10229036  0.05755181  0.40439638
  0.25238317 -0.16554196 -0.31227541  0.06142845  0.14416924  0.01474781
 -0.35843107 -0.07813606  0.30304143 -0.23796052 -0.2103844  -0.2323198
  0.224611    0.05757279 -0.02620022 -0.07502762 -0.14087118 -0.05303064
 -0.06447471  0.23680766 -0.08128117 -0.27887648 -0.4843193   0.19058807
  0.16267189  0.10734962  0.1502236   0.1334223  -0.22727017  0.0147562
 -0.12016187 -0.27329153  0.41978261  0.14243564  0.1374356   0.14216846
 -0.02071018  0.15927319 -0.16280563 -0.36027119  0.0412527  -0.30297568
  0.10744094 -0.24255326 -0.26686049 -0.13891901  0.16523157 -0.06207772
  0.40397489 -0.16353594  0.10719943 -0.06792881 -0.0676583   0.0695922
  0.33075938 -0.12417402 -0.22424497  0.4650211   0.2101682   0.48308343
 -0.37418607 -0.24263053 -0.24956681  0.20764443  0.03972431  0.14850153
  0.03487365  0.44250494 -0.17968647  0.05903837 -0.38077429 -0.13902846
  0.04162722  0.4505662  -0.05453026  0.21979566 -0.2095702  -0.08979733
 -0.1768727  -0.18875876  0.13862237  0.25563341 -0.24178523  0.34302375
 -0.40174738 -0.13850915  0.32634237 -0.08607513  0.2432081  -0.07818931
  0.06936537 -0.01704991 -0.00412122 -0.08476088 -0.01885427 -0.30679336
  0.0414858  -0.09412055 -0.2766166   0.19030026 -0.10934457  0.23927678
  0.10907624  0.20365649 -0.07475874  0.3260439   0.34465462 -0.08897509
  0.05024942 -0.26781768 -0.10470463 -0.29750746  0.02563452 -0.17644995
 -0.07179246  0.47239369 -0.20953017 -0.49785647 -0.04254793 -0.11780753
 -0.1240556   0.20902924 -0.37470007  0.19523787 -0.14164253  0.13598256
 -0.22771284 -0.10168912 -0.03131438 -0.34988672  0.27553013 -0.1228468
  0.32387733 -0.07170055  0.54616237  0.18983623 -0.46229044  0.24833369
  0.12566672 -0.048533    0.57812071  0.41642407 -0.0007738   0.22832869
  0.26615533 -0.15818597  0.19696614 -0.14095943  0.01642836  0.15186819
 -0.20714287  0.28862435 -0.2231428   0.03375496  0.1942222  -0.37271285
  0.20483305  0.01353435 -0.20751008  0.02837375  0.31002575 -0.18368338
  0.03243596  0.24227996  0.12069564 -0.35444397  0.4369927  -0.26834291
  0.22731584  0.07725202  0.15110809  0.10418864  0.0587478   0.15027268
 -0.07584172  0.15343538 -0.10175534 -0.29359478  0.21522331  0.01333549
  0.43321288  0.35272285  0.11756875 -0.11104141  0.08297493  0.45924383
 -0.56989586 -0.57191312  0.45548162  0.26910147 -0.10037272  0.32799393
 -0.32193443 -0.05127586 -0.30912292 -0.23221602  0.5847919  -0.10521973
 -0.07362494  0.4113442  -0.15904802  0.3323783  -0.24633528  0.06180799
  0.03006808 -0.14748076  0.37417114 -0.17574589 -0.09652947  0.0831627
  0.01569743 -0.2898851  -0.18817048 -0.56291771 -0.36710304  0.35173112
  0.01034245  0.13930729 -0.03978278 -0.12813409  0.10112004  0.24015737
  0.28514209 -0.11759534 -0.07031424] [ 0.22545928  0.14043103  0.05814986 -0.18108423 -0.04890471 -0.17474529
 -0.07501269 -0.08731554  0.07166796 -0.35353845 -0.00216049  0.00311465
  0.00355035 -0.11613742  0.35039395  0.11868504  0.10208045 -0.09293231
 -0.23520674 -0.15022536  0.15054683  0.02398561  0.0250901  -0.1372759
  0.28892088  0.19809884 -0.29014239 -0.08482962  0.21766968  0.21939342
  0.05132023  0.24580792  0.18073389 -0.09655745 -0.20439175 -0.19479689
  0.17599173  0.13510764 -0.13471164  0.18521444 -0.23168765  0.02554879
 -0.06095981 -0.07538343 -0.18674292  0.29449549 -0.60548162  0.39116088
  0.00743289  0.19059327 -0.22547287  0.11868136 -0.05707677 -0.33506718
  0.09183816 -0.10251595 -0.1861349   0.07612223  0.2486795   0.0738626
 -0.1874447  -0.09035707  0.37423232 -0.02937118  0.19492297  0.05050215
 -0.02197949 -0.17381279  0.15169992  0.10081621  0.14847447 -0.29694366
  0.26714051  0.37243313 -0.24917781  0.39615598  0.32097808  0.06106952
  0.17975852  0.02316856  0.24458173 -0.2045455  -0.30636793  0.19079708
 -0.20531125 -0.06238838 -0.50090396 -0.14841706 -0.23156972  0.25717866
 -0.05428842 -0.12838684 -0.44820273  0.37478298  0.0244087   0.18658602
 -0.01137302  0.04550066 -0.07908879  0.25564092 -0.05885408  0.25043774
  0.13561025  0.02855421 -0.02183964  0.08390336  0.2892977   0.255844
  0.19103186  0.22932079 -0.33515984  0.19450822  0.01123609 -0.11210833
  0.03378421  0.15434252 -0.10330758 -0.12114091  0.20012531 -0.2389477
 -0.21265292  0.20985918  0.13323762 -0.24710537  0.0051768   0.37466511
 -0.31916007 -0.18146344  0.10683765 -0.27648559  0.04078047  0.01650041
  0.10581292  0.2875472  -0.37952971 -0.22336712  0.14629988  0.11937005
 -0.23403358  0.21109357  0.08948421  0.11441079 -0.28373268  0.35385218
 -0.14640298 -0.31054688 -0.06371591  0.13928548  0.17176214 -0.05325636
 -0.11348898 -0.05837194 -0.04078844 -0.09196863  0.0496187  -0.25398052
  0.11475221 -0.09165803 -0.30590978 -0.01180851 -0.00225708  0.0680181
  0.13676889 -0.17130767 -0.17770384 -0.14883365  0.18551776 -0.17569098
  0.33274466  0.1271061   0.25645351  0.28824997  0.06770626  0.0950552
 -0.26325238 -0.12813947  0.03235099 -0.23700288  0.16681992 -0.11928849
  0.34041375  0.26502714 -0.0213557  -0.0435587  -0.04149605 -0.0273533
  0.27300331  0.05042333 -0.03240037 -0.22748947 -0.15522948 -0.13315418
 -0.03492364  0.26162109 -0.2000352  -0.46781451  0.08724312 -0.13771366
 -0.05666116 -0.11427394 -0.06524481 -0.34018263 -0.27237874 -0.44574827
 -0.14442372 -0.12321121 -0.09791604 -0.07823785  0.12509744  0.06903059
  0.34407312  0.20614883 -0.0546672  -0.4212079   0.09816984  0.01442679
 -0.03181122  0.14218311 -0.0302345  -0.02846253  0.18564133  0.1621328
  0.29685977 -0.19868402  0.09679341 -0.04935393 -0.03942186 -0.12953433
  0.35399374  0.15472858  0.09901279  0.03327325  0.30584764 -0.26786944
  0.09865458 -0.07575983 -0.2638177  -0.2123455  -0.09670261 -0.07614928
 -0.18014777  0.06153392 -0.36755168 -0.18797521  0.03895974 -0.01277609
  0.26253316 -0.32365087  0.3419854   0.35982332 -0.28616875  0.17900148
 -0.16762295 -0.02807664 -0.27956378  0.08995356  0.12263316  0.19697754
  0.43351105  0.14824964  0.04284264 -0.24552903  0.28557834  0.00938116
  0.33154902 -0.01271712  0.13364948  0.3150925  -0.28187966  0.15679187
  0.20202871 -0.23863733 -0.21314859 -0.28027183 -0.21857017  0.37492263
 -0.18657823 -0.27118585 -0.07659694 -0.04671615  0.15798616  0.03988554
  0.40582883 -0.1411293  -0.07206683 -0.24128114 -0.34969258  0.31713104
  0.27373612 -0.22536333 -0.0142113   0.14752267  0.20677039  0.1481083
 -0.48343182 -0.07808949 -0.33047462 -0.28812623 -0.17447004  0.06958713
  0.15085506  0.44889218 -0.12642696  0.01431076  0.03874785  0.12873568
 -0.01142939  0.33413243  0.16643927] [ 0.24989802  0.07626641  0.06149831 -0.11079762 -0.00570099 -0.16137436
  0.01758201 -0.19006416  0.06939683 -0.70707691 -0.00432098  0.00622931
  0.00710069 -0.23227483  0.7007879   0.23737007  0.2041609  -0.18586463
 -0.47041348 -0.30045071  0.30109367  0.04797122  0.05018021 -0.27455181
  0.57784176  0.39619768 -0.58028477 -0.16965924  0.43533936  0.43878683
  0.10264046  0.49161583  0.36146778 -0.19311491 -0.4087835  -0.38959378
  0.35198346  0.27021527 -0.26942328  0.37042889 -0.4633753   0.05109758
 -0.12191962 -0.15076686 -0.37348583  0.58899099 -1.21096325  0.78232175
  0.01486577  0.38118654 -0.45094573  0.23736273 -0.11415353 -0.67013437
  0.18367632 -0.2050319  -0.37226981  0.15224446  0.49735901  0.14772519
 -0.3748894  -0.18071415  0.74846464 -0.05874236  0.38984594  0.1010043
 -0.04395898 -0.34762558  0.30339983  0.20163241  0.29694894 -0.59388733
  0.53428102  0.74486625 -0.49835563  0.79231197  0.64195615  0.12213904
  0.35951704  0.04633712  0.48916346 -0.409091   -0.61273587  0.38159415
 -0.41062251 -0.12477675 -1.00180793 -0.29683411 -0.46313944  0.51435733
 -0.10857683 -0.25677368 -0.89640546  0.74956596  0.0488174   0.37317204
 -0.02274604  0.09100132 -0.15817758  0.51128185 -0.11770817  0.50087547
  0.27122051  0.05710842 -0.04367927  0.16780671  0.5785954   0.51168799
  0.38206372  0.45864159 -0.67031968  0.38901645  0.02247218 -0.22421665
  0.06756843  0.30868503 -0.20661516 -0.24228182  0.40025061 -0.47789541
 -0.42530584  0.41971835  0.26647523 -0.49421075  0.0103536   0.74933022
 -0.63832015 -0.36292687  0.21367531 -0.55297118  0.08156094  0.03300082
  0.21162584  0.5750944  -0.75905943 -0.44673425  0.29259977  0.2387401
 -0.46806717  0.42218715  0.17896841  0.22882158 -0.56746536  0.70770437
 -0.29280597 -0.62109375 -0.12743182  0.27857095  0.34352428 -0.10651273
 -0.22697796 -0.11674388 -0.08157688 -0.18393725  0.0992374  -0.50796103
  0.22950442 -0.18331605 -0.61181957 -0.02361703 -0.00451415  0.1360362
  0.27353778 -0.34261534 -0.35540769 -0.29766729  0.37103552 -0.35138196
  0.66548932  0.2542122   0.51290703  0.57649994  0.13541251  0.1901104
 -0.52650476 -0.25627893  0.06470197 -0.47400576  0.33363983 -0.23857698
  0.6808275   0.53005427 -0.0427114  -0.0871174  -0.08299211 -0.0547066
  0.54600662  0.10084666 -0.06480073 -0.45497894 -0.31045896 -0.26630837
 -0.06984729  0.52324218 -0.4000704  -0.93562901  0.17448625 -0.27542731
 -0.11332233 -0.22854789 -0.13048962 -0.68036526 -0.54475749 -0.89149654
 -0.28884745 -0.24642241 -0.19583209 -0.15647569  0.25019488  0.13806118
  0.68814623  0.41229767 -0.10933441 -0.84241581  0.19633968  0.02885358
 -0.06362244  0.28436622 -0.06046899 -0.05692505  0.37128267  0.3242656
  0.59371954 -0.39736804  0.19358683 -0.09870785 -0.07884371 -0.25906867
  0.70798749  0.30945715  0.19802558  0.06654651  0.61169529 -0.53573889
  0.19730917 -0.15151966 -0.5276354  -0.42469099 -0.19340521 -0.15229857
 -0.36029553  0.12306783 -0.73510337 -0.37595043  0.07791948 -0.02555218
  0.52506632 -0.64730173  0.68397081  0.71964663 -0.57233751  0.35800296
 -0.33524591 -0.05615328 -0.55912757  0.17990711  0.24526632  0.39395508
  0.8670221   0.29649928  0.08568528 -0.49105805  0.57115668  0.01876232
  0.66309804 -0.02543424  0.26729897  0.63018501 -0.56375933  0.31358373
  0.40405741 -0.47727466 -0.42629719 -0.56054366 -0.43714035  0.74984527
 -0.37315646 -0.54237169 -0.15319388 -0.0934323   0.31597233  0.07977108
  0.81165767 -0.2822586  -0.14413366 -0.48256227 -0.69938517  0.63426208
  0.54747224 -0.45072666 -0.02842259  0.29504535  0.41354078  0.29621661
 -0.96686363 -0.15617898 -0.66094923 -0.57625246 -0.34894007  0.13917427
  0.30171013  0.89778435 -0.25285393  0.02862151  0.07749571  0.25747135
 -0.02285879  0.66826487  0.33287853] [ 0.37799484  0.05870396  0.14980455 -0.11711875 -0.01510709 -0.26127893
 -0.0470322  -0.16032267 -0.14419466  0.15769137 -0.00804457  0.18978409
 -0.65901667  0.25663289 -0.39856306 -0.12564836 -0.25163886  0.12803756
  0.03169429  0.18433602 -0.09659535  0.08998741  0.31017488 -0.22010209
 -0.47726977 -0.25146496  0.31635576 -0.33804423 -0.2942408  -0.11322062
  0.0992874  -0.63837421  0.26777729 -0.03385993 -0.32824183 -0.41857761
  0.29864007  0.12152846 -0.11772893  0.06385873  0.15046445  0.65753317
 -0.32969323 -0.3293891   0.42965478  0.20622933 -0.03062844 -0.29719561
  0.19581716  0.32146949  0.57148075  0.00489462  0.141123    0.25395456
  0.03719174 -0.06331875  0.10016194 -0.37114924  0.17994885  0.07794655
  0.00805304  0.19708872  0.26435623  0.10173831  0.18961698  0.19111246
  0.29375848 -0.23660387 -0.25133476 -0.09667379  0.0033966   0.05462505
  0.07263593 -0.55374181  0.40128842  0.41710514 -0.3982943   0.34486425
  0.44944054 -0.29940945  0.4612579  -0.32088044 -0.14437956  0.33440632
 -0.30587995  0.10354698  0.01700788 -0.17179851  0.18129709 -0.54739964
 -0.12359408  0.05578221  0.12972052  0.04058065  0.01324331  0.27609307
 -0.2082818   0.32035226  0.08172476 -0.37270451 -0.16036163 -0.13365406
  0.39572591  0.54999626 -0.11419438  0.00716822  0.11007836 -0.297232
  0.19595289  0.01062462 -0.07146515  0.09262569 -0.02141697 -0.01460917
  0.52771604  0.34328637 -0.44790828  0.01925498  0.0778878   0.31527674
 -0.03025819  0.16152491 -0.17331468  0.31687054  0.04011716 -0.17421135
  0.0467023  -0.33276272 -0.11095465  0.36480871 -0.2063805   0.00194107
 -0.17143881  0.27290532 -0.22466578  0.25701016  0.03237506 -0.44335002
 -0.65098035  0.0983712   0.18882032  0.14156912  0.01897349 -0.23508656
  0.38968608  0.09772808 -0.04088361 -0.05379603 -0.03520326 -0.1361455
 -0.27138349 -0.59534872 -0.10536537 -0.15727013  0.49749792 -0.33278671
  0.28717589  0.00180368  0.14349742  0.14723827 -0.40647751  0.20514193
 -0.12568796  0.32031661  0.42518219 -0.07410625  0.00635578 -0.20719106
  0.05552593 -0.06651794 -0.32118937 -0.36515337  0.17881899  0.05697092
  0.28582525  0.06157579  0.18520497 -0.18032759  0.24865353 -0.53147376
  0.03206314  0.1570529   0.22147883 -0.32127303  0.52141517 -0.64352435
 -0.23148069 -0.16960637  0.39492714 -0.48041004 -0.18309502  0.13447118
  0.60121095  0.06400753  0.30468148 -0.05690772 -0.04009159  0.06502476
  0.13964136 -0.25986755  0.26353073 -0.18149844 -0.110514   -0.25490236
 -0.17901358  0.34484231  0.45882893 -0.06743819 -0.05284432 -0.22793385
  0.09283611 -0.32431075  0.09268848 -0.10565877  0.2290071   0.04277381
  0.06702442 -0.28770509  0.0914156  -0.18491291  0.30632433  0.01106676
  0.03295735 -0.24666335 -0.24617885  0.06992318 -0.09501625  0.08835248
 -0.09109688 -0.16713721  0.27644151 -0.09578344 -0.06636949 -0.31655645
 -0.05748437  0.26083896 -0.06627095 -0.23629892 -0.27687213  0.09827203
 -0.32212359  0.2012188  -0.07796172  0.19002469 -0.36154985 -0.10550145
 -0.47576913 -0.07329938  0.12321116  0.04270335  0.44820377 -0.3079465
 -0.18066488 -0.11204262  0.2865884  -0.23042431 -0.20363195 -0.07177036
  0.05600784 -0.23067851  0.10925549  0.0804047  -0.00703146 -0.2968345
  0.06524255 -0.10072028 -0.39173904 -0.21994612 -0.11571118  0.08604842
 -0.17529999  0.53469157 -0.39459565 -0.06702133  0.0839677  -0.28455761
  0.01124609  0.37072897 -0.3344343  -0.45953187 -0.05406001 -0.0472048
 -0.36764687  0.04582518  0.06378065  0.1785918   0.00953389 -0.13010539
 -0.38187581 -0.27484381  0.14415881 -0.08112881 -0.16272976  0.09576791
 -0.28134891  0.27006724 -0.2464423   0.2412238  -0.04096731  0.49749488
 -0.34232727 -0.15166859 -0.06079253 -0.17502876 -0.2232012  -0.62774676
 -0.03193115 -0.03825163  0.43489039] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  3.23701560e-01  1.23859271e-01  3.17038149e-01
 -1.99837953e-01  1.45933881e-01  1.97479472e-01  1.11766860e-01
 -2.97712028e-01  2.12801516e-01 -8.30430072e-03 -1.36353567e-01
  5.88244051e-02 -1.61227629e-01  3.60989422e-01  2.74212211e-01
 -1.38430834e-01 -5.86703680e-02 -4.27209228e-01 -4.38389741e-02
 -1.70130521e-01 -2.79001564e-01 -5.08604087e-02 -2.22016856e-01
  3.46666753e-01 -1.82083368e-01 -1.79737844e-02 -7.58341700e-02
 -2.81926543e-01 -8.88730120e-03 -9.31498557e-02 -1.63259923e-01
  2.63657838e-01  2.68971026e-01 -1.65585026e-01 -1.38222620e-01
 -1.05521366e-01 -2.16932997e-01  1.53523952e-01 -6.68047294e-02
 -1.14297360e-01 -1.98990464e-01  9.14934743e-03  2.44847938e-01
 -1.28512876e-02  1.28243208e-01 -1.80381879e-01  1.76793709e-01
  1.34143546e-01 -9.09292847e-02  1.11920275e-02 -1.59795374e-01
  4.03871059e-01 -2.08633363e-01  2.61400253e-01 -1.77513242e-01
  2.64400899e-01 -3.74767929e-01 -1.41106667e-02  7.07195699e-02
 -2.59310529e-02 -1.52463704e-01 -2.09277481e-01  2.42659152e-01
 -4.37896281e-01 -2.74111658e-01  3.23784024e-01 -1.12194913e-02
 -7.78998435e-03 -2.63834894e-01 -1.86299264e-01 -1.20873287e-01
  2.04141185e-01  1.60952255e-01  3.01728785e-01  2.21852630e-01
  1.73017696e-01  3.71879220e-01  2.80739516e-01 -1.18493669e-01
  1.09062918e-01 -2.55998373e-01  3.87480080e-01  4.10263650e-02
  2.02835858e-01 -3.17999512e-01  3.78140330e-01  2.57947147e-01
 -4.50112194e-01  1.04353130e-01  1.40699465e-03  7.67216682e-02
 -1.11133285e-01  1.40065238e-01 -3.31422426e-02  6.38056397e-02
 -1.78506866e-01  1.02249458e-01 -1.90868706e-01 -2.67677069e-01
  8.56479257e-02  1.85055599e-01  2.22840309e-01 -1.93136290e-01
 -1.12496831e-01  2.01004639e-01 -3.48373577e-02 -1.59934118e-01
  2.03856632e-01  2.03415796e-01  1.91151500e-02 -2.14736730e-01
 -2.01674789e-01 -1.03147745e-01  2.00410113e-01  6.62044063e-02
 -5.02013564e-02 -1.48162618e-01 -1.72178164e-01 -1.56178564e-01
  2.05861077e-01 -2.44715880e-03 -4.20740098e-02  1.99603319e-01
 -3.03595811e-01 -2.62882337e-02  5.65342568e-02  1.27887994e-01
  1.71164036e-01 -1.49366796e-01 -6.41742051e-02  1.59788847e-01
 -1.77765921e-01  1.73824430e-01 -1.28364572e-02  3.04782897e-01
  8.32331404e-02  4.28962141e-01  1.86484933e-01  3.57698888e-01
  1.70930754e-02  1.14737764e-01  1.83813170e-01  7.07985610e-02
  1.53863117e-01  1.45394146e-01  6.10771216e-02 -7.36576989e-02
  5.53056449e-02  1.07640862e-01  8.00179467e-02  3.23191434e-02
  3.09991091e-01  3.02178174e-01 -9.09306332e-02  4.91853952e-02
 -1.28031313e-01 -2.54896469e-02  2.34120145e-01 -1.74658343e-01
 -2.32459098e-01  6.18232414e-02 -2.38484278e-01 -6.28195284e-03
  1.03620939e-01  2.82353729e-01 -6.52055070e-02  7.75345117e-02
 -1.90856859e-01 -1.25437349e-01  1.37492150e-01 -1.64269745e-01
  2.51677603e-01  2.33259335e-01  2.17757478e-01  2.26037607e-01
  4.26998675e-01 -1.67967141e-01  2.59857118e-01  7.12665543e-02
  1.49024948e-01 -4.46223438e-01  1.28380001e-01  1.54407173e-01
 -2.38353878e-01 -4.55187351e-01 -1.71284303e-02 -2.03437835e-01
  4.44028795e-01  1.49454489e-01  2.29066014e-01  5.37483320e-02
  1.16622731e-01  3.27921122e-01  8.48986357e-02  2.98905432e-01
 -1.16589181e-01  3.75571474e-02  2.16130674e-01  5.97562492e-02
  1.57818839e-01 -3.79134119e-01  1.97058737e-01  8.41842405e-03
 -6.73631504e-02  3.26564431e-01  1.02038927e-01  9.84950215e-02
 -4.74490523e-02 -3.58012021e-01 -8.46094415e-02 -2.53597409e-01
  1.31108731e-01 -1.36945480e-02 -2.58189082e-01 -4.04197037e-01
 -9.93263200e-02  7.22196922e-02 -3.21413815e-01  1.38105333e-01
  5.27646877e-02 -3.74778695e-02 -3.09642814e-02  8.79964158e-02
  9.22843367e-02  6.59269169e-02 -4.25867379e-01 -1.13668293e-01
 -2.31259083e-03  1.71499252e-02  4.65329364e-02  2.83751369e-01
 -9.33092088e-06  2.11278170e-01 -1.37060642e-01  4.17382754e-02
 -7.07651116e-03  1.57808796e-01 -6.90133646e-02  2.53847003e-01
 -2.96002537e-01  1.61520377e-01  9.76996198e-02 -2.18553003e-02
 -1.24721631e-01  1.75630823e-01 -6.57364130e-02 -2.27706671e-01
 -1.50046155e-01 -4.29430157e-01 -8.62716325e-03 -3.48888189e-01
  8.30511972e-02  1.19602770e-01 -1.06453761e-01 -3.39551032e-01
  5.58682233e-02 -1.50723115e-01 -8.26126337e-02 -1.29831970e-01
  9.88700613e-02  4.37329859e-01 -1.03127375e-01  2.38669097e-01
 -8.08308348e-02  3.22031081e-02 -4.68674600e-02 -1.37694642e-01
 -3.00011188e-01 -2.28143096e-01  1.68973625e-01 -1.38916388e-01
 -2.30189320e-02  7.20473751e-03  2.29728416e-01 -5.10682501e-02
 -2.43143052e-01  7.35338554e-02 -9.28391144e-02  1.04980737e-01
 -1.35518312e-01 -3.77203338e-02 -6.18087426e-02  3.51022571e-01
 -1.79783516e-02  3.15492228e-02 -5.28395176e-01 -6.82753976e-03
 -7.57957473e-02  5.80136776e-01  7.35457381e-03  2.11273469e-02
  1.24203205e-01 -1.12393416e-01  3.32168788e-01 -2.33854130e-01
 -1.09314382e-01  1.09809563e-01  6.32748082e-02 -3.05012822e-01
 -1.37086868e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/08/2021 14:24:00 - INFO - data_loader -   *** Example ***
10/08/2021 14:24:00 - INFO - data_loader -   guid: test-4
10/08/2021 14:24:00 - INFO - data_loader -   tokens: [CLS] no flash th ##resh [SEP]
10/08/2021 14:24:00 - INFO - data_loader -   input_ids: 101 2053 5956 16215 21898 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:24:00 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:24:00 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:24:00 - INFO - data_loader -   intent_label: 4 (id = 4)
10/08/2021 14:24:00 - INFO - data_loader -   slot_labels: 0 4 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 14:24:00 - INFO - data_loader -   ner_embeds: [ 0.14663459 -0.05262458 -0.00995038 -0.09029851  0.12263373 -0.14626154
  0.02043098 -0.1690678   0.03294891 -0.05268421 -0.3358649   0.27530029
 -0.07744435 -0.07584086 -0.02695106 -0.26804128  0.02627846 -0.10347202
  0.2013707  -0.01272027  0.16158915 -0.03340857  0.17287622 -0.18740179
  0.26615396  0.09241544 -0.11608424 -0.27987984 -0.28424925 -0.18137644
 -0.22812925  0.0249024   0.30128998  0.13045995  0.03118261  0.16935138
 -0.06230416  0.17561074  0.00367108 -0.07402565  0.04037194 -0.07837126
 -0.1561325   0.20836185  0.14040159  0.11099035  0.17083055 -0.35949063
  0.03112581 -0.18540204  0.04089637 -0.29396915  0.49076098 -0.05263726
 -0.16885227  0.11559261  0.25719538 -0.1559715  -0.06362429 -0.12877826
  0.06794425 -0.01322358  0.26562798  0.08987913  0.29401389 -0.37333494
 -0.01440139  0.30768135 -0.37712052 -0.108851    0.07931803  0.17231582
 -0.44193396  0.03251904  0.0195513  -0.03582918 -0.13726601 -0.28466982
 -0.33915862 -0.0583167   0.01170416  0.32957253  0.08330661  0.28316665
  0.00079225  0.41026247  0.02413866 -0.45413318 -0.1344302  -0.09248621
  0.51263183 -0.13745138 -0.11134322 -0.20985258  0.39278489 -0.1011213
  0.18303782 -0.12043855  0.26268336 -0.0461308   0.30422714  0.07902091
 -0.15427054  0.18763816 -0.00932948  0.02431204  0.30051631 -0.24693149
  0.11975729 -0.0125444   0.13902089 -0.25323379 -0.1082405   0.42633086
  0.19724542 -0.07689937 -0.1337288   0.27917713 -0.0091875   0.25963399
 -0.14592938 -0.05517824  0.0350663   0.20550938  0.11277337 -0.2699483
  0.01591763 -0.00071367  0.07296164  0.02445946  0.14451511 -0.04559916
 -0.17699969 -0.42008832  0.10587756  0.09828436  0.28259453 -0.0146672
 -0.11086253 -0.35100189  0.04879292 -0.0834544   0.16815855 -0.13103473
 -0.02294332  0.36864796  0.14379583  0.44768068 -0.23370998  0.152519
  0.04665052 -0.17252302  0.31976756 -0.16741526 -0.25466728 -0.26076594
  0.00063117 -0.05335071  0.15843306 -0.26611817  0.05734076 -0.08238135
  0.18893163 -0.20564257 -0.16225532 -0.20887086  0.08916163 -0.01871778
  0.12805638  0.26127747 -0.0709104  -0.27082589 -0.1627634   0.13914403
  0.12214889 -0.1563787  -0.24772033 -0.06986653  0.2272698  -0.05595371
 -0.00991442 -0.20372061  0.04004419 -0.04990653  0.43829471  0.11963772
 -0.12598085  0.24185398 -0.04867083 -0.08653479 -0.26793903  0.13082959
 -0.43375427 -0.13357146  0.19298363 -0.22029646  0.38383847 -0.07818928
 -0.0058459  -0.32991308 -0.08181709  0.14099021  0.09491039  0.21827479
 -0.00399596 -0.08103576  0.13853465  0.01428204  0.44172066  0.10182345
  0.14742956 -0.14591892  0.33454806  0.04376031  0.32970932  0.15365525
 -0.22210287 -0.16479632 -0.28103629  0.00301148  0.01037944  0.20718177
 -0.2371673   0.33999005 -0.13203178 -0.26044402 -0.22456646 -0.09428472
  0.12473079 -0.10682054  0.08118179  0.18629588 -0.10320079  0.10400119
 -0.36503249  0.0367523  -0.26162294 -0.13888274 -0.00607623  0.1842383
 -0.33304039  0.17386849 -0.0694743   0.03439113  0.08467512  0.23954627
  0.3646954  -0.02856011 -0.21535675  0.18716544  0.25370413 -0.18916315
  0.1572178  -0.06640452  0.13099778  0.01409951 -0.31194866 -0.14290154
 -0.1712914  -0.32875261 -0.09570646  0.28620249 -0.23103324  0.22911115
  0.02136808 -0.04183525  0.2437249  -0.1248548  -0.05797715  0.476778
 -0.21734898  0.23745687  0.26735842  0.27674222  0.07875728 -0.16760489
 -0.46723711 -0.08130792  0.09524607 -0.19139667  0.13001987  0.25027886
 -0.01569632  0.08894423 -0.10037478  0.12834264 -0.4296048   0.41507876
  0.2195386  -0.00381741 -0.146727    0.23849073  0.10347191  0.17253159
 -0.4031671   0.19611832  0.17422171  0.3443523  -0.18065456  0.04004083
  0.10621159 -0.19717942  0.25161341 -0.0291713   0.11438692  0.13997085
  0.00901562 -0.24855603 -0.01550403] [ 2.25459278e-01  1.40431032e-01  5.81498630e-02 -1.81084231e-01
 -4.89047095e-02 -1.74745291e-01 -7.50126913e-02 -8.73155370e-02
  7.16679618e-02  1.25394598e-01 -4.58207503e-02  2.23141849e-01
 -8.35005194e-02  2.18542852e-02 -2.40041584e-01  2.32223094e-01
 -2.77668476e-01  3.73124212e-01 -8.71874616e-02  3.83881703e-02
 -1.26365438e-01 -2.40316495e-01  1.15468716e-02  2.10762560e-01
 -5.91321029e-02  2.84156371e-02  8.14856216e-02  2.38105897e-02
 -2.70854354e-01 -1.92513719e-01  1.25984997e-01  1.12061597e-01
  2.49806475e-02 -1.61922574e-01  1.39838189e-01 -2.46768266e-01
 -5.23167709e-03 -1.66744813e-02  1.80778354e-01  8.12133923e-02
  5.08625627e-01  3.04824054e-01 -8.94072233e-04 -6.08654857e-01
  2.78478414e-01 -4.41571660e-02  2.07712293e-01  1.14362404e-01
 -1.83386520e-01  3.12768430e-01  1.23897381e-01 -5.70185333e-02
 -1.10704482e-01  1.90146863e-01 -9.88270193e-02 -1.20513253e-02
  7.60345021e-03  8.39602947e-02  2.28646681e-01 -1.10418312e-01
  1.08972535e-01 -2.00526536e-01  2.39356652e-01 -4.65607233e-02
  2.37649083e-01 -4.36315350e-02  1.52174637e-01  1.69666320e-01
  8.87352675e-02  1.29999042e-01  1.70585871e-01 -2.78999265e-02
 -1.75184280e-01  2.80241668e-03  5.64590573e-01  1.55722246e-01
  9.31789652e-02  7.10239485e-02 -3.76761466e-01 -1.39737219e-01
  3.05664718e-01  2.70990998e-01 -2.39271857e-02 -3.31656396e-01
 -2.91238338e-01 -2.21117973e-01  5.08115411e-01  3.00455898e-01
 -7.46001974e-02 -1.04837507e-01  2.69698352e-01  1.83545008e-01
 -2.04822123e-01  2.17381358e-01  1.05495967e-01 -4.05144840e-02
 -1.92508660e-02  2.61573166e-01 -9.27600861e-02 -1.01952322e-01
  3.14051658e-01  2.69240856e-01 -7.67153278e-02  3.78605187e-01
  2.01875016e-01  2.71632999e-01 -3.38223785e-01  5.29468283e-02
 -2.48893630e-02 -1.80690438e-01  6.68021590e-02  1.07697681e-01
  1.50705487e-01  5.54191649e-01  4.18807745e-01  3.16528641e-02
  2.49241933e-01  3.37423921e-01  7.46652111e-02 -1.66090682e-01
  3.43011767e-01 -1.65220261e-01  9.00832936e-02 -9.10406634e-02
 -1.60424292e-01  2.70328037e-02 -1.30315229e-01 -1.18883595e-01
  1.76333889e-01  4.56375331e-01 -2.69919876e-02  3.97206336e-01
 -2.76748329e-01 -5.20444587e-02  5.31832017e-02 -5.47271669e-02
  2.27373280e-02 -2.94360191e-01 -2.05238596e-01 -1.11228257e-01
  2.84980208e-01 -7.09310398e-02  5.13390779e-01 -6.09368011e-02
 -5.55906594e-01  7.79048875e-02  4.05872576e-02 -1.73409849e-01
 -1.27729684e-01  2.95072705e-01 -1.44398078e-01  6.76527098e-02
  2.48805229e-02 -2.40556542e-02 -1.53798372e-01  1.45721808e-01
 -9.20603052e-03  1.50570590e-02  2.57659316e-01  9.23206098e-03
  2.38869637e-01 -1.97279677e-01 -2.61781782e-01 -1.20797202e-01
  3.52535278e-01 -1.82237431e-01  2.25169256e-01  3.48716490e-02
 -4.82748598e-02  3.86858195e-01 -1.17425591e-01  3.54388922e-01
  2.22282037e-01  1.04886666e-01 -8.25274587e-02  3.85302097e-01
 -5.35820462e-02  7.05288798e-02 -8.27145949e-02 -4.54306155e-01
  4.82606381e-01  4.17126268e-01  2.73681343e-01  1.75098404e-01
  9.38367024e-02 -2.42745116e-01  1.40059039e-01  2.34043449e-01
  8.00344720e-02  1.39589623e-01 -2.24081203e-01 -1.65721729e-01
 -3.66046339e-01  1.88634600e-02 -3.13522294e-04 -1.13870846e-02
 -4.96250451e-01 -8.55301768e-02  2.48776853e-01  8.14482570e-02
 -6.28912169e-03  6.34502561e-04 -3.95629071e-02 -3.51808429e-01
  1.23636425e-01 -8.39065854e-03  3.79676491e-01 -2.82233119e-01
  4.92829047e-02  2.94220477e-01 -5.81718087e-02 -5.01236916e-01
 -2.16908634e-01  1.17528718e-02 -7.30974674e-02  1.94192022e-01
  4.09695774e-01 -2.88432211e-01 -5.68990171e-01 -2.20375776e-01
 -1.56441316e-01  2.38581207e-02 -1.78503811e-01 -4.32383478e-01
 -5.20660937e-01  3.62517312e-02 -2.23280620e-02  3.02442074e-01
 -3.07071209e-01 -3.16816479e-01  3.34909588e-01 -6.21571057e-02
 -3.81129473e-01  2.70324588e-01  2.54224509e-01 -1.14248954e-01
 -1.70490026e-01 -1.51490286e-01 -1.17840335e-01 -3.85452248e-02
 -5.28914034e-02 -2.25615948e-01 -1.32957697e-01 -2.56876260e-01
 -1.60663113e-01 -2.66581684e-01 -2.38061264e-01 -2.39603911e-02
 -1.02599629e-03  2.02158004e-01  3.73692125e-01 -1.20554283e-01
 -1.89108908e-01 -9.60974470e-02 -1.99621499e-01  1.47400334e-01
 -1.27148882e-01  4.46694568e-02  3.11173469e-01  4.18690108e-02
  4.32042927e-01  5.26103854e-01 -1.23612717e-01  3.40382338e-01
  5.63322976e-02  7.90986195e-02  9.31319490e-04 -2.97582000e-01
  3.77876699e-01  2.65440464e-01 -2.07945183e-01 -3.41242671e-01
  2.90700108e-01  9.99119654e-02 -1.44291103e-01  7.06645250e-02
 -8.20965320e-02  3.62904429e-01 -8.94528776e-02 -5.42930484e-01
  2.74409860e-01 -6.14459515e-02 -6.90109357e-02  4.99851912e-01
 -3.82813513e-01  4.12926599e-02  3.33658516e-01  1.17729045e-01
  1.44520998e-01  9.21088606e-02  7.48607889e-02 -8.42146352e-02
  6.86477423e-02  1.91821828e-01  6.84695318e-02 -4.91592735e-01
 -1.96789876e-01  1.74556617e-02 -1.90122172e-01 -4.07537907e-01
  1.20613560e-01  2.85331253e-02 -2.58079678e-01 -5.23248017e-02
  3.25045586e-01  2.44301334e-01 -1.03783146e-01 -2.29251787e-01
 -2.85607636e-01] [ 0.11639418  0.08558187  0.09059657 -0.02518909 -0.00108023 -0.16276515
  0.06848613 -0.19651432  0.05127958  0.39657205 -0.60230106 -0.08149493
  0.15973438  0.01809616  0.3189649   0.36637193 -0.39408684  0.12602732
  0.17328483 -0.33012807 -0.1033791   0.33976269  0.23798619  0.0061833
 -0.09862921  0.17721513 -0.20794101  0.32887343  0.18203562  0.05424209
 -0.61248124  0.08396109 -0.18478701  0.40284237 -0.34042001  0.18879886
 -0.71824163 -0.00395675 -0.34308207  0.3848134   0.23313916 -0.09144089
 -0.5649398   0.28186604  0.01116756  0.04711701  0.18503805 -0.05114907
 -0.28590235  0.16898711 -0.22722934 -0.0812641   0.29243898  0.20676653
 -0.00525717 -0.23643881 -0.06438124  0.16510794  0.63400942 -0.26558605
  0.43602675  0.28679594 -0.15027483  0.51098955  0.02922901 -0.25720492
 -0.13470131 -0.23301065  0.13701728 -0.33296368  0.11297016 -0.47461486
  0.31816426 -0.13700137 -0.0345954  -0.16489738 -0.16377768  0.0852666
  0.36224303 -0.01274441  0.46551806  0.67031622  0.32305783 -0.47310108
 -0.09944304 -0.23549543  0.08732879 -0.06093499 -0.21561444 -0.14623637
 -0.2159832  -0.1666829  -0.05133099 -0.27708972  0.3164247   0.03029547
 -0.15193121 -0.45444423  0.18247966 -0.03864337  0.17727871  0.33349326
  0.15338367  0.23665312  0.08216643  0.14426221 -0.20782571 -0.24828945
 -0.27102411 -0.08577754 -0.16378205  0.03004782  0.11406841  0.40908337
  0.31243005  0.1728255  -0.3418915   0.05431088  0.36158022 -0.11696012
 -0.11556079  0.30373529 -0.08410316  0.38923097  0.12930439  0.38214946
  0.05069624 -0.17892025 -0.0422167  -0.01742607 -0.14422399 -0.09243224
 -0.24105607 -0.15970014  0.07794084 -0.05104246  0.36080462 -0.08270667
  0.6852895   0.32093298 -0.1279504  -0.24634558  0.22682305 -0.03390764
 -0.48115388 -0.16715455  0.35717881  0.56374252 -0.20417561  0.43098125
 -0.35161078 -0.424707    0.22329468 -0.43413466 -0.0898327   0.05555767
  0.4117437  -0.19571842  0.06631115  0.13009261  0.15390034 -0.05841699
 -0.31602466  0.14698751  0.17285194 -0.28745717  0.64837587  0.12150687
 -0.43738851  0.11643701 -0.10104842 -0.28879303  0.14200215 -0.25419632
  0.186404   -0.37322634 -0.01962263  0.32126462 -0.3835834  -0.17419358
 -0.00228885  0.14463845  0.1998482   0.34308109 -0.217319    0.2400817
 -0.27247059 -0.25990841  0.16476382 -0.28506368 -0.09891334  0.2049807
  0.12892359  0.11975369  0.08801949 -0.01130949  0.04462396  0.16068918
  0.30429637  0.64823353 -0.07351818 -0.32883972 -0.06382166  0.12941544
 -0.13843106  0.219439   -0.50222677  0.23495473  0.64555687 -0.53406644
  0.07187327 -0.08485591  0.18488559  0.03257831  0.11880191  0.18554996
  0.48692724  0.14839618 -0.54172981  0.2663514   0.27358085  0.05123441
 -0.0144651  -0.08632246  0.06595635 -0.21491082 -0.34764892  0.27380604
 -0.11231612 -0.0377158  -0.13413873 -0.49853396  0.2481439  -0.17067087
 -0.3563647  -0.06732167 -0.52512789 -0.36002192 -0.28726301  0.28410694
 -0.01759438  0.29701453 -0.32235518 -0.14781319  0.54166514  0.02024485
 -0.0565396   0.3965672  -0.20090665 -0.37177718 -0.0514672  -0.07632269
  0.09074973  0.57277489 -0.0347687   0.46036941 -0.27179429  0.30901599
  0.2297052  -0.32792434  0.3195993   0.32717916  0.07741376  0.51658666
  0.11538628 -0.2751905   0.09569509 -0.14144447 -0.3512122  -0.54055953
  0.28451189 -0.33744127  0.13570522 -0.26124713 -0.07663043  0.1788757
 -0.21937403  0.23827218  0.56674033  0.09589699 -0.10157046 -0.2222148
  0.34869513 -0.00345346 -0.39290324 -0.21197726  0.31590784 -0.14572076
 -0.08235612 -0.51514089  0.1322175   0.00497496  0.33261749 -0.02311541
  0.3449904   0.12854025  0.07278868 -0.18515085 -0.20190591 -0.03062501
 -0.09758679 -0.04378706  0.2414871  -0.53557205 -0.22295845 -0.16511074
 -0.2670348   0.48650032  0.09950233] [ 0.17355511  0.13193156  0.15399472 -0.06267814  0.02265351 -0.17596389
  0.06274267 -0.19548027  0.01290474 -0.2248574  -0.25976893 -0.13229211
 -0.19540513  0.23566061  0.47955897 -0.14886652 -0.23343655 -0.13537459
 -0.15797138 -0.02821555  0.01919629  0.45112944 -0.07564125 -0.04285863
 -0.47573036  0.01108681 -0.05702286 -0.38659209 -0.05864334 -0.09304729
 -0.02778733 -0.12062229  0.02422148 -0.10259202 -0.13367333 -0.09372113
 -0.35344666 -0.07769953 -0.14538868  0.21221416  0.25591359  0.10101881
 -0.31939372 -0.20440505 -0.18160486  0.111222   -0.25951481  0.33956477
  0.13507625 -0.10743041  0.21145165  0.11597162  0.1503436   0.29179433
 -0.13180865 -0.13140167 -0.04351049  0.24499509 -0.0637204   0.04552774
 -0.0029697  -0.45572889 -0.02139272 -0.1593333  -0.24804136 -0.22376116
 -0.14107883  0.02380488 -0.10591546  0.4872435  -0.05482406 -0.0986092
  0.46105161 -0.01490019  0.28346246  0.19758834  0.37750652 -0.32922405
 -0.21075714  0.11210182 -0.42346701  0.02555863  0.04116366  0.14678268
  0.22940186  0.25894666 -0.30169642 -0.16374895 -0.22521831  0.54356891
 -0.18831822  0.05351327  0.04742083  0.07163542 -0.24166033  0.11359739
  0.02446436  0.00647333  0.23877631  0.16249184  0.24078399 -0.31217095
  0.04914969  0.2035002   0.46761885 -0.43336007  0.42401546 -0.08221661
 -0.08839788  0.49383333  0.22858234  0.27849317  0.1562092  -0.11488369
 -0.1286388   0.10915907  0.02341236 -0.11059565  0.40392649 -0.09248535
 -0.30291465 -0.10408034 -0.03996358 -0.18877931 -0.05483912 -0.19411194
 -0.15120679  0.34179869 -0.19089884 -0.54379857  0.08221298  0.04508251
 -0.28598991  0.16865225  0.0417951  -0.09683928 -0.0146797   0.14171711
  0.15941048  0.3609595   0.34668714 -0.14644794 -0.04648498  0.13240486
  0.15649293 -0.09941635 -0.01359378 -0.08730491  0.02731961  0.03133513
 -0.16899389  0.07424525 -0.12476614 -0.01971386  0.34164584 -0.299303
  0.17959614  0.06501527 -0.16036269 -0.33241859  0.30295894 -0.23442288
 -0.41632551  0.07725742  0.09937679 -0.0913354  -0.15101118  0.02470924
 -0.03597192 -0.04126653 -0.33042726 -0.06668059 -0.05401574  0.20226236
 -0.27331474 -0.07749528 -0.2461037   0.06924374  0.02663248 -0.20983198
  0.05696566  0.14378048 -0.46031904 -0.03661842  0.06936565  0.01813379
  0.22670919 -0.42925677 -0.14108925 -0.01433361  0.31506467  0.42738062
  0.29935479 -0.10293898  0.05553744  0.1158354   0.12214161 -0.34261954
 -0.2201957  -0.07435343 -0.18344416  0.14003043  0.10355017 -0.18646182
 -0.42950127 -0.0902339  -0.17919722  0.0539127  -0.02458611 -0.30030206
 -0.25844395  0.22522154 -0.10678175  0.26605734 -0.07390788  0.03403652
  0.19127871 -0.10438658  0.23987591  0.03752192  0.01694156 -0.09318238
 -0.13304092  0.1651334   0.13945146 -0.01191519  0.37360215 -0.18379942
  0.16693658 -0.25604078  0.37736198  0.23052402 -0.19635452  0.01749717
  0.19417839  0.04279524  0.01302929 -0.08014741  0.09819721 -0.1109496
  0.06311258 -0.04763008 -0.08275133 -0.07285496  0.44342405 -0.06186419
  0.25995705 -0.07282485 -0.09140841 -0.39703652 -0.29729438 -0.04916627
 -0.31891823 -0.20770228 -0.14692786  0.3142848   0.09383614  0.09987332
 -0.43876237  0.07571991  0.070099   -0.25734657  0.02101515 -0.04808918
 -0.07106861  0.21688336  0.04789212  0.13033059 -0.13362615  0.33803043
 -0.05517948  0.13447449  0.02241156  0.05829157 -0.48772833  0.28414699
  0.19382735 -0.28103477 -0.10851552  0.03626259  0.21889156 -0.25592905
 -0.19117364 -0.35527638 -0.2949475  -0.25387573  0.00631707  0.25038657
  0.14923099  0.01003003 -0.05274218  0.22710292 -0.09006042  0.31976128
  0.12263451  0.16969077  0.30018586 -0.2267904   0.313054   -0.2917707
  0.2100883  -0.14770269 -0.12693664  0.12338947 -0.02471904  0.04379147
 -0.01438147  0.12365875  0.37421694] [ 0.21875346  0.1611518   0.16730075  0.08056363  0.00323358 -0.24987771
  0.05385944 -0.22917029  0.06343329 -0.44971481 -0.51953787 -0.26458421
 -0.39081025  0.47132123  0.95911795 -0.29773304 -0.46687311 -0.27074918
 -0.31594276 -0.05643111  0.03839258  0.90225887 -0.1512825  -0.08571727
 -0.95146072  0.02217362 -0.11404572 -0.77318418 -0.11728668 -0.18609458
 -0.05557467 -0.24124458  0.04844295 -0.20518404 -0.26734665 -0.18744226
 -0.70689332 -0.15539907 -0.29077736  0.42442831  0.51182717  0.20203762
 -0.63878745 -0.40881011 -0.36320972  0.222444   -0.51902962  0.67912954
  0.27015251 -0.21486083  0.4229033   0.23194323  0.30068719  0.58358866
 -0.26361731 -0.26280335 -0.08702097  0.48999017 -0.1274408   0.09105548
 -0.00593941 -0.91145778 -0.04278544 -0.31866661 -0.49608272 -0.44752231
 -0.28215766  0.04760977 -0.21183091  0.97448701 -0.10964812 -0.1972184
  0.92210323 -0.02980038  0.56692493  0.39517668  0.75501305 -0.6584481
 -0.42151427  0.22420365 -0.84693402  0.05111726  0.08232733  0.29356536
  0.45880371  0.51789331 -0.60339284 -0.3274979  -0.45043662  1.08713782
 -0.37663645  0.10702655  0.09484167  0.14327084 -0.48332065  0.22719477
  0.04892872  0.01294666  0.47755262  0.32498369  0.48156798 -0.62434191
  0.09829938  0.40700039  0.93523771 -0.86672014  0.84803092 -0.16443321
 -0.17679575  0.98766667  0.45716467  0.55698633  0.3124184  -0.22976738
 -0.25727761  0.21831813  0.04682472 -0.2211913   0.80785298 -0.18497069
 -0.6058293  -0.20816068 -0.07992716 -0.37755862 -0.10967825 -0.38822389
 -0.30241358  0.68359739 -0.38179767 -1.08759713  0.16442597  0.09016503
 -0.57197982  0.3373045   0.08359019 -0.19367856 -0.0293594   0.28343421
  0.31882095  0.721919    0.69337428 -0.29289588 -0.09296997  0.26480973
  0.31298587 -0.19883271 -0.02718757 -0.17460983  0.05463922  0.06267027
 -0.33798778  0.1484905  -0.24953228 -0.03942772  0.68329167 -0.59860599
  0.35919228  0.13003054 -0.32072538 -0.66483718  0.60591787 -0.46884575
 -0.83265102  0.15451485  0.19875358 -0.1826708  -0.30202237  0.04941848
 -0.07194384 -0.08253306 -0.66085452 -0.13336118 -0.10803147  0.40452471
 -0.54662949 -0.15499057 -0.49220741  0.13848749  0.05326496 -0.41966397
  0.11393132  0.28756097 -0.92063808 -0.07323683  0.1387313   0.03626758
  0.45341837 -0.85851353 -0.28217849 -0.02866721  0.63012934  0.85476124
  0.59870958 -0.20587796  0.11107488  0.2316708   0.24428321 -0.68523908
 -0.44039139 -0.14870687 -0.36688831  0.28006086  0.20710033 -0.37292364
 -0.85900253 -0.1804678  -0.35839444  0.1078254  -0.04917223 -0.60060412
 -0.5168879   0.45044309 -0.2135635   0.53211468 -0.14781576  0.06807303
  0.38255742 -0.20877317  0.47975183  0.07504383  0.03388312 -0.18636477
 -0.26608184  0.3302668   0.27890292 -0.02383037  0.7472043  -0.36759883
  0.33387315 -0.51208156  0.75472397  0.46104804 -0.39270905  0.03499433
  0.38835678  0.08559048  0.02605858 -0.16029482  0.19639443 -0.2218992
  0.12622516 -0.09526016 -0.16550265 -0.14570992  0.88684809 -0.12372839
  0.51991409 -0.1456497  -0.18281682 -0.79407305 -0.59458876 -0.09833253
 -0.63783646 -0.41540456 -0.29385573  0.6285696   0.18767227  0.19974664
 -0.87752473  0.15143982  0.14019799 -0.51469314  0.04203031 -0.09617835
 -0.14213721  0.43376672  0.09578424  0.26066118 -0.2672523   0.67606086
 -0.11035895  0.26894897  0.04482312  0.11658315 -0.97545666  0.56829399
  0.38765469 -0.56206954 -0.21703103  0.07252519  0.43778312 -0.51185811
 -0.38234729 -0.71055275 -0.58989501 -0.50775146  0.01263414  0.50077313
  0.29846197  0.02006006 -0.10548437  0.45420584 -0.18012084  0.63952255
  0.24526902  0.33938155  0.60037172 -0.4535808   0.62610799 -0.58354139
  0.4201766  -0.29540539 -0.25387329  0.24677894 -0.04943808  0.08758294
 -0.02876293  0.24731749  0.74843389] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  3.23701560e-01  1.23859271e-01  3.17038149e-01
 -1.99837953e-01  1.45933881e-01  1.97479472e-01  1.11766860e-01
 -2.97712028e-01  2.12801516e-01 -8.30430072e-03 -1.36353567e-01
  5.88244051e-02 -1.61227629e-01  3.60989422e-01  2.74212211e-01
 -1.38430834e-01 -5.86703680e-02 -4.27209228e-01 -4.38389741e-02
 -1.70130521e-01 -2.79001564e-01 -5.08604087e-02 -2.22016856e-01
  3.46666753e-01 -1.82083368e-01 -1.79737844e-02 -7.58341700e-02
 -2.81926543e-01 -8.88730120e-03 -9.31498557e-02 -1.63259923e-01
  2.63657838e-01  2.68971026e-01 -1.65585026e-01 -1.38222620e-01
 -1.05521366e-01 -2.16932997e-01  1.53523952e-01 -6.68047294e-02
 -1.14297360e-01 -1.98990464e-01  9.14934743e-03  2.44847938e-01
 -1.28512876e-02  1.28243208e-01 -1.80381879e-01  1.76793709e-01
  1.34143546e-01 -9.09292847e-02  1.11920275e-02 -1.59795374e-01
  4.03871059e-01 -2.08633363e-01  2.61400253e-01 -1.77513242e-01
  2.64400899e-01 -3.74767929e-01 -1.41106667e-02  7.07195699e-02
 -2.59310529e-02 -1.52463704e-01 -2.09277481e-01  2.42659152e-01
 -4.37896281e-01 -2.74111658e-01  3.23784024e-01 -1.12194913e-02
 -7.78998435e-03 -2.63834894e-01 -1.86299264e-01 -1.20873287e-01
  2.04141185e-01  1.60952255e-01  3.01728785e-01  2.21852630e-01
  1.73017696e-01  3.71879220e-01  2.80739516e-01 -1.18493669e-01
  1.09062918e-01 -2.55998373e-01  3.87480080e-01  4.10263650e-02
  2.02835858e-01 -3.17999512e-01  3.78140330e-01  2.57947147e-01
 -4.50112194e-01  1.04353130e-01  1.40699465e-03  7.67216682e-02
 -1.11133285e-01  1.40065238e-01 -3.31422426e-02  6.38056397e-02
 -1.78506866e-01  1.02249458e-01 -1.90868706e-01 -2.67677069e-01
  8.56479257e-02  1.85055599e-01  2.22840309e-01 -1.93136290e-01
 -1.12496831e-01  2.01004639e-01 -3.48373577e-02 -1.59934118e-01
  2.03856632e-01  2.03415796e-01  1.91151500e-02 -2.14736730e-01
 -2.01674789e-01 -1.03147745e-01  2.00410113e-01  6.62044063e-02
 -5.02013564e-02 -1.48162618e-01 -1.72178164e-01 -1.56178564e-01
  2.05861077e-01 -2.44715880e-03 -4.20740098e-02  1.99603319e-01
 -3.03595811e-01 -2.62882337e-02  5.65342568e-02  1.27887994e-01
  1.71164036e-01 -1.49366796e-01 -6.41742051e-02  1.59788847e-01
 -1.77765921e-01  1.73824430e-01 -1.28364572e-02  3.04782897e-01
  8.32331404e-02  4.28962141e-01  1.86484933e-01  3.57698888e-01
  1.70930754e-02  1.14737764e-01  1.83813170e-01  7.07985610e-02
  1.53863117e-01  1.45394146e-01  6.10771216e-02 -7.36576989e-02
  5.53056449e-02  1.07640862e-01  8.00179467e-02  3.23191434e-02
  3.09991091e-01  3.02178174e-01 -9.09306332e-02  4.91853952e-02
 -1.28031313e-01 -2.54896469e-02  2.34120145e-01 -1.74658343e-01
 -2.32459098e-01  6.18232414e-02 -2.38484278e-01 -6.28195284e-03
  1.03620939e-01  2.82353729e-01 -6.52055070e-02  7.75345117e-02
 -1.90856859e-01 -1.25437349e-01  1.37492150e-01 -1.64269745e-01
  2.51677603e-01  2.33259335e-01  2.17757478e-01  2.26037607e-01
  4.26998675e-01 -1.67967141e-01  2.59857118e-01  7.12665543e-02
  1.49024948e-01 -4.46223438e-01  1.28380001e-01  1.54407173e-01
 -2.38353878e-01 -4.55187351e-01 -1.71284303e-02 -2.03437835e-01
  4.44028795e-01  1.49454489e-01  2.29066014e-01  5.37483320e-02
  1.16622731e-01  3.27921122e-01  8.48986357e-02  2.98905432e-01
 -1.16589181e-01  3.75571474e-02  2.16130674e-01  5.97562492e-02
  1.57818839e-01 -3.79134119e-01  1.97058737e-01  8.41842405e-03
 -6.73631504e-02  3.26564431e-01  1.02038927e-01  9.84950215e-02
 -4.74490523e-02 -3.58012021e-01 -8.46094415e-02 -2.53597409e-01
  1.31108731e-01 -1.36945480e-02 -2.58189082e-01 -4.04197037e-01
 -9.93263200e-02  7.22196922e-02 -3.21413815e-01  1.38105333e-01
  5.27646877e-02 -3.74778695e-02 -3.09642814e-02  8.79964158e-02
  9.22843367e-02  6.59269169e-02 -4.25867379e-01 -1.13668293e-01
 -2.31259083e-03  1.71499252e-02  4.65329364e-02  2.83751369e-01
 -9.33092088e-06  2.11278170e-01 -1.37060642e-01  4.17382754e-02
 -7.07651116e-03  1.57808796e-01 -6.90133646e-02  2.53847003e-01
 -2.96002537e-01  1.61520377e-01  9.76996198e-02 -2.18553003e-02
 -1.24721631e-01  1.75630823e-01 -6.57364130e-02 -2.27706671e-01
 -1.50046155e-01 -4.29430157e-01 -8.62716325e-03 -3.48888189e-01
  8.30511972e-02  1.19602770e-01 -1.06453761e-01 -3.39551032e-01
  5.58682233e-02 -1.50723115e-01 -8.26126337e-02 -1.29831970e-01
  9.88700613e-02  4.37329859e-01 -1.03127375e-01  2.38669097e-01
 -8.08308348e-02  3.22031081e-02 -4.68674600e-02 -1.37694642e-01
 -3.00011188e-01 -2.28143096e-01  1.68973625e-01 -1.38916388e-01
 -2.30189320e-02  7.20473751e-03  2.29728416e-01 -5.10682501e-02
 -2.43143052e-01  7.35338554e-02 -9.28391144e-02  1.04980737e-01
 -1.35518312e-01 -3.77203338e-02 -6.18087426e-02  3.51022571e-01
 -1.79783516e-02  3.15492228e-02 -5.28395176e-01 -6.82753976e-03
 -7.57957473e-02  5.80136776e-01  7.35457381e-03  2.11273469e-02
  1.24203205e-01 -1.12393416e-01  3.32168788e-01 -2.33854130e-01
 -1.09314382e-01  1.09809563e-01  6.32748082e-02 -3.05012822e-01
 -1.37086868e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/08/2021 14:24:02 - INFO - data_loader -   Saving features into cached file ./data\cached_test_low_bert-base-uncased_50
10/08/2021 14:24:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\Users\k3lan/.cache\torch\transformers\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
10/08/2021 14:24:10 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "low",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/08/2021 14:24:10 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at C:\Users\k3lan/.cache\torch\transformers\f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
10/08/2021 14:24:12 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at bert-base-uncased were not used when initializing JointBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing JointBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing JointBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/08/2021 14:24:12 - WARNING - transformers.modeling_utils -   Some weights of JointBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['intent_classifier.linear.weight', 'intent_classifier.linear.bias', 'slot_classifier.linear.weight', 'slot_classifier.linear.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
True
NVIDIA GeForce RTX 2070 Super with Max-Q Design
Using device: cuda
10/08/2021 14:24:15 - INFO - trainer -   ***** Running training *****
10/08/2021 14:24:15 - INFO - trainer -     Num examples = 29358
10/08/2021 14:24:15 - INFO - trainer -     Num Epochs = 20
10/08/2021 14:24:15 - INFO - trainer -     Total train batch size = 32
10/08/2021 14:24:15 - INFO - trainer -     Gradient Accumulation steps = 1
10/08/2021 14:24:15 - INFO - trainer -     Total optimization steps = 18360
10/08/2021 14:24:15 - INFO - trainer -     Logging steps = 200
10/08/2021 14:24:15 - INFO - trainer -     Save steps = 200
Epoch:   0%|                                                                                    | 0/20 [00:00<?, ?it/s]10/08/2021 14:25:01 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 199/918 [00:45<02:42,  4.43it/s]
10/08/2021 14:25:01 - INFO - trainer -     Num examples = 3258
10/08/2021 14:25:01 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:06<00:00,  7.42it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:25:08 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:25:08 - INFO - trainer -     T-F1 = 0.9458742371981957
10/08/2021 14:25:08 - INFO - trainer -     T-F1(C) = 0.865979381443299
10/08/2021 14:25:08 - INFO - trainer -     T-F1(L) = 0.8216216216216216
10/08/2021 14:25:08 - INFO - trainer -     T-F1(O) = 0.9790547209123808
10/08/2021 14:25:08 - INFO - trainer -     T-F1(P) = 0.9895227008149011
10/08/2021 14:25:08 - INFO - trainer -     T-F1(S) = 0.9474161378059837
10/08/2021 14:25:08 - INFO - trainer -     T-F1(T) = 0.8853333333333335
10/08/2021 14:25:08 - INFO - trainer -     U-F1(A) = 0.656084656084656
10/08/2021 14:25:08 - INFO - trainer -     U-F1(E) = 0.6492374727668845
10/08/2021 14:25:08 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:25:08 - INFO - trainer -     U-F1(O) = 0.9601095702790619
10/08/2021 14:25:08 - INFO - trainer -     intent_acc = 0.925414364640884
10/08/2021 14:25:08 - INFO - trainer -     loss = 0.3220762768212487
10/08/2021 14:25:08 - INFO - trainer -     semantic_frame_acc = 0.8391651319828115
10/08/2021 14:25:08 - INFO - trainer -     slot_f1 = 0.9426162632580908
10/08/2021 14:25:08 - INFO - trainer -     slot_precision = 0.9186323880201431
10/08/2021 14:25:08 - INFO - trainer -     slot_recall = 0.9678860653448758

10/08/2021 14:25:08 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:25:08 - INFO - trainer -     T-F1 = 0.9458742371981957
10/08/2021 14:25:08 - INFO - trainer -     T-F1(C) = 0.865979381443299
10/08/2021 14:25:08 - INFO - trainer -     T-F1(L) = 0.8216216216216216
10/08/2021 14:25:08 - INFO - trainer -     T-F1(O) = 0.9790547209123808
10/08/2021 14:25:08 - INFO - trainer -     T-F1(P) = 0.9895227008149011
10/08/2021 14:25:08 - INFO - trainer -     T-F1(S) = 0.9474161378059837
10/08/2021 14:25:08 - INFO - trainer -     T-F1(T) = 0.8853333333333335
10/08/2021 14:25:08 - INFO - trainer -     U-F1(A) = 0.656084656084656
10/08/2021 14:25:08 - INFO - trainer -     U-F1(E) = 0.6492374727668845
10/08/2021 14:25:08 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:25:08 - INFO - trainer -     U-F1(O) = 0.9601095702790619
10/08/2021 14:25:08 - INFO - trainer -     intent_acc = 0.925414364640884
10/08/2021 14:25:08 - INFO - trainer -     semantic_frame_acc = 0.8391651319828115
10/08/2021 14:25:08 - INFO - trainer -     slot_f1 = 0.9426162632580908
10/08/2021 14:25:08 - INFO - trainer -     slot_precision = 0.9186323880201431
10/08/2021 14:25:08 - INFO - trainer -     slot_recall = 0.9678860653448758
10/08/2021 14:25:08 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:25:08 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:25:08 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:25:54 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 399/918 [01:39<01:59,  4.36it/s]
10/08/2021 14:25:54 - INFO - trainer -     Num examples = 3258
10/08/2021 14:25:54 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.27it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:26:02 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:26:02 - INFO - trainer -     T-F1 = 0.9736842105263158
10/08/2021 14:26:02 - INFO - trainer -     T-F1(C) = 0.9362318840579711
10/08/2021 14:26:02 - INFO - trainer -     T-F1(L) = 0.9404388714733541
10/08/2021 14:26:02 - INFO - trainer -     T-F1(O) = 0.99004329004329
10/08/2021 14:26:02 - INFO - trainer -     T-F1(P) = 0.9941383352872215
10/08/2021 14:26:02 - INFO - trainer -     T-F1(S) = 0.9767873723305478
10/08/2021 14:26:02 - INFO - trainer -     T-F1(T) = 0.9181692094313454
10/08/2021 14:26:02 - INFO - trainer -     U-F1(A) = 0.7850467289719626
10/08/2021 14:26:02 - INFO - trainer -     U-F1(E) = 0.7535714285714287
10/08/2021 14:26:02 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:26:02 - INFO - trainer -     U-F1(O) = 0.9665791776027997
10/08/2021 14:26:02 - INFO - trainer -     intent_acc = 0.9383057090239411
10/08/2021 14:26:02 - INFO - trainer -     loss = 0.23300079256296158
10/08/2021 14:26:02 - INFO - trainer -     semantic_frame_acc = 0.8922651933701657
10/08/2021 14:26:02 - INFO - trainer -     slot_f1 = 0.9723741410741831
10/08/2021 14:26:02 - INFO - trainer -     slot_precision = 0.9766197183098592
10/08/2021 14:26:02 - INFO - trainer -     slot_recall = 0.9681653169505725

10/08/2021 14:26:02 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:26:02 - INFO - trainer -     T-F1 = 0.9736842105263158
10/08/2021 14:26:02 - INFO - trainer -     T-F1(C) = 0.9362318840579711
10/08/2021 14:26:02 - INFO - trainer -     T-F1(L) = 0.9404388714733541
10/08/2021 14:26:02 - INFO - trainer -     T-F1(O) = 0.99004329004329
10/08/2021 14:26:02 - INFO - trainer -     T-F1(P) = 0.9941383352872215
10/08/2021 14:26:02 - INFO - trainer -     T-F1(S) = 0.9767873723305478
10/08/2021 14:26:02 - INFO - trainer -     T-F1(T) = 0.9181692094313454
10/08/2021 14:26:02 - INFO - trainer -     U-F1(A) = 0.7850467289719626
10/08/2021 14:26:02 - INFO - trainer -     U-F1(E) = 0.7535714285714287
10/08/2021 14:26:02 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:26:02 - INFO - trainer -     U-F1(O) = 0.9665791776027997
10/08/2021 14:26:02 - INFO - trainer -     intent_acc = 0.9383057090239411
10/08/2021 14:26:02 - INFO - trainer -     semantic_frame_acc = 0.8922651933701657
10/08/2021 14:26:02 - INFO - trainer -     slot_f1 = 0.9723741410741831
10/08/2021 14:26:02 - INFO - trainer -     slot_precision = 0.9766197183098592
10/08/2021 14:26:02 - INFO - trainer -     slot_recall = 0.9681653169505725
10/08/2021 14:26:02 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:26:02 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:26:02 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:26:49 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 599/918 [02:33<01:13,  4.31it/s]
10/08/2021 14:26:49 - INFO - trainer -     Num examples = 3258
10/08/2021 14:26:49 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.20it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:26:56 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:26:56 - INFO - trainer -     T-F1 = 0.9790058241907085
10/08/2021 14:26:56 - INFO - trainer -     T-F1(C) = 0.9426573426573427
10/08/2021 14:26:56 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:26:56 - INFO - trainer -     T-F1(O) = 0.9915728809873321
10/08/2021 14:26:56 - INFO - trainer -     T-F1(P) = 0.993886462882096
10/08/2021 14:26:56 - INFO - trainer -     T-F1(S) = 0.9785681714546284
10/08/2021 14:26:56 - INFO - trainer -     T-F1(T) = 0.943502824858757
10/08/2021 14:26:56 - INFO - trainer -     U-F1(A) = 0.7777777777777778
10/08/2021 14:26:56 - INFO - trainer -     U-F1(E) = 0.7734241908006814
10/08/2021 14:26:56 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:26:56 - INFO - trainer -     U-F1(O) = 0.9676398170946182
10/08/2021 14:26:56 - INFO - trainer -     intent_acc = 0.939840392879067
10/08/2021 14:26:56 - INFO - trainer -     loss = 0.21850056756360858
10/08/2021 14:26:56 - INFO - trainer -     semantic_frame_acc = 0.9020871700429711
10/08/2021 14:26:56 - INFO - trainer -     slot_f1 = 0.9775498891352551
10/08/2021 14:26:56 - INFO - trainer -     slot_precision = 0.9702888583218707
10/08/2021 14:26:56 - INFO - trainer -     slot_recall = 0.9849204132923765

10/08/2021 14:26:56 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:26:56 - INFO - trainer -     T-F1 = 0.9790058241907085
10/08/2021 14:26:56 - INFO - trainer -     T-F1(C) = 0.9426573426573427
10/08/2021 14:26:56 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:26:56 - INFO - trainer -     T-F1(O) = 0.9915728809873321
10/08/2021 14:26:56 - INFO - trainer -     T-F1(P) = 0.993886462882096
10/08/2021 14:26:56 - INFO - trainer -     T-F1(S) = 0.9785681714546284
10/08/2021 14:26:56 - INFO - trainer -     T-F1(T) = 0.943502824858757
10/08/2021 14:26:56 - INFO - trainer -     U-F1(A) = 0.7777777777777778
10/08/2021 14:26:56 - INFO - trainer -     U-F1(E) = 0.7734241908006814
10/08/2021 14:26:56 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:26:56 - INFO - trainer -     U-F1(O) = 0.9676398170946182
10/08/2021 14:26:56 - INFO - trainer -     intent_acc = 0.939840392879067
10/08/2021 14:26:56 - INFO - trainer -     semantic_frame_acc = 0.9020871700429711
10/08/2021 14:26:56 - INFO - trainer -     slot_f1 = 0.9775498891352551
10/08/2021 14:26:56 - INFO - trainer -     slot_precision = 0.9702888583218707
10/08/2021 14:26:56 - INFO - trainer -     slot_recall = 0.9849204132923765
10/08/2021 14:26:56 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:26:57 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:26:57 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:27:43 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 799/918 [03:28<00:27,  4.27it/s]
10/08/2021 14:27:43 - INFO - trainer -     Num examples = 3258
10/08/2021 14:27:43 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.15it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:27:51 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:27:51 - INFO - trainer -     T-F1 = 0.9823129251700681
10/08/2021 14:27:51 - INFO - trainer -     T-F1(C) = 0.9581529581529582
10/08/2021 14:27:51 - INFO - trainer -     T-F1(L) = 0.973293768545994
10/08/2021 14:27:51 - INFO - trainer -     T-F1(O) = 0.9929447519808966
10/08/2021 14:27:51 - INFO - trainer -     T-F1(P) = 0.9950161243037232
10/08/2021 14:27:51 - INFO - trainer -     T-F1(S) = 0.9857471264367816
10/08/2021 14:27:51 - INFO - trainer -     T-F1(T) = 0.9400544959128065
10/08/2021 14:27:51 - INFO - trainer -     U-F1(A) = 0.7456140350877193
10/08/2021 14:27:51 - INFO - trainer -     U-F1(E) = 0.7460035523978686
10/08/2021 14:27:51 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:27:51 - INFO - trainer -     U-F1(O) = 0.967005967005967
10/08/2021 14:27:51 - INFO - trainer -     intent_acc = 0.9361571516267649
10/08/2021 14:27:51 - INFO - trainer -     loss = 0.2107438374968136
10/08/2021 14:27:51 - INFO - trainer -     semantic_frame_acc = 0.9057704112952731
10/08/2021 14:27:51 - INFO - trainer -     slot_f1 = 0.9810479375696767
10/08/2021 14:27:51 - INFO - trainer -     slot_precision = 0.9791376912378303
10/08/2021 14:27:51 - INFO - trainer -     slot_recall = 0.9829656520524993

10/08/2021 14:27:51 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:27:51 - INFO - trainer -     T-F1 = 0.9823129251700681
10/08/2021 14:27:51 - INFO - trainer -     T-F1(C) = 0.9581529581529582
10/08/2021 14:27:51 - INFO - trainer -     T-F1(L) = 0.973293768545994
10/08/2021 14:27:51 - INFO - trainer -     T-F1(O) = 0.9929447519808966
10/08/2021 14:27:51 - INFO - trainer -     T-F1(P) = 0.9950161243037232
10/08/2021 14:27:51 - INFO - trainer -     T-F1(S) = 0.9857471264367816
10/08/2021 14:27:51 - INFO - trainer -     T-F1(T) = 0.9400544959128065
10/08/2021 14:27:51 - INFO - trainer -     U-F1(A) = 0.7456140350877193
10/08/2021 14:27:51 - INFO - trainer -     U-F1(E) = 0.7460035523978686
10/08/2021 14:27:51 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:27:51 - INFO - trainer -     U-F1(O) = 0.967005967005967
10/08/2021 14:27:51 - INFO - trainer -     intent_acc = 0.9361571516267649
10/08/2021 14:27:51 - INFO - trainer -     semantic_frame_acc = 0.9057704112952731
10/08/2021 14:27:51 - INFO - trainer -     slot_f1 = 0.9810479375696767
10/08/2021 14:27:51 - INFO - trainer -     slot_precision = 0.9791376912378303
10/08/2021 14:27:51 - INFO - trainer -     slot_recall = 0.9829656520524993
10/08/2021 14:27:51 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:27:51 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:27:51 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
Iteration: 100%|| 918/918 [04:03<00:00,  3.76it/s]
Epoch:   5%|                                                                     | 1/20 [04:03<1:17:14, 243.95s/it]10/08/2021 14:28:38 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 81/918 [00:18<03:16,  4.26it/s]
10/08/2021 14:28:38 - INFO - trainer -     Num examples = 3258
10/08/2021 14:28:38 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.11it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:28:46 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:28:46 - INFO - trainer -     T-F1 = 0.9854322668481961
10/08/2021 14:28:46 - INFO - trainer -     T-F1(C) = 0.9668109668109668
10/08/2021 14:28:46 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 14:28:46 - INFO - trainer -     T-F1(O) = 0.9941945635071346
10/08/2021 14:28:46 - INFO - trainer -     T-F1(P) = 0.9958968347010552
10/08/2021 14:28:46 - INFO - trainer -     T-F1(S) = 0.9853211009174312
10/08/2021 14:28:46 - INFO - trainer -     T-F1(T) = 0.9556786703601109
10/08/2021 14:28:46 - INFO - trainer -     U-F1(A) = 0.7798165137614679
10/08/2021 14:28:46 - INFO - trainer -     U-F1(E) = 0.7667210440456771
10/08/2021 14:28:46 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:28:46 - INFO - trainer -     U-F1(O) = 0.9650053022269353
10/08/2021 14:28:46 - INFO - trainer -     intent_acc = 0.9361571516267649
10/08/2021 14:28:46 - INFO - trainer -     loss = 0.20411617294246076
10/08/2021 14:28:46 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/08/2021 14:28:46 - INFO - trainer -     slot_f1 = 0.9846796657381616
10/08/2021 14:28:46 - INFO - trainer -     slot_precision = 0.982217282578494
10/08/2021 14:28:46 - INFO - trainer -     slot_recall = 0.9871544261379503

10/08/2021 14:28:46 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:28:46 - INFO - trainer -     T-F1 = 0.9854322668481961
10/08/2021 14:28:46 - INFO - trainer -     T-F1(C) = 0.9668109668109668
10/08/2021 14:28:46 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 14:28:46 - INFO - trainer -     T-F1(O) = 0.9941945635071346
10/08/2021 14:28:46 - INFO - trainer -     T-F1(P) = 0.9958968347010552
10/08/2021 14:28:46 - INFO - trainer -     T-F1(S) = 0.9853211009174312
10/08/2021 14:28:46 - INFO - trainer -     T-F1(T) = 0.9556786703601109
10/08/2021 14:28:46 - INFO - trainer -     U-F1(A) = 0.7798165137614679
10/08/2021 14:28:46 - INFO - trainer -     U-F1(E) = 0.7667210440456771
10/08/2021 14:28:46 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:28:46 - INFO - trainer -     U-F1(O) = 0.9650053022269353
10/08/2021 14:28:46 - INFO - trainer -     intent_acc = 0.9361571516267649
10/08/2021 14:28:46 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/08/2021 14:28:46 - INFO - trainer -     slot_f1 = 0.9846796657381616
10/08/2021 14:28:46 - INFO - trainer -     slot_precision = 0.982217282578494
10/08/2021 14:28:46 - INFO - trainer -     slot_recall = 0.9871544261379503
10/08/2021 14:28:46 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:28:46 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:28:46 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:29:33 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 281/918 [01:13<02:29,  4.26it/s]
10/08/2021 14:29:33 - INFO - trainer -     Num examples = 3258
10/08/2021 14:29:33 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.10it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:29:41 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:29:41 - INFO - trainer -     T-F1 = 0.9846571622539037
10/08/2021 14:29:41 - INFO - trainer -     T-F1(C) = 0.9701280227596016
10/08/2021 14:29:41 - INFO - trainer -     T-F1(L) = 0.9616519174041298
10/08/2021 14:29:41 - INFO - trainer -     T-F1(O) = 0.9938623648905547
10/08/2021 14:29:41 - INFO - trainer -     T-F1(P) = 0.9961954931226222
10/08/2021 14:29:41 - INFO - trainer -     T-F1(S) = 0.9821673525377229
10/08/2021 14:29:41 - INFO - trainer -     T-F1(T) = 0.9624478442280946
10/08/2021 14:29:41 - INFO - trainer -     U-F1(A) = 0.767123287671233
10/08/2021 14:29:41 - INFO - trainer -     U-F1(E) = 0.7715736040609137
10/08/2021 14:29:41 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:29:41 - INFO - trainer -     U-F1(O) = 0.9660151435111815
10/08/2021 14:29:41 - INFO - trainer -     intent_acc = 0.9376918354818907
10/08/2021 14:29:41 - INFO - trainer -     loss = 0.1971564015337065
10/08/2021 14:29:41 - INFO - trainer -     semantic_frame_acc = 0.9097605893186004
10/08/2021 14:29:41 - INFO - trainer -     slot_f1 = 0.9838933629547347
10/08/2021 14:29:41 - INFO - trainer -     slot_precision = 0.9784589892294946
10/08/2021 14:29:41 - INFO - trainer -     slot_recall = 0.9893884389835241

10/08/2021 14:29:41 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:29:41 - INFO - trainer -     T-F1 = 0.9854322668481961
10/08/2021 14:29:41 - INFO - trainer -     T-F1(C) = 0.9668109668109668
10/08/2021 14:29:41 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 14:29:41 - INFO - trainer -     T-F1(O) = 0.9941945635071346
10/08/2021 14:29:41 - INFO - trainer -     T-F1(P) = 0.9958968347010552
10/08/2021 14:29:41 - INFO - trainer -     T-F1(S) = 0.9853211009174312
10/08/2021 14:29:41 - INFO - trainer -     T-F1(T) = 0.9556786703601109
10/08/2021 14:29:41 - INFO - trainer -     U-F1(A) = 0.7798165137614679
10/08/2021 14:29:41 - INFO - trainer -     U-F1(E) = 0.7667210440456771
10/08/2021 14:29:41 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:29:41 - INFO - trainer -     U-F1(O) = 0.9650053022269353
10/08/2021 14:29:41 - INFO - trainer -     intent_acc = 0.9361571516267649
10/08/2021 14:29:41 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/08/2021 14:29:41 - INFO - trainer -     slot_f1 = 0.9846796657381616
10/08/2021 14:29:41 - INFO - trainer -     slot_precision = 0.982217282578494
10/08/2021 14:29:41 - INFO - trainer -     slot_recall = 0.9871544261379503
                                                                                                                       10/08/2021 14:30:27 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 481/918 [02:08<01:42,  4.28it/s]
10/08/2021 14:30:27 - INFO - trainer -     Num examples = 3258
10/08/2021 14:30:27 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.08it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:30:35 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:30:35 - INFO - trainer -     T-F1 = 0.9880903490759754
10/08/2021 14:30:35 - INFO - trainer -     T-F1(C) = 0.9725036179450074
10/08/2021 14:30:35 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:30:35 - INFO - trainer -     T-F1(O) = 0.9952899139191164
10/08/2021 14:30:35 - INFO - trainer -     T-F1(P) = 0.9953106682297773
10/08/2021 14:30:35 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 14:30:35 - INFO - trainer -     T-F1(T) = 0.9660056657223796
10/08/2021 14:30:35 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:30:35 - INFO - trainer -     U-F1(E) = 0.7827586206896552
10/08/2021 14:30:35 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:30:35 - INFO - trainer -     U-F1(O) = 0.9676739283204496
10/08/2021 14:30:35 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:30:35 - INFO - trainer -     loss = 0.19773036979284941
10/08/2021 14:30:35 - INFO - trainer -     semantic_frame_acc = 0.9201964395334561
10/08/2021 14:30:35 - INFO - trainer -     slot_f1 = 0.9873984878185381
10/08/2021 14:30:35 - INFO - trainer -     slot_precision = 0.990171300196574
10/08/2021 14:30:35 - INFO - trainer -     slot_recall = 0.9846411616866797

10/08/2021 14:30:35 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:30:35 - INFO - trainer -     T-F1 = 0.9880903490759754
10/08/2021 14:30:35 - INFO - trainer -     T-F1(C) = 0.9725036179450074
10/08/2021 14:30:35 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:30:35 - INFO - trainer -     T-F1(O) = 0.9952899139191164
10/08/2021 14:30:35 - INFO - trainer -     T-F1(P) = 0.9953106682297773
10/08/2021 14:30:35 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 14:30:35 - INFO - trainer -     T-F1(T) = 0.9660056657223796
10/08/2021 14:30:35 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:30:35 - INFO - trainer -     U-F1(E) = 0.7827586206896552
10/08/2021 14:30:35 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:30:35 - INFO - trainer -     U-F1(O) = 0.9676739283204496
10/08/2021 14:30:35 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:30:35 - INFO - trainer -     semantic_frame_acc = 0.9201964395334561
10/08/2021 14:30:35 - INFO - trainer -     slot_f1 = 0.9873984878185381
10/08/2021 14:30:35 - INFO - trainer -     slot_precision = 0.990171300196574
10/08/2021 14:30:35 - INFO - trainer -     slot_recall = 0.9846411616866797
10/08/2021 14:30:35 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:30:36 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:30:36 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:31:23 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 681/918 [03:03<00:55,  4.26it/s]
10/08/2021 14:31:23 - INFO - trainer -     Num examples = 3258
10/08/2021 14:31:23 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.08it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:31:30 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:31:30 - INFO - trainer -     T-F1 = 0.9880968668764536
10/08/2021 14:31:30 - INFO - trainer -     T-F1(C) = 0.9753265602322206
10/08/2021 14:31:30 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:31:30 - INFO - trainer -     T-F1(O) = 0.9952888937022798
10/08/2021 14:31:30 - INFO - trainer -     T-F1(P) = 0.9947214076246335
10/08/2021 14:31:30 - INFO - trainer -     T-F1(S) = 0.991635687732342
10/08/2021 14:31:30 - INFO - trainer -     T-F1(T) = 0.9614325068870524
10/08/2021 14:31:30 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:31:30 - INFO - trainer -     U-F1(E) = 0.7238095238095239
10/08/2021 14:31:30 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:31:30 - INFO - trainer -     U-F1(O) = 0.9658655520724487
10/08/2021 14:31:30 - INFO - trainer -     intent_acc = 0.9355432780847146
10/08/2021 14:31:30 - INFO - trainer -     loss = 0.1933807827094022
10/08/2021 14:31:30 - INFO - trainer -     semantic_frame_acc = 0.9158993247391037
10/08/2021 14:31:30 - INFO - trainer -     slot_f1 = 0.9873984878185381
10/08/2021 14:31:30 - INFO - trainer -     slot_precision = 0.990171300196574
10/08/2021 14:31:30 - INFO - trainer -     slot_recall = 0.9846411616866797

10/08/2021 14:31:30 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:31:30 - INFO - trainer -     T-F1 = 0.9880903490759754
10/08/2021 14:31:30 - INFO - trainer -     T-F1(C) = 0.9725036179450074
10/08/2021 14:31:30 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:31:30 - INFO - trainer -     T-F1(O) = 0.9952899139191164
10/08/2021 14:31:30 - INFO - trainer -     T-F1(P) = 0.9953106682297773
10/08/2021 14:31:30 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 14:31:30 - INFO - trainer -     T-F1(T) = 0.9660056657223796
10/08/2021 14:31:30 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:31:30 - INFO - trainer -     U-F1(E) = 0.7827586206896552
10/08/2021 14:31:30 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:31:30 - INFO - trainer -     U-F1(O) = 0.9676739283204496
10/08/2021 14:31:30 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:31:30 - INFO - trainer -     semantic_frame_acc = 0.9201964395334561
10/08/2021 14:31:30 - INFO - trainer -     slot_f1 = 0.9873984878185381
10/08/2021 14:31:30 - INFO - trainer -     slot_precision = 0.990171300196574
10/08/2021 14:31:30 - INFO - trainer -     slot_recall = 0.9846411616866797
                                                                                                                       10/08/2021 14:32:17 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 881/918 [03:58<00:08,  4.25it/s]
10/08/2021 14:32:17 - INFO - trainer -     Num examples = 3258
10/08/2021 14:32:17 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.06it/s]
10/08/2021 14:32:25 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:32:25 - INFO - trainer -     T-F1 = 0.9892209032610179| 51/51 [00:07<00:00,  7.21it/s]
10/08/2021 14:32:25 - INFO - trainer -     T-F1(C) = 0.9738372093023255
10/08/2021 14:32:25 - INFO - trainer -     T-F1(L) = 0.984984984984985
10/08/2021 14:32:25 - INFO - trainer -     T-F1(O) = 0.9957174608337399
10/08/2021 14:32:25 - INFO - trainer -     T-F1(P) = 0.9976594499707432
10/08/2021 14:32:25 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 14:32:25 - INFO - trainer -     T-F1(T) = 0.9641873278236915
10/08/2021 14:32:25 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:32:25 - INFO - trainer -     U-F1(E) = 0.7625418060200669
10/08/2021 14:32:25 - INFO - trainer -     U-F1(I) = 0.09302325581395349
10/08/2021 14:32:25 - INFO - trainer -     U-F1(O) = 0.9640898637891385
10/08/2021 14:32:25 - INFO - trainer -     intent_acc = 0.9330877839165131
10/08/2021 14:32:25 - INFO - trainer -     loss = 0.18736241896655045
10/08/2021 14:32:25 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/08/2021 14:32:25 - INFO - trainer -     slot_f1 = 0.9884094400223433
10/08/2021 14:32:25 - INFO - trainer -     slot_precision = 0.9885474860335196
10/08/2021 14:32:25 - INFO - trainer -     slot_recall = 0.9882714325607372

10/08/2021 14:32:25 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:32:25 - INFO - trainer -     T-F1 = 0.9880903490759754
10/08/2021 14:32:25 - INFO - trainer -     T-F1(C) = 0.9725036179450074
10/08/2021 14:32:25 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 14:32:25 - INFO - trainer -     T-F1(O) = 0.9952899139191164
10/08/2021 14:32:25 - INFO - trainer -     T-F1(P) = 0.9953106682297773
10/08/2021 14:32:25 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 14:32:25 - INFO - trainer -     T-F1(T) = 0.9660056657223796
10/08/2021 14:32:25 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:32:25 - INFO - trainer -     U-F1(E) = 0.7827586206896552
10/08/2021 14:32:25 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:32:25 - INFO - trainer -     U-F1(O) = 0.9676739283204496
10/08/2021 14:32:25 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:32:25 - INFO - trainer -     semantic_frame_acc = 0.9201964395334561
10/08/2021 14:32:25 - INFO - trainer -     slot_f1 = 0.9873984878185381
10/08/2021 14:32:25 - INFO - trainer -     slot_precision = 0.990171300196574
10/08/2021 14:32:25 - INFO - trainer -     slot_recall = 0.9846411616866797
Iteration: 100%|| 918/918 [04:14<00:00,  3.61it/s]
Epoch:  10%|                                                                 | 2/20 [08:18<1:15:00, 250.05s/it]10/08/2021 14:33:12 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 163/918 [00:38<02:56,  4.28it/s]
10/08/2021 14:33:12 - INFO - trainer -     Num examples = 3258
10/08/2021 14:33:12 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 14:33:19 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:33:19 - INFO - trainer -     T-F1 = 0.9896004378763| 51/51 [00:07<00:00,  7.19it/s]
10/08/2021 14:33:19 - INFO - trainer -     T-F1(C) = 0.9753265602322206
10/08/2021 14:33:19 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:33:19 - INFO - trainer -     T-F1(O) = 0.9958847736625515
10/08/2021 14:33:19 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/08/2021 14:33:19 - INFO - trainer -     T-F1(S) = 0.9916279069767441
10/08/2021 14:33:19 - INFO - trainer -     T-F1(T) = 0.9678321678321679
10/08/2021 14:33:19 - INFO - trainer -     U-F1(A) = 0.7706422018348624
10/08/2021 14:33:19 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:33:19 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/08/2021 14:33:19 - INFO - trainer -     U-F1(O) = 0.9676511954992967
10/08/2021 14:33:19 - INFO - trainer -     intent_acc = 0.939840392879067
10/08/2021 14:33:19 - INFO - trainer -     loss = 0.19761825791176627
10/08/2021 14:33:19 - INFO - trainer -     semantic_frame_acc = 0.9220380601596071
10/08/2021 14:33:19 - INFO - trainer -     slot_f1 = 0.989223233030091
10/08/2021 14:33:19 - INFO - trainer -     slot_precision = 0.9915824915824916
10/08/2021 14:33:19 - INFO - trainer -     slot_recall = 0.9868751745322536

10/08/2021 14:33:19 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:33:19 - INFO - trainer -     T-F1 = 0.9896004378763
10/08/2021 14:33:19 - INFO - trainer -     T-F1(C) = 0.9753265602322206
10/08/2021 14:33:19 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:33:19 - INFO - trainer -     T-F1(O) = 0.9958847736625515
10/08/2021 14:33:19 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/08/2021 14:33:19 - INFO - trainer -     T-F1(S) = 0.9916279069767441
10/08/2021 14:33:19 - INFO - trainer -     T-F1(T) = 0.9678321678321679
10/08/2021 14:33:19 - INFO - trainer -     U-F1(A) = 0.7706422018348624
10/08/2021 14:33:19 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:33:19 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/08/2021 14:33:19 - INFO - trainer -     U-F1(O) = 0.9676511954992967
10/08/2021 14:33:19 - INFO - trainer -     intent_acc = 0.939840392879067
10/08/2021 14:33:19 - INFO - trainer -     semantic_frame_acc = 0.9220380601596071
10/08/2021 14:33:19 - INFO - trainer -     slot_f1 = 0.989223233030091
10/08/2021 14:33:19 - INFO - trainer -     slot_precision = 0.9915824915824916
10/08/2021 14:33:19 - INFO - trainer -     slot_recall = 0.9868751745322536
10/08/2021 14:33:19 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:33:20 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:33:20 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:34:07 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 363/918 [01:33<02:10,  4.26it/s]
10/08/2021 14:34:07 - INFO - trainer -     Num examples = 3258
10/08/2021 14:34:07 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.05it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:34:14 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:34:14 - INFO - trainer -     T-F1 = 0.9887763482069531
10/08/2021 14:34:14 - INFO - trainer -     T-F1(C) = 0.9739130434782609
10/08/2021 14:34:14 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 14:34:14 - INFO - trainer -     T-F1(O) = 0.9955603681645913
10/08/2021 14:34:14 - INFO - trainer -     T-F1(P) = 0.9956024626209323
10/08/2021 14:34:14 - INFO - trainer -     T-F1(S) = 0.9921186833565137
10/08/2021 14:34:14 - INFO - trainer -     T-F1(T) = 0.9621318373071528
10/08/2021 14:34:14 - INFO - trainer -     U-F1(A) = 0.7777777777777778
10/08/2021 14:34:14 - INFO - trainer -     U-F1(E) = 0.7368421052631579
10/08/2021 14:34:14 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:34:14 - INFO - trainer -     U-F1(O) = 0.9656854206584219
10/08/2021 14:34:14 - INFO - trainer -     intent_acc = 0.9367710251688153
10/08/2021 14:34:14 - INFO - trainer -     loss = 0.1995681427127006
10/08/2021 14:34:14 - INFO - trainer -     semantic_frame_acc = 0.9177409453652547
10/08/2021 14:34:14 - INFO - trainer -     slot_f1 = 0.9879585550266032
10/08/2021 14:34:14 - INFO - trainer -     slot_precision = 0.9907329401853412
10/08/2021 14:34:14 - INFO - trainer -     slot_recall = 0.9851996648980732

10/08/2021 14:34:14 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:34:14 - INFO - trainer -     T-F1 = 0.9896004378763
10/08/2021 14:34:14 - INFO - trainer -     T-F1(C) = 0.9753265602322206
10/08/2021 14:34:14 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:34:14 - INFO - trainer -     T-F1(O) = 0.9958847736625515
10/08/2021 14:34:14 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/08/2021 14:34:14 - INFO - trainer -     T-F1(S) = 0.9916279069767441
10/08/2021 14:34:14 - INFO - trainer -     T-F1(T) = 0.9678321678321679
10/08/2021 14:34:14 - INFO - trainer -     U-F1(A) = 0.7706422018348624
10/08/2021 14:34:14 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:34:14 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/08/2021 14:34:14 - INFO - trainer -     U-F1(O) = 0.9676511954992967
10/08/2021 14:34:14 - INFO - trainer -     intent_acc = 0.939840392879067
10/08/2021 14:34:14 - INFO - trainer -     semantic_frame_acc = 0.9220380601596071
10/08/2021 14:34:14 - INFO - trainer -     slot_f1 = 0.989223233030091
10/08/2021 14:34:14 - INFO - trainer -     slot_precision = 0.9915824915824916
10/08/2021 14:34:14 - INFO - trainer -     slot_recall = 0.9868751745322536
                                                                                                                       10/08/2021 14:35:02 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 563/918 [02:28<01:25,  4.17it/s]
10/08/2021 14:35:02 - INFO - trainer -     Num examples = 3258
10/08/2021 14:35:02 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:35:10 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:35:10 - INFO - trainer -     T-F1 = 0.9889177726091121
10/08/2021 14:35:10 - INFO - trainer -     T-F1(C) = 0.9766763848396501
10/08/2021 14:35:10 - INFO - trainer -     T-F1(L) = 0.9793510324483775
10/08/2021 14:35:10 - INFO - trainer -     T-F1(O) = 0.9956137975848811
10/08/2021 14:35:10 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:35:10 - INFO - trainer -     T-F1(S) = 0.9907149489322191
10/08/2021 14:35:10 - INFO - trainer -     T-F1(T) = 0.9651324965132496
10/08/2021 14:35:10 - INFO - trainer -     U-F1(A) = 0.776255707762557
10/08/2021 14:35:10 - INFO - trainer -     U-F1(E) = 0.7309833024118738
10/08/2021 14:35:10 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:35:10 - INFO - trainer -     U-F1(O) = 0.9652765660443203
10/08/2021 14:35:10 - INFO - trainer -     intent_acc = 0.9355432780847146
10/08/2021 14:35:10 - INFO - trainer -     loss = 0.19580737822780422
10/08/2021 14:35:10 - INFO - trainer -     semantic_frame_acc = 0.9168201350521793
10/08/2021 14:35:10 - INFO - trainer -     slot_f1 = 0.9883802323953521
10/08/2021 14:35:10 - INFO - trainer -     slot_precision = 0.9910162829870859
10/08/2021 14:35:10 - INFO - trainer -     slot_recall = 0.9857581681094666

10/08/2021 14:35:10 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:35:10 - INFO - trainer -     T-F1 = 0.9896004378763
10/08/2021 14:35:10 - INFO - trainer -     T-F1(C) = 0.9753265602322206
10/08/2021 14:35:10 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:35:10 - INFO - trainer -     T-F1(O) = 0.9958847736625515
10/08/2021 14:35:10 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/08/2021 14:35:10 - INFO - trainer -     T-F1(S) = 0.9916279069767441
10/08/2021 14:35:10 - INFO - trainer -     T-F1(T) = 0.9678321678321679
10/08/2021 14:35:10 - INFO - trainer -     U-F1(A) = 0.7706422018348624
10/08/2021 14:35:10 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:35:10 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/08/2021 14:35:10 - INFO - trainer -     U-F1(O) = 0.9676511954992967
10/08/2021 14:35:10 - INFO - trainer -     intent_acc = 0.939840392879067
10/08/2021 14:35:10 - INFO - trainer -     semantic_frame_acc = 0.9220380601596071
10/08/2021 14:35:10 - INFO - trainer -     slot_f1 = 0.989223233030091
10/08/2021 14:35:10 - INFO - trainer -     slot_precision = 0.9915824915824916
10/08/2021 14:35:10 - INFO - trainer -     slot_recall = 0.9868751745322536
                                                                                                                       10/08/2021 14:35:58 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 763/918 [03:24<00:36,  4.22it/s]
10/08/2021 14:35:58 - INFO - trainer -     Num examples = 3258
10/08/2021 14:35:58 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.99it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:36:05 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:36:05 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:36:05 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:36:05 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:36:05 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:36:05 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:36:05 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:36:05 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:36:05 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:36:05 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:36:05 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:36:05 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:36:05 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:36:05 - INFO - trainer -     loss = 0.1900238378667364
10/08/2021 14:36:05 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:36:05 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:36:05 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:36:05 - INFO - trainer -     slot_recall = 0.9854789165037698

10/08/2021 14:36:05 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:36:05 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:36:05 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:36:05 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:36:05 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:36:05 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:36:05 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:36:05 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:36:05 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:36:05 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:36:05 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:36:05 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:36:05 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:36:05 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:36:05 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:36:05 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:36:05 - INFO - trainer -     slot_recall = 0.9854789165037698
10/08/2021 14:36:05 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:36:06 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:36:06 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
Iteration: 100%|| 918/918 [04:09<00:00,  3.68it/s]
Epoch:  15%|                                                              | 3/20 [12:27<1:10:44, 249.71s/it]10/08/2021 14:36:53 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 45/918 [00:10<03:26,  4.22it/s]
10/08/2021 14:36:53 - INFO - trainer -     Num examples = 3258
10/08/2021 14:36:53 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.02it/s]
10/08/2021 14:37:01 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:37:01 - INFO - trainer -     T-F1 = 0.989623156744948| 51/51 [00:07<00:00,  7.13it/s]
10/08/2021 14:37:01 - INFO - trainer -     T-F1(C) = 0.9756097560975611
10/08/2021 14:37:01 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 14:37:01 - INFO - trainer -     T-F1(O) = 0.9958812052893996
10/08/2021 14:37:01 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/08/2021 14:37:01 - INFO - trainer -     T-F1(S) = 0.9907749077490775
10/08/2021 14:37:01 - INFO - trainer -     T-F1(T) = 0.9660056657223796
10/08/2021 14:37:01 - INFO - trainer -     U-F1(A) = 0.7706422018348624
10/08/2021 14:37:01 - INFO - trainer -     U-F1(E) = 0.7530224525043178
10/08/2021 14:37:01 - INFO - trainer -     U-F1(I) = 0.22857142857142856
10/08/2021 14:37:01 - INFO - trainer -     U-F1(O) = 0.9655172413793104
10/08/2021 14:37:01 - INFO - trainer -     intent_acc = 0.9361571516267649
10/08/2021 14:37:01 - INFO - trainer -     loss = 0.21042435221812306
10/08/2021 14:37:01 - INFO - trainer -     semantic_frame_acc = 0.9174340085942296
10/08/2021 14:37:01 - INFO - trainer -     slot_f1 = 0.9889680212260857
10/08/2021 14:37:01 - INFO - trainer -     slot_precision = 0.9891061452513966
10/08/2021 14:37:01 - INFO - trainer -     slot_recall = 0.9888299357721307

10/08/2021 14:37:01 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:37:01 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:37:01 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:37:01 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:37:01 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:37:01 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:37:01 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:37:01 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:37:01 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:37:01 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:37:01 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:37:01 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:37:01 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:37:01 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:37:01 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:37:01 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:37:01 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:37:49 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 245/918 [01:05<02:39,  4.22it/s]
10/08/2021 14:37:49 - INFO - trainer -     Num examples = 3258
10/08/2021 14:37:49 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.02it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:37:56 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:37:56 - INFO - trainer -     T-F1 = 0.9849857973758962
10/08/2021 14:37:56 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/08/2021 14:37:56 - INFO - trainer -     T-F1(L) = 0.9794721407624633
10/08/2021 14:37:56 - INFO - trainer -     T-F1(O) = 0.9940706087145733
10/08/2021 14:37:56 - INFO - trainer -     T-F1(P) = 0.9947582993593477
10/08/2021 14:37:56 - INFO - trainer -     T-F1(S) = 0.9826800364630811
10/08/2021 14:37:56 - INFO - trainer -     T-F1(T) = 0.9626556016597512
10/08/2021 14:37:56 - INFO - trainer -     U-F1(A) = 0.6853932584269663
10/08/2021 14:37:56 - INFO - trainer -     U-F1(E) = 0.7364341085271319
10/08/2021 14:37:56 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:37:56 - INFO - trainer -     U-F1(O) = 0.9642795513373599
10/08/2021 14:37:56 - INFO - trainer -     intent_acc = 0.934622467771639
10/08/2021 14:37:56 - INFO - trainer -     loss = 0.23980025698741278
10/08/2021 14:37:56 - INFO - trainer -     semantic_frame_acc = 0.9066912216083487
10/08/2021 14:37:56 - INFO - trainer -     slot_f1 = 0.9839468585662883
10/08/2021 14:37:56 - INFO - trainer -     slot_precision = 0.9753086419753086
10/08/2021 14:37:56 - INFO - trainer -     slot_recall = 0.9927394582518849

10/08/2021 14:37:56 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:37:56 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:37:56 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:37:56 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:37:56 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:37:56 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:37:56 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:37:56 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:37:56 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:37:56 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:37:56 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:37:56 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:37:56 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:37:56 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:37:56 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:37:56 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:37:56 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:38:44 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 445/918 [02:00<01:52,  4.21it/s]
10/08/2021 14:38:44 - INFO - trainer -     Num examples = 3258
10/08/2021 14:38:44 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.00it/s]
10/08/2021 14:38:51 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:38:51 - INFO - trainer -     T-F1 = 0.988926862611073| 51/51 [00:07<00:00,  7.13it/s]
10/08/2021 14:38:51 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/08/2021 14:38:51 - INFO - trainer -     T-F1(L) = 0.984984984984985
10/08/2021 14:38:51 - INFO - trainer -     T-F1(O) = 0.9956123720275174
10/08/2021 14:38:51 - INFO - trainer -     T-F1(P) = 0.9950161243037232
10/08/2021 14:38:51 - INFO - trainer -     T-F1(S) = 0.9925719591457753
10/08/2021 14:38:51 - INFO - trainer -     T-F1(T) = 0.9706293706293706
10/08/2021 14:38:51 - INFO - trainer -     U-F1(A) = 0.7798165137614679
10/08/2021 14:38:51 - INFO - trainer -     U-F1(E) = 0.69215291750503
10/08/2021 14:38:51 - INFO - trainer -     U-F1(I) = 0.25
10/08/2021 14:38:51 - INFO - trainer -     U-F1(O) = 0.9630272522131574
10/08/2021 14:38:51 - INFO - trainer -     intent_acc = 0.9318600368324125
10/08/2021 14:38:51 - INFO - trainer -     loss = 0.19954991106893502
10/08/2021 14:38:51 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/08/2021 14:38:51 - INFO - trainer -     slot_f1 = 0.9885346756152126
10/08/2021 14:38:51 - INFO - trainer -     slot_precision = 0.9899187902548305
10/08/2021 14:38:51 - INFO - trainer -     slot_recall = 0.9871544261379503

10/08/2021 14:38:51 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:38:51 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:38:51 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:38:51 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:38:51 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:38:51 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:38:51 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:38:51 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:38:51 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:38:51 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:38:51 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:38:51 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:38:51 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:38:51 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:38:51 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:38:51 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:38:51 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:39:39 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 645/918 [02:55<01:03,  4.27it/s]
10/08/2021 14:39:39 - INFO - trainer -     Num examples = 3258
10/08/2021 14:39:39 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.01it/s]
10/08/2021 14:39:46 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:39:46 - INFO - trainer -     T-F1 = 0.9886874744445959| 51/51 [00:07<00:00,  7.14it/s]
10/08/2021 14:39:46 - INFO - trainer -     T-F1(C) = 0.9740634005763689
10/08/2021 14:39:46 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 14:39:46 - INFO - trainer -     T-F1(O) = 0.9954986712945388
10/08/2021 14:39:46 - INFO - trainer -     T-F1(P) = 0.9976608187134504
10/08/2021 14:39:46 - INFO - trainer -     T-F1(S) = 0.986200551977921
10/08/2021 14:39:46 - INFO - trainer -     T-F1(T) = 0.9691876750700281
10/08/2021 14:39:46 - INFO - trainer -     U-F1(A) = 0.767123287671233
10/08/2021 14:39:46 - INFO - trainer -     U-F1(E) = 0.7439252336448597
10/08/2021 14:39:46 - INFO - trainer -     U-F1(I) = 0.27450980392156865
10/08/2021 14:39:46 - INFO - trainer -     U-F1(O) = 0.9651549641043602
10/08/2021 14:39:46 - INFO - trainer -     intent_acc = 0.9349294045426643
10/08/2021 14:39:46 - INFO - trainer -     loss = 0.2099570482969284
10/08/2021 14:39:46 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/08/2021 14:39:46 - INFO - trainer -     slot_f1 = 0.9881500069705841
10/08/2021 14:39:46 - INFO - trainer -     slot_precision = 0.9866369710467706
10/08/2021 14:39:46 - INFO - trainer -     slot_recall = 0.9896676905892209

10/08/2021 14:39:46 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:39:46 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:39:46 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:39:46 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:39:46 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:39:46 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:39:46 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:39:46 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:39:46 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:39:46 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:39:46 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:39:46 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:39:46 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:39:46 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:39:46 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:39:46 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:39:46 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:40:33 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 845/918 [03:50<00:17,  4.25it/s]
10/08/2021 14:40:33 - INFO - trainer -     Num examples = 3258
10/08/2021 14:40:33 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.88it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/08/2021 14:40:41 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:40:41 - INFO - trainer -     T-F1 = 0.9897834082550061
10/08/2021 14:40:41 - INFO - trainer -     T-F1(C) = 0.9767441860465117
10/08/2021 14:40:41 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 14:40:41 - INFO - trainer -     T-F1(O) = 0.9959316517493897
10/08/2021 14:40:41 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/08/2021 14:40:41 - INFO - trainer -     T-F1(S) = 0.9884951679705476
10/08/2021 14:40:41 - INFO - trainer -     T-F1(T) = 0.9724517906336088
10/08/2021 14:40:41 - INFO - trainer -     U-F1(A) = 0.776255707762557
10/08/2021 14:40:41 - INFO - trainer -     U-F1(E) = 0.7718120805369127
10/08/2021 14:40:41 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:40:41 - INFO - trainer -     U-F1(O) = 0.9672188931970391
10/08/2021 14:40:41 - INFO - trainer -     intent_acc = 0.9389195825659914
10/08/2021 14:40:41 - INFO - trainer -     loss = 0.19961152883136973
10/08/2021 14:40:41 - INFO - trainer -     semantic_frame_acc = 0.9201964395334561
10/08/2021 14:40:41 - INFO - trainer -     slot_f1 = 0.9894091415830545
10/08/2021 14:40:41 - INFO - trainer -     slot_precision = 0.9874826147426982
10/08/2021 14:40:41 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 14:40:41 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:40:41 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:40:41 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:40:41 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:40:41 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:40:41 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:40:41 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:40:41 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:40:41 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:40:41 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:40:41 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:40:41 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:40:41 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:40:41 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:40:41 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:40:41 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:40:41 - INFO - trainer -     slot_recall = 0.9854789165037698
Iteration: 100%|| 918/918 [04:15<00:00,  3.59it/s]
Epoch:  20%|                                                          | 4/20 [16:43<1:07:13, 252.12s/it]10/08/2021 14:41:28 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 127/918 [00:29<03:06,  4.24it/s]
10/08/2021 14:41:28 - INFO - trainer -     Num examples = 3258
10/08/2021 14:41:28 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 14:41:36 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:41:36 - INFO - trainer -     T-F1 = 0.9910032715376227| 51/51 [00:07<00:00,  7.18it/s]
10/08/2021 14:41:36 - INFO - trainer -     T-F1(C) = 0.9715909090909091
10/08/2021 14:41:36 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 14:41:36 - INFO - trainer -     T-F1(O) = 0.9964208242950109
10/08/2021 14:41:36 - INFO - trainer -     T-F1(P) = 0.9970760233918129
10/08/2021 14:41:36 - INFO - trainer -     T-F1(S) = 0.993058769088385
10/08/2021 14:41:36 - INFO - trainer -     T-F1(T) = 0.9762237762237763
10/08/2021 14:41:36 - INFO - trainer -     U-F1(A) = 0.767123287671233
10/08/2021 14:41:36 - INFO - trainer -     U-F1(E) = 0.7114624505928856
10/08/2021 14:41:36 - INFO - trainer -     U-F1(I) = 0.25
10/08/2021 14:41:36 - INFO - trainer -     U-F1(O) = 0.9635571054925894
10/08/2021 14:41:36 - INFO - trainer -     intent_acc = 0.9312461632903621
10/08/2021 14:41:36 - INFO - trainer -     loss = 0.23132214885131985
10/08/2021 14:41:36 - INFO - trainer -     semantic_frame_acc = 0.9158993247391037
10/08/2021 14:41:36 - INFO - trainer -     slot_f1 = 0.99079754601227
10/08/2021 14:41:36 - INFO - trainer -     slot_precision = 0.9894179894179894
10/08/2021 14:41:36 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 14:41:36 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:41:36 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:41:36 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:41:36 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:41:36 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:41:36 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:41:36 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:41:36 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:41:36 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:41:36 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:41:36 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:41:36 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:41:36 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:41:36 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:41:36 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:41:36 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:41:36 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:42:23 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 327/918 [01:24<02:18,  4.27it/s]
10/08/2021 14:42:23 - INFO - trainer -     Num examples = 3258
10/08/2021 14:42:23 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.88it/s]
10/08/2021 14:42:31 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:42:31 - INFO - trainer -     T-F1 = 0.9890978468247479| 51/51 [00:07<00:00,  7.18it/s]
10/08/2021 14:42:31 - INFO - trainer -     T-F1(C) = 0.9726618705035971
10/08/2021 14:42:31 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 14:42:31 - INFO - trainer -     T-F1(O) = 0.9956611346132986
10/08/2021 14:42:31 - INFO - trainer -     T-F1(P) = 0.997072599531616
10/08/2021 14:42:31 - INFO - trainer -     T-F1(S) = 0.9866789159393661
10/08/2021 14:42:31 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/08/2021 14:42:31 - INFO - trainer -     U-F1(A) = 0.7533632286995515
10/08/2021 14:42:31 - INFO - trainer -     U-F1(E) = 0.7355072463768115
10/08/2021 14:42:31 - INFO - trainer -     U-F1(I) = 0.3255813953488372
10/08/2021 14:42:31 - INFO - trainer -     U-F1(O) = 0.965952965952966
10/08/2021 14:42:31 - INFO - trainer -     intent_acc = 0.9349294045426643
10/08/2021 14:42:31 - INFO - trainer -     loss = 0.2093451587738944
10/08/2021 14:42:31 - INFO - trainer -     semantic_frame_acc = 0.9158993247391037
10/08/2021 14:42:31 - INFO - trainer -     slot_f1 = 0.988848620016727
10/08/2021 14:42:31 - INFO - trainer -     slot_precision = 0.9871973281380462
10/08/2021 14:42:31 - INFO - trainer -     slot_recall = 0.9905054454063111

10/08/2021 14:42:31 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:42:31 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:42:31 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:42:31 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:42:31 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:42:31 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:42:31 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:42:31 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:42:31 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:42:31 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:42:31 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:42:31 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:42:31 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:42:31 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:42:31 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:42:31 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:42:31 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:43:18 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 527/918 [02:19<01:32,  4.24it/s]
10/08/2021 14:43:18 - INFO - trainer -     Num examples = 3258
10/08/2021 14:43:18 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 14:43:26 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:43:26 - INFO - trainer -     T-F1 = 0.9909762100082035| 51/51 [00:07<00:00,  7.18it/s]
10/08/2021 14:43:26 - INFO - trainer -     T-F1(C) = 0.9713467048710601
10/08/2021 14:43:26 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/08/2021 14:43:26 - INFO - trainer -     T-F1(O) = 0.9964250893727656
10/08/2021 14:43:26 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:43:26 - INFO - trainer -     T-F1(S) = 0.9911668991166899
10/08/2021 14:43:26 - INFO - trainer -     T-F1(T) = 0.9774647887323944
10/08/2021 14:43:26 - INFO - trainer -     U-F1(A) = 0.7466666666666667
10/08/2021 14:43:26 - INFO - trainer -     U-F1(E) = 0.7728813559322034
10/08/2021 14:43:26 - INFO - trainer -     U-F1(I) = 0.22727272727272727
10/08/2021 14:43:26 - INFO - trainer -     U-F1(O) = 0.9648223439985858
10/08/2021 14:43:26 - INFO - trainer -     intent_acc = 0.9349294045426643
10/08/2021 14:43:26 - INFO - trainer -     loss = 0.20102759063536046
10/08/2021 14:43:26 - INFO - trainer -     semantic_frame_acc = 0.91804788213628
10/08/2021 14:43:26 - INFO - trainer -     slot_f1 = 0.9907692307692308
10/08/2021 14:43:26 - INFO - trainer -     slot_precision = 0.9924348557018773
10/08/2021 14:43:26 - INFO - trainer -     slot_recall = 0.9891091873778274

10/08/2021 14:43:26 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:43:26 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:43:26 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:43:26 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:43:26 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:43:26 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:43:26 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:43:26 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:43:26 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:43:26 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:43:26 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:43:26 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:43:26 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:43:26 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:43:26 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:43:26 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:43:26 - INFO - trainer -     slot_recall = 0.9854789165037698
                                                                                                                       10/08/2021 14:44:13 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 727/918 [03:14<00:44,  4.25it/s]
10/08/2021 14:44:13 - INFO - trainer -     Num examples = 3258
10/08/2021 14:44:13 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 14:44:20 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:44:20 - INFO - trainer -     T-F1 = 0.990848244775304| 51/51 [00:07<00:00,  7.17it/s]
10/08/2021 14:44:20 - INFO - trainer -     T-F1(C) = 0.9716713881019831
10/08/2021 14:44:20 - INFO - trainer -     T-F1(L) = 0.984984984984985
10/08/2021 14:44:20 - INFO - trainer -     T-F1(O) = 0.9963695475480899
10/08/2021 14:44:20 - INFO - trainer -     T-F1(P) = 0.996780801872988
10/08/2021 14:44:20 - INFO - trainer -     T-F1(S) = 0.9920966992096699
10/08/2021 14:44:20 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 14:44:20 - INFO - trainer -     U-F1(A) = 0.7636363636363637
10/08/2021 14:44:20 - INFO - trainer -     U-F1(E) = 0.755877034358047
10/08/2021 14:44:20 - INFO - trainer -     U-F1(I) = 0.29787234042553196
10/08/2021 14:44:20 - INFO - trainer -     U-F1(O) = 0.9666432584269662
10/08/2021 14:44:20 - INFO - trainer -     intent_acc = 0.9370779619398404
10/08/2021 14:44:20 - INFO - trainer -     loss = 0.20948437382193172
10/08/2021 14:44:20 - INFO - trainer -     semantic_frame_acc = 0.9208103130755064
10/08/2021 14:44:20 - INFO - trainer -     slot_f1 = 0.9906385356993154
10/08/2021 14:44:20 - INFO - trainer -     slot_precision = 0.991331096196868
10/08/2021 14:44:20 - INFO - trainer -     slot_recall = 0.9899469421949176

10/08/2021 14:44:20 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:44:20 - INFO - trainer -     T-F1 = 0.9906721536351166
10/08/2021 14:44:20 - INFO - trainer -     T-F1(C) = 0.9781659388646288
10/08/2021 14:44:20 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/08/2021 14:44:20 - INFO - trainer -     T-F1(O) = 0.9963215406253381
10/08/2021 14:44:20 - INFO - trainer -     T-F1(P) = 0.9961910342806914
10/08/2021 14:44:20 - INFO - trainer -     T-F1(S) = 0.9920893438808748
10/08/2021 14:44:20 - INFO - trainer -     T-F1(T) = 0.9759547383309759
10/08/2021 14:44:20 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:44:20 - INFO - trainer -     U-F1(E) = 0.7775831873905429
10/08/2021 14:44:20 - INFO - trainer -     U-F1(I) = 0.0
10/08/2021 14:44:20 - INFO - trainer -     U-F1(O) = 0.9679003683564288
10/08/2021 14:44:20 - INFO - trainer -     intent_acc = 0.9407612031921424
10/08/2021 14:44:20 - INFO - trainer -     semantic_frame_acc = 0.9241866175567833
10/08/2021 14:44:20 - INFO - trainer -     slot_f1 = 0.9903185070857303
10/08/2021 14:44:20 - INFO - trainer -     slot_precision = 0.9952058657642414
10/08/2021 14:44:20 - INFO - trainer -     slot_recall = 0.9854789165037698
Iteration: 100%|| 918/918 [04:06<00:00,  3.72it/s]
Epoch:  25%|                                                      | 5/20 [20:50<1:02:32, 250.14s/it]10/08/2021 14:45:07 - INFO - trainer -   ***** Running evaluation on dev dataset *****  | 9/918 [00:02<03:33,  4.25it/s]
10/08/2021 14:45:07 - INFO - trainer -     Num examples = 3258
10/08/2021 14:45:07 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.03it/s]
10/08/2021 14:45:15 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:45:15 - INFO - trainer -     T-F1 = 0.9923329682365826| 51/51 [00:07<00:00,  7.16it/s]
10/08/2021 14:45:15 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:45:15 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:45:15 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:45:15 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:45:15 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:45:15 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:45:15 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:45:15 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:45:15 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:45:15 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:45:15 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:45:15 - INFO - trainer -     loss = 0.20151573441484394
10/08/2021 14:45:15 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:45:15 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:45:15 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:45:15 - INFO - trainer -     slot_recall = 0.9891091873778274

10/08/2021 14:45:15 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:45:15 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:45:15 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:45:15 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:45:15 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:45:15 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:45:15 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:45:15 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:45:15 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:45:15 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:45:15 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:45:15 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:45:15 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:45:15 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:45:15 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:45:15 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:45:15 - INFO - trainer -     slot_recall = 0.9891091873778274
10/08/2021 14:45:15 - INFO - transformers.configuration_utils -   Configuration saved in final_low_bert_dg_model\config.json
10/08/2021 14:45:16 - INFO - transformers.modeling_utils -   Model weights saved in final_low_bert_dg_model\pytorch_model.bin
10/08/2021 14:45:16 - INFO - trainer -   Saving model checkpoint to final_low_bert_dg_model
Best model saved
                                                                                                                       10/08/2021 14:46:03 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 209/918 [00:57<02:46,  4.25it/s]
10/08/2021 14:46:03 - INFO - trainer -     Num examples = 3258
10/08/2021 14:46:03 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.03it/s]
10/08/2021 14:46:10 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:46:10 - INFO - trainer -     T-F1 = 0.9881809536747724| 51/51 [00:07<00:00,  7.16it/s]
10/08/2021 14:46:10 - INFO - trainer -     T-F1(C) = 0.9715099715099714
10/08/2021 14:46:10 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 14:46:10 - INFO - trainer -     T-F1(O) = 0.9952755905511812
10/08/2021 14:46:10 - INFO - trainer -     T-F1(P) = 0.9976608187134504
10/08/2021 14:46:10 - INFO - trainer -     T-F1(S) = 0.9830818472793782
10/08/2021 14:46:10 - INFO - trainer -     T-F1(T) = 0.9748603351955307
10/08/2021 14:46:10 - INFO - trainer -     U-F1(A) = 0.7614678899082569
10/08/2021 14:46:10 - INFO - trainer -     U-F1(E) = 0.7686832740213523
10/08/2021 14:46:10 - INFO - trainer -     U-F1(I) = 0.21739130434782608
10/08/2021 14:46:10 - INFO - trainer -     U-F1(O) = 0.9655536028119508
10/08/2021 14:46:10 - INFO - trainer -     intent_acc = 0.93646408839779
10/08/2021 14:46:10 - INFO - trainer -     loss = 0.21326282328250362
10/08/2021 14:46:10 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/08/2021 14:46:10 - INFO - trainer -     slot_f1 = 0.9877709838799332
10/08/2021 14:46:10 - INFO - trainer -     slot_precision = 0.9831258644536652
10/08/2021 14:46:10 - INFO - trainer -     slot_recall = 0.9924602066461882

10/08/2021 14:46:10 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:46:10 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:46:10 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:46:10 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:46:10 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:46:10 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:46:10 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:46:10 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:46:10 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:46:10 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:46:10 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:46:10 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:46:10 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:46:10 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:46:10 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:46:10 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:46:10 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:46:57 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 409/918 [01:52<02:00,  4.24it/s]
10/08/2021 14:46:57 - INFO - trainer -     Num examples = 3258
10/08/2021 14:46:57 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.04it/s]
10/08/2021 14:47:05 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:47:05 - INFO - trainer -     T-F1 = 0.9901693063899508| 51/51 [00:07<00:00,  7.17it/s]
10/08/2021 14:47:05 - INFO - trainer -     T-F1(C) = 0.9713467048710601
10/08/2021 14:47:05 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 14:47:05 - INFO - trainer -     T-F1(O) = 0.9960979839583786
10/08/2021 14:47:05 - INFO - trainer -     T-F1(P) = 0.9976608187134504
10/08/2021 14:47:05 - INFO - trainer -     T-F1(S) = 0.990282276723739
10/08/2021 14:47:05 - INFO - trainer -     T-F1(T) = 0.9746478873239436
10/08/2021 14:47:05 - INFO - trainer -     U-F1(A) = 0.7317073170731707
10/08/2021 14:47:05 - INFO - trainer -     U-F1(E) = 0.7332053742802304
10/08/2021 14:47:05 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/08/2021 14:47:05 - INFO - trainer -     U-F1(O) = 0.9643416246303705
10/08/2021 14:47:05 - INFO - trainer -     intent_acc = 0.9337016574585635
10/08/2021 14:47:05 - INFO - trainer -     loss = 0.22629477090987504
10/08/2021 14:47:05 - INFO - trainer -     semantic_frame_acc = 0.9168201350521793
10/08/2021 14:47:05 - INFO - trainer -     slot_f1 = 0.9899441340782122
10/08/2021 14:47:05 - INFO - trainer -     slot_precision = 0.9902207320480582
10/08/2021 14:47:05 - INFO - trainer -     slot_recall = 0.9896676905892209

10/08/2021 14:47:05 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:47:05 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:47:05 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:47:05 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:47:05 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:47:05 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:47:05 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:47:05 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:47:05 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:47:05 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:47:05 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:47:05 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:47:05 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:47:05 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:47:05 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:47:05 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:47:05 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:47:52 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 609/918 [02:46<01:12,  4.26it/s]
10/08/2021 14:47:52 - INFO - trainer -     Num examples = 3258
10/08/2021 14:47:52 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.03it/s]
10/08/2021 14:48:00 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:48:00 - INFO - trainer -     T-F1 = 0.9907154560349536| 51/51 [00:07<00:00,  7.15it/s]
10/08/2021 14:48:00 - INFO - trainer -     T-F1(C) = 0.9782293178519593
10/08/2021 14:48:00 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 14:48:00 - INFO - trainer -     T-F1(O) = 0.9963147626273574
10/08/2021 14:48:00 - INFO - trainer -     T-F1(P) = 0.9964994165694282
10/08/2021 14:48:00 - INFO - trainer -     T-F1(S) = 0.990282276723739
10/08/2021 14:48:00 - INFO - trainer -     T-F1(T) = 0.9760900140646976
10/08/2021 14:48:00 - INFO - trainer -     U-F1(A) = 0.767123287671233
10/08/2021 14:48:00 - INFO - trainer -     U-F1(E) = 0.7382297551789078
10/08/2021 14:48:00 - INFO - trainer -     U-F1(I) = 0.2857142857142857
10/08/2021 14:48:00 - INFO - trainer -     U-F1(O) = 0.9642732049036777
10/08/2021 14:48:00 - INFO - trainer -     intent_acc = 0.9333947206875384
10/08/2021 14:48:00 - INFO - trainer -     loss = 0.23465249490211992
10/08/2021 14:48:00 - INFO - trainer -     semantic_frame_acc = 0.9177409453652547
10/08/2021 14:48:00 - INFO - trainer -     slot_f1 = 0.9902234636871509
10/08/2021 14:48:00 - INFO - trainer -     slot_precision = 0.9905001397038279
10/08/2021 14:48:00 - INFO - trainer -     slot_recall = 0.9899469421949176

10/08/2021 14:48:00 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:48:00 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:48:00 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:48:00 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:48:00 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:48:00 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:48:00 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:48:00 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:48:00 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:48:00 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:48:00 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:48:00 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:48:00 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:48:00 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:48:00 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:48:00 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:48:00 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:48:47 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 809/918 [03:41<00:25,  4.24it/s]
10/08/2021 14:48:47 - INFO - trainer -     Num examples = 3258
10/08/2021 14:48:47 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.03it/s]
10/08/2021 14:48:54 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:48:54 - INFO - trainer -     T-F1 = 0.9899374490073429| 51/51 [00:07<00:00,  7.16it/s]
10/08/2021 14:48:54 - INFO - trainer -     T-F1(C) = 0.9715099715099714
10/08/2021 14:48:54 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/08/2021 14:48:54 - INFO - trainer -     T-F1(O) = 0.995983063728151
10/08/2021 14:48:54 - INFO - trainer -     T-F1(P) = 0.9976608187134504
10/08/2021 14:48:54 - INFO - trainer -     T-F1(S) = 0.9867033470884916
10/08/2021 14:48:54 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 14:48:54 - INFO - trainer -     U-F1(A) = 0.7685185185185185
10/08/2021 14:48:54 - INFO - trainer -     U-F1(E) = 0.7472527472527474
10/08/2021 14:48:54 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/08/2021 14:48:54 - INFO - trainer -     U-F1(O) = 0.9655293088363955
10/08/2021 14:48:54 - INFO - trainer -     intent_acc = 0.93646408839779
10/08/2021 14:48:54 - INFO - trainer -     loss = 0.2344320358014574
10/08/2021 14:48:54 - INFO - trainer -     semantic_frame_acc = 0.9195825659914058
10/08/2021 14:48:54 - INFO - trainer -     slot_f1 = 0.9897079276773296
10/08/2021 14:48:54 - INFO - trainer -     slot_precision = 0.9858686616791354
10/08/2021 14:48:54 - INFO - trainer -     slot_recall = 0.9935772130689752

10/08/2021 14:48:54 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:48:54 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:48:54 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:48:54 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:48:54 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:48:54 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:48:54 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:48:54 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:48:54 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:48:54 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:48:54 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:48:54 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:48:54 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:48:54 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:48:54 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:48:54 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:48:54 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:14<00:00,  3.60it/s]
Epoch:  30%|                                                    | 6/20 [25:04<58:44, 251.76s/it]10/08/2021 14:49:41 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 91/918 [00:21<03:15,  4.23it/s]
10/08/2021 14:49:41 - INFO - trainer -     Num examples = 3258
10/08/2021 14:49:41 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:13<00:00,  3.91it/s]
10/08/2021 14:49:55 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:49:55 - INFO - trainer -     T-F1 = 0.9904839586731919| 51/51 [00:13<00:00,  1.19it/s]
10/08/2021 14:49:55 - INFO - trainer -     T-F1(C) = 0.9756795422031473
10/08/2021 14:49:55 - INFO - trainer -     T-F1(L) = 0.9881656804733727
10/08/2021 14:49:55 - INFO - trainer -     T-F1(O) = 0.996199782844734
10/08/2021 14:49:55 - INFO - trainer -     T-F1(P) = 0.9964973730297724
10/08/2021 14:49:55 - INFO - trainer -     T-F1(S) = 0.9908003679852806
10/08/2021 14:49:55 - INFO - trainer -     T-F1(T) = 0.9763560500695412
10/08/2021 14:49:55 - INFO - trainer -     U-F1(A) = 0.7488151658767772
10/08/2021 14:49:55 - INFO - trainer -     U-F1(E) = 0.7490909090909091
10/08/2021 14:49:55 - INFO - trainer -     U-F1(I) = 0.25
10/08/2021 14:49:55 - INFO - trainer -     U-F1(O) = 0.9640792009812511
10/08/2021 14:49:55 - INFO - trainer -     intent_acc = 0.9337016574585635
10/08/2021 14:49:55 - INFO - trainer -     loss = 0.2621654469300719
10/08/2021 14:49:55 - INFO - trainer -     semantic_frame_acc = 0.91804788213628
10/08/2021 14:49:55 - INFO - trainer -     slot_f1 = 0.9902669632925473
10/08/2021 14:49:55 - INFO - trainer -     slot_precision = 0.986153420105234
10/08/2021 14:49:55 - INFO - trainer -     slot_recall = 0.9944149678860653

10/08/2021 14:49:55 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:49:55 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:49:55 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:49:55 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:49:55 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:49:55 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:49:55 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:49:55 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:49:55 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:49:55 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:49:55 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:49:55 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:49:55 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:49:55 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:49:55 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:49:55 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:49:55 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:52:05 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 291/918 [02:45<05:38,  1.85it/s]
10/08/2021 14:52:05 - INFO - trainer -     Num examples = 3258
10/08/2021 14:52:05 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:19<00:00,  2.68it/s]
10/08/2021 14:52:25 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:52:25 - INFO - trainer -     T-F1 = 0.9903071672354948| 51/51 [00:19<00:00,  2.77it/s]
10/08/2021 14:52:25 - INFO - trainer -     T-F1(C) = 0.9728958630527819
10/08/2021 14:52:25 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 14:52:25 - INFO - trainer -     T-F1(O) = 0.9961519700829223
10/08/2021 14:52:25 - INFO - trainer -     T-F1(P) = 0.9958992384299942
10/08/2021 14:52:25 - INFO - trainer -     T-F1(S) = 0.9925925925925926
10/08/2021 14:52:25 - INFO - trainer -     T-F1(T) = 0.9747899159663864
10/08/2021 14:52:25 - INFO - trainer -     U-F1(A) = 0.7777777777777778
10/08/2021 14:52:25 - INFO - trainer -     U-F1(E) = 0.7747747747747746
10/08/2021 14:52:25 - INFO - trainer -     U-F1(I) = 0.32653061224489793
10/08/2021 14:52:25 - INFO - trainer -     U-F1(O) = 0.9676966292134832
10/08/2021 14:52:25 - INFO - trainer -     intent_acc = 0.9401473296500921
10/08/2021 14:52:25 - INFO - trainer -     loss = 0.24870940915071496
10/08/2021 14:52:25 - INFO - trainer -     semantic_frame_acc = 0.9232658072437078
10/08/2021 14:52:25 - INFO - trainer -     slot_f1 = 0.9900851836335707
10/08/2021 14:52:25 - INFO - trainer -     slot_precision = 0.9902234636871509
10/08/2021 14:52:25 - INFO - trainer -     slot_recall = 0.9899469421949176

10/08/2021 14:52:25 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:52:25 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:52:25 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:52:25 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:52:25 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:52:25 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:52:25 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:52:25 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:52:25 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:52:25 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:52:25 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:52:25 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:52:25 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:52:25 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:52:25 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:52:25 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:52:25 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:54:12 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 491/918 [04:51<03:51,  1.84it/s]
10/08/2021 14:54:12 - INFO - trainer -     Num examples = 3258
10/08/2021 14:54:12 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.69it/s]
10/08/2021 14:54:31 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:54:31 - INFO - trainer -     T-F1 = 0.9917965545529122| 51/51 [00:18<00:00,  2.78it/s]
10/08/2021 14:54:31 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:54:31 - INFO - trainer -     T-F1(L) = 0.9881656804733727
10/08/2021 14:54:31 - INFO - trainer -     T-F1(O) = 0.9967500812479689
10/08/2021 14:54:31 - INFO - trainer -     T-F1(P) = 0.9959040374488005
10/08/2021 14:54:31 - INFO - trainer -     T-F1(S) = 0.9930458970792767
10/08/2021 14:54:31 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 14:54:31 - INFO - trainer -     U-F1(A) = 0.7720930232558139
10/08/2021 14:54:31 - INFO - trainer -     U-F1(E) = 0.7423423423423423
10/08/2021 14:54:31 - INFO - trainer -     U-F1(I) = 0.31999999999999995
10/08/2021 14:54:31 - INFO - trainer -     U-F1(O) = 0.9645365168539325
10/08/2021 14:54:31 - INFO - trainer -     intent_acc = 0.9343155310006138
10/08/2021 14:54:31 - INFO - trainer -     loss = 0.24310462074536904
10/08/2021 14:54:31 - INFO - trainer -     semantic_frame_acc = 0.9214241866175568
10/08/2021 14:54:31 - INFO - trainer -     slot_f1 = 0.9916083916083916
10/08/2021 14:54:31 - INFO - trainer -     slot_precision = 0.9932754272905576
10/08/2021 14:54:31 - INFO - trainer -     slot_recall = 0.9899469421949176

10/08/2021 14:54:31 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:54:31 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:54:31 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:54:31 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:54:31 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:54:31 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:54:31 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:54:31 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:54:31 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:54:31 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:54:31 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:54:31 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:54:31 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:54:31 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:54:31 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:54:31 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:54:31 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:56:17 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 691/918 [06:57<01:57,  1.93it/s]
10/08/2021 14:56:17 - INFO - trainer -     Num examples = 3258
10/08/2021 14:56:17 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.77it/s]
10/08/2021 14:56:36 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:56:36 - INFO - trainer -     T-F1 = 0.9915207877461707| 51/51 [00:18<00:00,  2.83it/s]
10/08/2021 14:56:36 - INFO - trainer -     T-F1(C) = 0.9840348330914369
10/08/2021 14:56:36 - INFO - trainer -     T-F1(L) = 0.984984984984985
10/08/2021 14:56:36 - INFO - trainer -     T-F1(O) = 0.9966421143847487
10/08/2021 14:56:36 - INFO - trainer -     T-F1(P) = 0.9976621858562245
10/08/2021 14:56:36 - INFO - trainer -     T-F1(S) = 0.9888888888888887
10/08/2021 14:56:36 - INFO - trainer -     T-F1(T) = 0.980225988700565
10/08/2021 14:56:36 - INFO - trainer -     U-F1(A) = 0.7741935483870968
10/08/2021 14:56:36 - INFO - trainer -     U-F1(E) = 0.7481481481481482
10/08/2021 14:56:36 - INFO - trainer -     U-F1(I) = 0.058823529411764705
10/08/2021 14:56:36 - INFO - trainer -     U-F1(O) = 0.9662882096069869
10/08/2021 14:56:36 - INFO - trainer -     intent_acc = 0.9370779619398404
10/08/2021 14:56:36 - INFO - trainer -     loss = 0.25411559977367815
10/08/2021 14:56:36 - INFO - trainer -     semantic_frame_acc = 0.9226519337016574
10/08/2021 14:56:36 - INFO - trainer -     slot_f1 = 0.9913262451035254
10/08/2021 14:56:36 - INFO - trainer -     slot_precision = 0.9932716568544996
10/08/2021 14:56:36 - INFO - trainer -     slot_recall = 0.9893884389835241

10/08/2021 14:56:36 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:56:36 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:56:36 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:56:36 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:56:36 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:56:36 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:56:36 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:56:36 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:56:36 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:56:36 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:56:36 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:56:36 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:56:36 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:56:36 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:56:36 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:56:36 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:56:36 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 14:58:22 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 891/918 [09:01<00:14,  1.88it/s]
10/08/2021 14:58:22 - INFO - trainer -     Num examples = 3258
10/08/2021 14:58:22 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.70it/s]
10/08/2021 14:58:41 - INFO - trainer -   ***** Eval results *****
10/08/2021 14:58:41 - INFO - trainer -     T-F1 = 0.9911383776414451| 51/51 [00:18<00:00,  2.77it/s]
10/08/2021 14:58:41 - INFO - trainer -     T-F1(C) = 0.9728958630527819
10/08/2021 14:58:41 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 14:58:41 - INFO - trainer -     T-F1(O) = 0.9964752453771487
10/08/2021 14:58:41 - INFO - trainer -     T-F1(P) = 0.9970794392523364
10/08/2021 14:58:41 - INFO - trainer -     T-F1(S) = 0.9907493061979649
10/08/2021 14:58:41 - INFO - trainer -     T-F1(T) = 0.9845722300140253
10/08/2021 14:58:41 - INFO - trainer -     U-F1(A) = 0.7641509433962266
10/08/2021 14:58:41 - INFO - trainer -     U-F1(E) = 0.7009900990099011
10/08/2021 14:58:41 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/08/2021 14:58:41 - INFO - trainer -     U-F1(O) = 0.9638888888888889
10/08/2021 14:58:41 - INFO - trainer -     intent_acc = 0.932780847145488
10/08/2021 14:58:41 - INFO - trainer -     loss = 0.2680043959442307
10/08/2021 14:58:41 - INFO - trainer -     semantic_frame_acc = 0.9168201350521793
10/08/2021 14:58:41 - INFO - trainer -     slot_f1 = 0.9909357132896388
10/08/2021 14:58:41 - INFO - trainer -     slot_precision = 0.9896935933147633
10/08/2021 14:58:41 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 14:58:41 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 14:58:41 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 14:58:41 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 14:58:41 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 14:58:41 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 14:58:41 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 14:58:41 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 14:58:41 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 14:58:41 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 14:58:41 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 14:58:41 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 14:58:41 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 14:58:41 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 14:58:41 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 14:58:41 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 14:58:41 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 14:58:41 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [09:35<00:00,  1.60it/s]
Epoch:  35%|                                               | 7/20 [34:40<1:17:26, 357.46s/it]10/08/2021 15:00:25 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 173/918 [01:29<06:34,  1.89it/s]
10/08/2021 15:00:25 - INFO - trainer -     Num examples = 3258
10/08/2021 15:00:25 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:21<00:00,  2.37it/s]
10/08/2021 15:00:47 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:00:47 - INFO - trainer -     T-F1 = 0.9900585591720006| 51/51 [00:21<00:00,  2.64it/s]
10/08/2021 15:00:47 - INFO - trainer -     T-F1(C) = 0.9714285714285715
10/08/2021 15:00:47 - INFO - trainer -     T-F1(L) = 0.9821428571428571
10/08/2021 15:00:47 - INFO - trainer -     T-F1(O) = 0.9960397113871859
10/08/2021 15:00:47 - INFO - trainer -     T-F1(P) = 0.9970777323202805
10/08/2021 15:00:47 - INFO - trainer -     T-F1(S) = 0.9884951679705476
10/08/2021 15:00:47 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:00:47 - INFO - trainer -     U-F1(A) = 0.7536231884057972
10/08/2021 15:00:47 - INFO - trainer -     U-F1(E) = 0.773286467486819
10/08/2021 15:00:47 - INFO - trainer -     U-F1(I) = 0.3404255319148936
10/08/2021 15:00:47 - INFO - trainer -     U-F1(O) = 0.9668013349727735
10/08/2021 15:00:47 - INFO - trainer -     intent_acc = 0.9386126457949663
10/08/2021 15:00:47 - INFO - trainer -     loss = 0.28868309291554434
10/08/2021 15:00:47 - INFO - trainer -     semantic_frame_acc = 0.9208103130755064
10/08/2021 15:00:47 - INFO - trainer -     slot_f1 = 0.9898314528485862
10/08/2021 15:00:47 - INFO - trainer -     slot_precision = 0.9874930516953864
10/08/2021 15:00:47 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 15:00:47 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:00:47 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:00:47 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:00:47 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:00:47 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:00:47 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:00:47 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:00:47 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:00:47 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:00:47 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:00:47 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:00:47 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:00:47 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:00:47 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:00:47 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:00:47 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:00:47 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:02:31 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 373/918 [03:35<04:47,  1.90it/s]
10/08/2021 15:02:31 - INFO - trainer -     Num examples = 3258
10/08/2021 15:02:31 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:20<00:00,  2.48it/s]
10/08/2021 15:02:52 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:02:52 - INFO - trainer -     T-F1 = 0.9909934497816593| 51/51 [00:20<00:00,  2.51it/s]
10/08/2021 15:02:52 - INFO - trainer -     T-F1(C) = 0.9742857142857144
10/08/2021 15:02:52 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 15:02:52 - INFO - trainer -     T-F1(O) = 0.9964223764093669
10/08/2021 15:02:52 - INFO - trainer -     T-F1(P) = 0.9967845659163987
10/08/2021 15:02:52 - INFO - trainer -     T-F1(S) = 0.9902732746641963
10/08/2021 15:02:52 - INFO - trainer -     T-F1(T) = 0.9845722300140253
10/08/2021 15:02:52 - INFO - trainer -     U-F1(A) = 0.7663551401869159
10/08/2021 15:02:52 - INFO - trainer -     U-F1(E) = 0.7422303473491773
10/08/2021 15:02:52 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/08/2021 15:02:52 - INFO - trainer -     U-F1(O) = 0.9653603918824353
10/08/2021 15:02:52 - INFO - trainer -     intent_acc = 0.9355432780847146
10/08/2021 15:02:52 - INFO - trainer -     loss = 0.2873057489301644
10/08/2021 15:02:52 - INFO - trainer -     semantic_frame_acc = 0.9192756292203806
10/08/2021 15:02:52 - INFO - trainer -     slot_f1 = 0.990787269681742
10/08/2021 15:02:52 - INFO - trainer -     slot_precision = 0.9905107451855987
10/08/2021 15:02:52 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 15:02:52 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:02:52 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:02:52 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:02:52 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:02:52 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:02:52 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:02:52 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:02:52 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:02:52 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:02:52 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:02:52 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:02:52 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:02:52 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:02:52 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:02:52 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:02:52 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:02:52 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:04:37 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 573/918 [05:41<03:00,  1.91it/s]
10/08/2021 15:04:37 - INFO - trainer -     Num examples = 3258
10/08/2021 15:04:37 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.76it/s]
10/08/2021 15:04:56 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:04:56 - INFO - trainer -     T-F1 = 0.9891097195752792| 51/51 [00:18<00:00,  2.85it/s]
10/08/2021 15:04:56 - INFO - trainer -     T-F1(C) = 0.9756097560975611
10/08/2021 15:04:56 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:04:56 - INFO - trainer -     T-F1(O) = 0.9956592512208357
10/08/2021 15:04:56 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/08/2021 15:04:56 - INFO - trainer -     T-F1(S) = 0.9880294659300185
10/08/2021 15:04:56 - INFO - trainer -     T-F1(T) = 0.975
10/08/2021 15:04:56 - INFO - trainer -     U-F1(A) = 0.7562189054726368
10/08/2021 15:04:56 - INFO - trainer -     U-F1(E) = 0.7406015037593986
10/08/2021 15:04:56 - INFO - trainer -     U-F1(I) = 0.21739130434782608
10/08/2021 15:04:56 - INFO - trainer -     U-F1(O) = 0.9646156527801988
10/08/2021 15:04:56 - INFO - trainer -     intent_acc = 0.934622467771639
10/08/2021 15:04:56 - INFO - trainer -     loss = 0.2934332858113682
10/08/2021 15:04:56 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/08/2021 15:04:56 - INFO - trainer -     slot_f1 = 0.9888610414926204
10/08/2021 15:04:56 - INFO - trainer -     slot_precision = 0.9861149680644266
10/08/2021 15:04:56 - INFO - trainer -     slot_recall = 0.991622451829098

10/08/2021 15:04:56 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:04:56 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:04:56 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:04:56 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:04:56 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:04:56 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:04:56 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:04:56 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:04:56 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:04:56 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:04:56 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:04:56 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:04:56 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:04:56 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:04:56 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:04:56 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:04:56 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:06:40 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 773/918 [07:44<01:15,  1.92it/s]
10/08/2021 15:06:40 - INFO - trainer -     Num examples = 3258
10/08/2021 15:06:40 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.76it/s]
10/08/2021 15:06:59 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:06:59 - INFO - trainer -     T-F1 = 0.9901558654634948| 51/51 [00:18<00:00,  2.83it/s]
10/08/2021 15:06:59 - INFO - trainer -     T-F1(C) = 0.976878612716763
10/08/2021 15:06:59 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/08/2021 15:06:59 - INFO - trainer -     T-F1(O) = 0.9961000974975626
10/08/2021 15:06:59 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/08/2021 15:06:59 - INFO - trainer -     T-F1(S) = 0.9889094269870611
10/08/2021 15:06:59 - INFO - trainer -     T-F1(T) = 0.9774011299435028
10/08/2021 15:06:59 - INFO - trainer -     U-F1(A) = 0.7065217391304347
10/08/2021 15:06:59 - INFO - trainer -     U-F1(E) = 0.7304015296367112
10/08/2021 15:06:59 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/08/2021 15:06:59 - INFO - trainer -     U-F1(O) = 0.9639514731369151
10/08/2021 15:06:59 - INFO - trainer -     intent_acc = 0.9337016574585635
10/08/2021 15:06:59 - INFO - trainer -     loss = 0.2820660045333937
10/08/2021 15:06:59 - INFO - trainer -     semantic_frame_acc = 0.9165131982811541
10/08/2021 15:06:59 - INFO - trainer -     slot_f1 = 0.9899300699300699
10/08/2021 15:06:59 - INFO - trainer -     slot_precision = 0.9915942841131969
10/08/2021 15:06:59 - INFO - trainer -     slot_recall = 0.9882714325607372

10/08/2021 15:06:59 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:06:59 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:06:59 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:06:59 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:06:59 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:06:59 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:06:59 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:06:59 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:06:59 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:06:59 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:06:59 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:06:59 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:06:59 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:06:59 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:06:59 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:06:59 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:06:59 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [09:18<00:00,  1.64it/s]
Epoch:  40%|                                           | 8/20 [43:58<1:24:18, 421.52s/it]10/08/2021 15:08:43 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 55/918 [00:28<07:26,  1.93it/s]
10/08/2021 15:08:43 - INFO - trainer -     Num examples = 3258
10/08/2021 15:08:43 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.78it/s]
10/08/2021 15:09:01 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:09:01 - INFO - trainer -     T-F1 = 0.9904684095860566| 51/51 [00:18<00:00,  2.87it/s]
10/08/2021 15:09:01 - INFO - trainer -     T-F1(C) = 0.9716713881019831
10/08/2021 15:09:01 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 15:09:01 - INFO - trainer -     T-F1(O) = 0.9962022569444445
10/08/2021 15:09:01 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/08/2021 15:09:01 - INFO - trainer -     T-F1(S) = 0.9898523985239852
10/08/2021 15:09:01 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 15:09:01 - INFO - trainer -     U-F1(A) = 0.7549019607843138
10/08/2021 15:09:01 - INFO - trainer -     U-F1(E) = 0.7532467532467532
10/08/2021 15:09:01 - INFO - trainer -     U-F1(I) = 0.15789473684210525
10/08/2021 15:09:01 - INFO - trainer -     U-F1(O) = 0.9653007846556234
10/08/2021 15:09:01 - INFO - trainer -     intent_acc = 0.93646408839779
10/08/2021 15:09:01 - INFO - trainer -     loss = 0.31891264824890625
10/08/2021 15:09:01 - INFO - trainer -     semantic_frame_acc = 0.9177409453652547
10/08/2021 15:09:01 - INFO - trainer -     slot_f1 = 0.9901100431815016
10/08/2021 15:09:01 - INFO - trainer -     slot_precision = 0.9877709838799333
10/08/2021 15:09:01 - INFO - trainer -     slot_recall = 0.9924602066461882

10/08/2021 15:09:01 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:09:01 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:09:01 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:09:01 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:09:01 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:09:01 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:09:01 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:09:01 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:09:01 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:09:01 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:09:01 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:09:01 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:09:01 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:09:01 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:09:01 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:09:01 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:09:01 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:10:43 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 255/918 [02:29<05:42,  1.94it/s]
10/08/2021 15:10:43 - INFO - trainer -     Num examples = 3258
10/08/2021 15:10:43 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:19<00:00,  2.58it/s]
10/08/2021 15:11:03 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:11:03 - INFO - trainer -     T-F1 = 0.988184164063561| 51/51 [00:19<00:00,  2.85it/s]
10/08/2021 15:11:03 - INFO - trainer -     T-F1(C) = 0.9713467048710601
10/08/2021 15:11:03 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:11:03 - INFO - trainer -     T-F1(O) = 0.9952750773909739
10/08/2021 15:11:03 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/08/2021 15:11:03 - INFO - trainer -     T-F1(S) = 0.983097304705345
10/08/2021 15:11:03 - INFO - trainer -     T-F1(T) = 0.9818688981868898
10/08/2021 15:11:03 - INFO - trainer -     U-F1(A) = 0.7058823529411764
10/08/2021 15:11:03 - INFO - trainer -     U-F1(E) = 0.6942800788954635
10/08/2021 15:11:03 - INFO - trainer -     U-F1(I) = 0.25
10/08/2021 15:11:03 - INFO - trainer -     U-F1(O) = 0.9609131788308545
10/08/2021 15:11:03 - INFO - trainer -     intent_acc = 0.9284837323511357
10/08/2021 15:11:03 - INFO - trainer -     loss = 0.40168830869244593
10/08/2021 15:11:03 - INFO - trainer -     semantic_frame_acc = 0.9076120319214241
10/08/2021 15:11:03 - INFO - trainer -     slot_f1 = 0.9879149881928045
10/08/2021 15:11:03 - INFO - trainer -     slot_precision = 0.9828634604754007
10/08/2021 15:11:03 - INFO - trainer -     slot_recall = 0.9930187098575817

10/08/2021 15:11:03 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:11:04 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:11:04 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:11:04 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:11:04 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:11:04 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:11:04 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:11:04 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:11:04 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:11:04 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:11:04 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:11:04 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:11:04 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:11:04 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:11:04 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:11:04 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:11:04 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:12:48 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 455/918 [04:34<04:04,  1.90it/s]
10/08/2021 15:12:48 - INFO - trainer -     Num examples = 3258
10/08/2021 15:12:48 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.72it/s]
10/08/2021 15:13:07 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:13:07 - INFO - trainer -     T-F1 = 0.9899264906071332| 51/51 [00:18<00:00,  2.78it/s]
10/08/2021 15:13:07 - INFO - trainer -     T-F1(C) = 0.972818311874106
10/08/2021 15:13:07 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/08/2021 15:13:07 - INFO - trainer -     T-F1(O) = 0.995984807379273
10/08/2021 15:13:07 - INFO - trainer -     T-F1(P) = 0.9979562043795619
10/08/2021 15:13:07 - INFO - trainer -     T-F1(S) = 0.9870967741935485
10/08/2021 15:13:07 - INFO - trainer -     T-F1(T) = 0.9790794979079497
10/08/2021 15:13:07 - INFO - trainer -     U-F1(A) = 0.6947368421052631
10/08/2021 15:13:07 - INFO - trainer -     U-F1(E) = 0.7203065134099617
10/08/2021 15:13:07 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/08/2021 15:13:07 - INFO - trainer -     U-F1(O) = 0.9621659146129816
10/08/2021 15:13:07 - INFO - trainer -     intent_acc = 0.9303253529772867
10/08/2021 15:13:07 - INFO - trainer -     loss = 0.35746682756671716
10/08/2021 15:13:07 - INFO - trainer -     semantic_frame_acc = 0.9106813996316758
10/08/2021 15:13:07 - INFO - trainer -     slot_f1 = 0.9896964633806739
10/08/2021 15:13:07 - INFO - trainer -     slot_precision = 0.986948069980561
10/08/2021 15:13:07 - INFO - trainer -     slot_recall = 0.9924602066461882

10/08/2021 15:13:07 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:13:07 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:13:07 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:13:07 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:13:07 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:13:07 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:13:07 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:13:07 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:13:07 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:13:07 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:13:07 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:13:07 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:13:07 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:13:07 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:13:07 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:13:07 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:13:07 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:14:53 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 655/918 [06:38<02:17,  1.91it/s]
10/08/2021 15:14:53 - INFO - trainer -     Num examples = 3258
10/08/2021 15:14:53 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.76it/s]
10/08/2021 15:15:12 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:15:12 - INFO - trainer -     T-F1 = 0.9916564081520995| 51/51 [00:18<00:00,  2.83it/s]
10/08/2021 15:15:12 - INFO - trainer -     T-F1(C) = 0.9798850574712643
10/08/2021 15:15:12 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:15:12 - INFO - trainer -     T-F1(O) = 0.996696452748443
10/08/2021 15:15:12 - INFO - trainer -     T-F1(P) = 0.996780801872988
10/08/2021 15:15:12 - INFO - trainer -     T-F1(S) = 0.9907063197026023
10/08/2021 15:15:12 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:15:12 - INFO - trainer -     U-F1(A) = 0.7582938388625593
10/08/2021 15:15:12 - INFO - trainer -     U-F1(E) = 0.7310606060606061
10/08/2021 15:15:12 - INFO - trainer -     U-F1(I) = 0.29787234042553196
10/08/2021 15:15:12 - INFO - trainer -     U-F1(O) = 0.9640488656195462
10/08/2021 15:15:12 - INFO - trainer -     intent_acc = 0.9337016574585635
10/08/2021 15:15:12 - INFO - trainer -     loss = 0.34971024987159993
10/08/2021 15:15:12 - INFO - trainer -     semantic_frame_acc = 0.9177409453652547
10/08/2021 15:15:12 - INFO - trainer -     slot_f1 = 0.9914649503288092
10/08/2021 15:15:12 - INFO - trainer -     slot_precision = 0.9935501962983735
10/08/2021 15:15:12 - INFO - trainer -     slot_recall = 0.9893884389835241

10/08/2021 15:15:12 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:15:12 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:15:12 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:15:12 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:15:12 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:15:12 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:15:12 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:15:12 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:15:12 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:15:12 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:15:12 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:15:12 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:15:12 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:15:12 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:15:12 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:15:12 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:15:12 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:16:55 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 855/918 [08:41<00:32,  1.92it/s]
10/08/2021 15:16:55 - INFO - trainer -     Num examples = 3258
10/08/2021 15:16:55 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.79it/s]
10/08/2021 15:17:14 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:17:14 - INFO - trainer -     T-F1 = 0.9912758996728461| 51/51 [00:18<00:00,  2.87it/s]
10/08/2021 15:17:14 - INFO - trainer -     T-F1(C) = 0.9798850574712643
10/08/2021 15:17:14 - INFO - trainer -     T-F1(L) = 0.984984984984985
10/08/2021 15:17:14 - INFO - trainer -     T-F1(O) = 0.996529284164859
10/08/2021 15:17:14 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/08/2021 15:17:14 - INFO - trainer -     T-F1(S) = 0.9907918968692448
10/08/2021 15:17:14 - INFO - trainer -     T-F1(T) = 0.9818181818181818
10/08/2021 15:17:14 - INFO - trainer -     U-F1(A) = 0.7389162561576355
10/08/2021 15:17:14 - INFO - trainer -     U-F1(E) = 0.697211155378486
10/08/2021 15:17:14 - INFO - trainer -     U-F1(I) = 0.25531914893617025
10/08/2021 15:17:14 - INFO - trainer -     U-F1(O) = 0.9628730048577376
10/08/2021 15:17:14 - INFO - trainer -     intent_acc = 0.9303253529772867
10/08/2021 15:17:14 - INFO - trainer -     loss = 0.35101049524896283
10/08/2021 15:17:14 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/08/2021 15:17:14 - INFO - trainer -     slot_f1 = 0.9910764082543223
10/08/2021 15:17:14 - INFO - trainer -     slot_precision = 0.9896964633806739
10/08/2021 15:17:14 - INFO - trainer -     slot_recall = 0.9924602066461882

10/08/2021 15:17:14 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:17:14 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:17:14 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:17:14 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:17:14 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:17:14 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:17:14 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:17:14 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:17:14 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:17:14 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:17:14 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:17:14 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:17:14 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:17:14 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:17:14 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:17:14 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:17:14 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [09:31<00:00,  1.61it/s]
Epoch:  45%|                                        | 9/20 [53:30<1:25:53, 468.53s/it]10/08/2021 15:18:56 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 137/918 [01:10<06:43,  1.94it/s]
10/08/2021 15:18:56 - INFO - trainer -     Num examples = 3258
10/08/2021 15:18:56 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.81it/s]
10/08/2021 15:19:15 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:19:15 - INFO - trainer -     T-F1 = 0.9897945298680093| 51/51 [00:18<00:00,  2.90it/s]
10/08/2021 15:19:15 - INFO - trainer -     T-F1(C) = 0.9744318181818182
10/08/2021 15:19:15 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:19:15 - INFO - trainer -     T-F1(O) = 0.9959298854941119
10/08/2021 15:19:15 - INFO - trainer -     T-F1(P) = 0.9964953271028038
10/08/2021 15:19:15 - INFO - trainer -     T-F1(S) = 0.9889604415823368
10/08/2021 15:19:15 - INFO - trainer -     T-F1(T) = 0.9761570827489481
10/08/2021 15:19:15 - INFO - trainer -     U-F1(A) = 0.7751196172248804
10/08/2021 15:19:15 - INFO - trainer -     U-F1(E) = 0.744360902255639
10/08/2021 15:19:15 - INFO - trainer -     U-F1(I) = 0.2857142857142857
10/08/2021 15:19:15 - INFO - trainer -     U-F1(O) = 0.9650716032134125
10/08/2021 15:19:15 - INFO - trainer -     intent_acc = 0.9358502148557397
10/08/2021 15:19:15 - INFO - trainer -     loss = 0.37589918383780646
10/08/2021 15:19:15 - INFO - trainer -     semantic_frame_acc = 0.9174340085942296
10/08/2021 15:19:15 - INFO - trainer -     slot_f1 = 0.9895615866388309
10/08/2021 15:19:15 - INFO - trainer -     slot_precision = 0.9864039955604883
10/08/2021 15:19:15 - INFO - trainer -     slot_recall = 0.9927394582518849

10/08/2021 15:19:15 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:19:15 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:19:15 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:19:15 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:19:15 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:19:15 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:19:15 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:19:15 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:19:15 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:19:15 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:19:15 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:19:15 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:19:15 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:19:15 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:19:15 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:19:15 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:19:15 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:20:57 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 337/918 [03:10<04:54,  1.97it/s]
10/08/2021 15:20:57 - INFO - trainer -     Num examples = 3258
10/08/2021 15:20:57 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:19<00:00,  2.61it/s]
10/08/2021 15:21:17 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:21:17 - INFO - trainer -     T-F1 = 0.9913922667031015| 51/51 [00:19<00:00,  2.76it/s]
10/08/2021 15:21:17 - INFO - trainer -     T-F1(C) = 0.9797687861271676
10/08/2021 15:21:17 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:21:17 - INFO - trainer -     T-F1(O) = 0.9965866608874682
10/08/2021 15:21:17 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/08/2021 15:21:17 - INFO - trainer -     T-F1(S) = 0.9911996294580825
10/08/2021 15:21:17 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:21:17 - INFO - trainer -     U-F1(A) = 0.6557377049180327
10/08/2021 15:21:17 - INFO - trainer -     U-F1(E) = 0.7476979742173112
10/08/2021 15:21:17 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 15:21:17 - INFO - trainer -     U-F1(O) = 0.9611971463372194
10/08/2021 15:21:17 - INFO - trainer -     intent_acc = 0.9300184162062615
10/08/2021 15:21:17 - INFO - trainer -     loss = 0.3640381146003218
10/08/2021 15:21:17 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/08/2021 15:21:17 - INFO - trainer -     slot_f1 = 0.9911949685534592
10/08/2021 15:21:17 - INFO - trainer -     slot_precision = 0.9921656407386682
10/08/2021 15:21:17 - INFO - trainer -     slot_recall = 0.9902261938006144

10/08/2021 15:21:17 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:21:17 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:21:17 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:21:17 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:21:17 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:21:17 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:21:17 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:21:17 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:21:17 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:21:17 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:21:17 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:21:17 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:21:17 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:21:17 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:21:17 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:21:17 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:21:17 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:22:57 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 537/918 [05:10<03:12,  1.98it/s]
10/08/2021 15:22:57 - INFO - trainer -     Num examples = 3258
10/08/2021 15:22:57 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:18<00:00,  2.68it/s]
10/08/2021 15:23:16 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:23:16 - INFO - trainer -     T-F1 = 0.9918055176181371| 51/51 [00:18<00:00,  2.94it/s]
10/08/2021 15:23:16 - INFO - trainer -     T-F1(C) = 0.9782923299565845
10/08/2021 15:23:16 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 15:23:16 - INFO - trainer -     T-F1(O) = 0.9967486723745529
10/08/2021 15:23:16 - INFO - trainer -     T-F1(P) = 0.9970794392523364
10/08/2021 15:23:16 - INFO - trainer -     T-F1(S) = 0.9912077741786209
10/08/2021 15:23:16 - INFO - trainer -     T-F1(T) = 0.9817158931082982
10/08/2021 15:23:16 - INFO - trainer -     U-F1(A) = 0.766990291262136
10/08/2021 15:23:16 - INFO - trainer -     U-F1(E) = 0.7532467532467532
10/08/2021 15:23:16 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/08/2021 15:23:16 - INFO - trainer -     U-F1(O) = 0.9663117472508291
10/08/2021 15:23:16 - INFO - trainer -     intent_acc = 0.9376918354818907
10/08/2021 15:23:16 - INFO - trainer -     loss = 0.3729736934982094
10/08/2021 15:23:16 - INFO - trainer -     semantic_frame_acc = 0.9214241866175568
10/08/2021 15:23:16 - INFO - trainer -     slot_f1 = 0.9916177703269069
10/08/2021 15:23:16 - INFO - trainer -     slot_precision = 0.9921722113502935
10/08/2021 15:23:16 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 15:23:16 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:23:16 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:23:16 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:23:16 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:23:16 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:23:16 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:23:16 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:23:16 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:23:16 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:23:16 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:23:16 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:23:16 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:23:16 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:23:16 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:23:16 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:23:16 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:23:16 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:24:56 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 737/918 [07:10<01:31,  1.98it/s]
10/08/2021 15:24:56 - INFO - trainer -     Num examples = 3258
10/08/2021 15:24:56 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.85it/s]
10/08/2021 15:25:15 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:25:15 - INFO - trainer -     T-F1 = 0.9904710046283692| 51/51 [00:17<00:00,  2.88it/s]
10/08/2021 15:25:15 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/08/2021 15:25:15 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:25:15 - INFO - trainer -     T-F1(O) = 0.9962018448182312
10/08/2021 15:25:15 - INFO - trainer -     T-F1(P) = 0.9970828471411902
10/08/2021 15:25:15 - INFO - trainer -     T-F1(S) = 0.9907663896583563
10/08/2021 15:25:15 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 15:25:15 - INFO - trainer -     U-F1(A) = 0.6974358974358974
10/08/2021 15:25:15 - INFO - trainer -     U-F1(E) = 0.7290076335877863
10/08/2021 15:25:15 - INFO - trainer -     U-F1(I) = 0.22727272727272727
10/08/2021 15:25:15 - INFO - trainer -     U-F1(O) = 0.9622805492786373
10/08/2021 15:25:15 - INFO - trainer -     intent_acc = 0.9306322897483118
10/08/2021 15:25:15 - INFO - trainer -     loss = 0.3837599238636447
10/08/2021 15:25:15 - INFO - trainer -     semantic_frame_acc = 0.9119091467157765
10/08/2021 15:25:15 - INFO - trainer -     slot_f1 = 0.9902534113060428
10/08/2021 15:25:15 - INFO - trainer -     slot_precision = 0.9875034712579839
10/08/2021 15:25:15 - INFO - trainer -     slot_recall = 0.9930187098575817

10/08/2021 15:25:15 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:25:15 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:25:15 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:25:15 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:25:15 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:25:15 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:25:15 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:25:15 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:25:15 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:25:15 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:25:15 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:25:15 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:25:15 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:25:15 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:25:15 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:25:15 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:25:15 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [09:00<00:00,  1.70it/s]
Epoch:  50%|                                   | 10/20 [1:02:31<1:21:47, 490.79s/it]10/08/2021 15:26:56 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 19/918 [00:09<07:36,  1.97it/s]
10/08/2021 15:26:56 - INFO - trainer -     Num examples = 3258
10/08/2021 15:26:56 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.83it/s]
10/08/2021 15:27:15 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:27:15 - INFO - trainer -     T-F1 = 0.990174672489083| 51/51 [00:17<00:00,  2.91it/s]
10/08/2021 15:27:15 - INFO - trainer -     T-F1(C) = 0.9783549783549784
10/08/2021 15:27:15 - INFO - trainer -     T-F1(L) = 0.9792284866468843
10/08/2021 15:27:15 - INFO - trainer -     T-F1(O) = 0.9960971379011275
10/08/2021 15:27:15 - INFO - trainer -     T-F1(P) = 0.9961999415375621
10/08/2021 15:27:15 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 15:27:15 - INFO - trainer -     T-F1(T) = 0.9789621318373072
10/08/2021 15:27:15 - INFO - trainer -     U-F1(A) = 0.7246376811594202
10/08/2021 15:27:15 - INFO - trainer -     U-F1(E) = 0.7504621072088724
10/08/2021 15:27:15 - INFO - trainer -     U-F1(I) = 0.27999999999999997
10/08/2021 15:27:15 - INFO - trainer -     U-F1(O) = 0.9646729625743267
10/08/2021 15:27:15 - INFO - trainer -     intent_acc = 0.9340085942295887
10/08/2021 15:27:15 - INFO - trainer -     loss = 0.3985909536188724
10/08/2021 15:27:15 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/08/2021 15:27:15 - INFO - trainer -     slot_f1 = 0.9899497487437187
10/08/2021 15:27:15 - INFO - trainer -     slot_precision = 0.9896734579960926
10/08/2021 15:27:15 - INFO - trainer -     slot_recall = 0.9902261938006144

10/08/2021 15:27:15 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:27:15 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:27:15 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:27:15 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:27:15 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:27:15 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:27:15 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:27:15 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:27:15 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:27:15 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:27:15 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:27:15 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:27:15 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:27:15 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:27:15 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:27:15 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:27:15 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:28:56 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 219/918 [02:09<05:52,  1.98it/s]
10/08/2021 15:28:56 - INFO - trainer -     Num examples = 3258
10/08/2021 15:28:56 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.85it/s]
10/08/2021 15:29:14 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:29:14 - INFO - trainer -     T-F1 = 0.990042286181967| 51/51 [00:17<00:00,  2.94it/s]
10/08/2021 15:29:14 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/08/2021 15:29:14 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:29:14 - INFO - trainer -     T-F1(O) = 0.9960422878828951
10/08/2021 15:29:14 - INFO - trainer -     T-F1(P) = 0.9961977186311787
10/08/2021 15:29:14 - INFO - trainer -     T-F1(S) = 0.9921186833565137
10/08/2021 15:29:14 - INFO - trainer -     T-F1(T) = 0.970954356846473
10/08/2021 15:29:14 - INFO - trainer -     U-F1(A) = 0.7254901960784313
10/08/2021 15:29:14 - INFO - trainer -     U-F1(E) = 0.7434944237918216
10/08/2021 15:29:14 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/08/2021 15:29:14 - INFO - trainer -     U-F1(O) = 0.9646033129904098
10/08/2021 15:29:14 - INFO - trainer -     intent_acc = 0.9340085942295887
10/08/2021 15:29:14 - INFO - trainer -     loss = 0.4764078786852313
10/08/2021 15:29:14 - INFO - trainer -     semantic_frame_acc = 0.9165131982811541
10/08/2021 15:29:14 - INFO - trainer -     slot_f1 = 0.9898144272359426
10/08/2021 15:29:14 - INFO - trainer -     slot_precision = 0.9891243725599553
10/08/2021 15:29:14 - INFO - trainer -     slot_recall = 0.9905054454063111

10/08/2021 15:29:14 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:29:14 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:29:14 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:29:14 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:29:14 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:29:14 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:29:14 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:29:14 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:29:14 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:29:14 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:29:14 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:29:14 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:29:14 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:29:14 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:29:14 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:29:14 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:29:14 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:30:54 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 419/918 [04:07<04:09,  2.00it/s]
10/08/2021 15:30:54 - INFO - trainer -     Num examples = 3258
10/08/2021 15:30:54 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.89it/s]
10/08/2021 15:31:12 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:31:12 - INFO - trainer -     T-F1 = 0.9895081073715765| 51/51 [00:17<00:00,  2.97it/s]
10/08/2021 15:31:12 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/08/2021 15:31:12 - INFO - trainer -     T-F1(L) = 0.9736070381231672
10/08/2021 15:31:12 - INFO - trainer -     T-F1(O) = 0.995823615555676
10/08/2021 15:31:12 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/08/2021 15:31:12 - INFO - trainer -     T-F1(S) = 0.9912240184757506
10/08/2021 15:31:12 - INFO - trainer -     T-F1(T) = 0.9747899159663864
10/08/2021 15:31:12 - INFO - trainer -     U-F1(A) = 0.7254901960784313
10/08/2021 15:31:12 - INFO - trainer -     U-F1(E) = 0.723404255319149
10/08/2021 15:31:12 - INFO - trainer -     U-F1(I) = 0.3137254901960785
10/08/2021 15:31:12 - INFO - trainer -     U-F1(O) = 0.9627437325905293
10/08/2021 15:31:12 - INFO - trainer -     intent_acc = 0.9312461632903621
10/08/2021 15:31:12 - INFO - trainer -     loss = 0.40858889225066874
10/08/2021 15:31:12 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/08/2021 15:31:12 - INFO - trainer -     slot_f1 = 0.9892682926829267
10/08/2021 15:31:12 - INFO - trainer -     slot_precision = 0.9874791318864775
10/08/2021 15:31:12 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 15:31:12 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:31:12 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:31:12 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:31:12 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:31:12 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:31:12 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:31:12 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:31:12 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:31:12 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:31:12 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:31:12 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:31:12 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:31:12 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:31:12 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:31:12 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:31:12 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:31:12 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:32:52 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 619/918 [06:05<02:30,  1.98it/s]
10/08/2021 15:32:52 - INFO - trainer -     Num examples = 3258
10/08/2021 15:32:52 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.87it/s]
10/08/2021 15:33:11 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:33:11 - INFO - trainer -     T-F1 = 0.9912616056799562| 51/51 [00:17<00:00,  2.94it/s]
10/08/2021 15:33:11 - INFO - trainer -     T-F1(C) = 0.9742857142857144
10/08/2021 15:33:11 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 15:33:11 - INFO - trainer -     T-F1(O) = 0.9965315412963365
10/08/2021 15:33:11 - INFO - trainer -     T-F1(P) = 0.9967864446392054
10/08/2021 15:33:11 - INFO - trainer -     T-F1(S) = 0.9916512059369202
10/08/2021 15:33:11 - INFO - trainer -     T-F1(T) = 0.9802816901408451
10/08/2021 15:33:11 - INFO - trainer -     U-F1(A) = 0.7121951219512194
10/08/2021 15:33:11 - INFO - trainer -     U-F1(E) = 0.7407407407407408
10/08/2021 15:33:11 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/08/2021 15:33:11 - INFO - trainer -     U-F1(O) = 0.9643979057591623
10/08/2021 15:33:11 - INFO - trainer -     intent_acc = 0.9330877839165131
10/08/2021 15:33:11 - INFO - trainer -     loss = 0.4325565908323316
10/08/2021 15:33:11 - INFO - trainer -     semantic_frame_acc = 0.9171270718232044
10/08/2021 15:33:11 - INFO - trainer -     slot_f1 = 0.9910614525139665
10/08/2021 15:33:11 - INFO - trainer -     slot_precision = 0.9913383626711372
10/08/2021 15:33:11 - INFO - trainer -     slot_recall = 0.9907846970120078

10/08/2021 15:33:11 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:33:11 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:33:11 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:33:11 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:33:11 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:33:11 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:33:11 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:33:11 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:33:11 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:33:11 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:33:11 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:33:11 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:33:11 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:33:11 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:33:11 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:33:11 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:33:11 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:34:51 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 819/918 [08:04<00:49,  1.99it/s]
10/08/2021 15:34:51 - INFO - trainer -     Num examples = 3258
10/08/2021 15:34:51 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.86it/s]
10/08/2021 15:35:09 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:35:09 - INFO - trainer -     T-F1 = 0.9919431926805953| 51/51 [00:17<00:00,  2.93it/s]
10/08/2021 15:35:09 - INFO - trainer -     T-F1(C) = 0.9855072463768115
10/08/2021 15:35:09 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 15:35:09 - INFO - trainer -     T-F1(O) = 0.996802687909825
10/08/2021 15:35:09 - INFO - trainer -     T-F1(P) = 0.9967864446392054
10/08/2021 15:35:09 - INFO - trainer -     T-F1(S) = 0.9898242368177613
10/08/2021 15:35:09 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 15:35:09 - INFO - trainer -     U-F1(A) = 0.6736842105263158
10/08/2021 15:35:09 - INFO - trainer -     U-F1(E) = 0.7385740402193784
10/08/2021 15:35:09 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/08/2021 15:35:09 - INFO - trainer -     U-F1(O) = 0.9616724738675958
10/08/2021 15:35:09 - INFO - trainer -     intent_acc = 0.9303253529772867
10/08/2021 15:35:09 - INFO - trainer -     loss = 0.4340631273009029
10/08/2021 15:35:09 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/08/2021 15:35:09 - INFO - trainer -     slot_f1 = 0.9917586255063556
10/08/2021 15:35:09 - INFO - trainer -     slot_precision = 0.9921743991056456
10/08/2021 15:35:09 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 15:35:09 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:35:09 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:35:09 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:35:09 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:35:09 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:35:09 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:35:09 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:35:09 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:35:09 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:35:09 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:35:09 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:35:09 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:35:09 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:35:09 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:35:09 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:35:09 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:35:09 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [09:12<00:00,  1.66it/s]
Epoch:  55%|                               | 11/20 [1:11:44<1:16:28, 509.78s/it]10/08/2021 15:36:51 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 101/918 [00:51<06:44,  2.02it/s]
10/08/2021 15:36:51 - INFO - trainer -     Num examples = 3258
10/08/2021 15:36:51 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:17<00:00,  2.87it/s]
10/08/2021 15:37:09 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:37:09 - INFO - trainer -     T-F1 = 0.990988530857455| 51/51 [00:17<00:00,  2.93it/s]
10/08/2021 15:37:09 - INFO - trainer -     T-F1(C) = 0.9769452449567723
10/08/2021 15:37:09 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 15:37:09 - INFO - trainer -     T-F1(O) = 0.996423151961847
10/08/2021 15:37:09 - INFO - trainer -     T-F1(P) = 0.9970777323202805
10/08/2021 15:37:09 - INFO - trainer -     T-F1(S) = 0.990282276723739
10/08/2021 15:37:09 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/08/2021 15:37:09 - INFO - trainer -     U-F1(A) = 0.6590909090909091
10/08/2021 15:37:09 - INFO - trainer -     U-F1(E) = 0.7592592592592594
10/08/2021 15:37:09 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/08/2021 15:37:09 - INFO - trainer -     U-F1(O) = 0.9630272522131574
10/08/2021 15:37:09 - INFO - trainer -     intent_acc = 0.9330877839165131
10/08/2021 15:37:09 - INFO - trainer -     loss = 0.4671476856239286
10/08/2021 15:37:09 - INFO - trainer -     semantic_frame_acc = 0.9165131982811541
10/08/2021 15:37:09 - INFO - trainer -     slot_f1 = 0.990782122905028
10/08/2021 15:37:09 - INFO - trainer -     slot_precision = 0.9910589550153674
10/08/2021 15:37:09 - INFO - trainer -     slot_recall = 0.9905054454063111

10/08/2021 15:37:09 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:37:09 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:37:09 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:37:09 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:37:09 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:37:09 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:37:09 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:37:09 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:37:09 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:37:09 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:37:09 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:37:09 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:37:09 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:37:09 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:37:09 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:37:09 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:37:09 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:38:44 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 301/918 [02:45<02:19,  4.41it/s]
10/08/2021 15:38:44 - INFO - trainer -     Num examples = 3258
10/08/2021 15:38:44 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.25it/s]
10/08/2021 15:38:52 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:38:52 - INFO - trainer -     T-F1 = 0.9918010385351189| 51/51 [00:07<00:00,  7.36it/s]
10/08/2021 15:38:52 - INFO - trainer -     T-F1(C) = 0.976878612716763
10/08/2021 15:38:52 - INFO - trainer -     T-F1(L) = 0.9940119760479043
10/08/2021 15:38:52 - INFO - trainer -     T-F1(O) = 0.9967493769639181
10/08/2021 15:38:52 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:38:52 - INFO - trainer -     T-F1(S) = 0.9907407407407407
10/08/2021 15:38:52 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 15:38:52 - INFO - trainer -     U-F1(A) = 0.6404494382022472
10/08/2021 15:38:52 - INFO - trainer -     U-F1(E) = 0.7593582887700535
10/08/2021 15:38:52 - INFO - trainer -     U-F1(I) = 0.24489795918367346
10/08/2021 15:38:52 - INFO - trainer -     U-F1(O) = 0.9605446927374303
10/08/2021 15:38:52 - INFO - trainer -     intent_acc = 0.929097605893186
10/08/2021 15:38:52 - INFO - trainer -     loss = 0.41572423633553235
10/08/2021 15:38:52 - INFO - trainer -     semantic_frame_acc = 0.9134438305709024
10/08/2021 15:38:52 - INFO - trainer -     slot_f1 = 0.9916130835896002
10/08/2021 15:38:52 - INFO - trainer -     slot_precision = 0.9927232017912119
10/08/2021 15:38:52 - INFO - trainer -     slot_recall = 0.9905054454063111

10/08/2021 15:38:52 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:38:52 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:38:52 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:38:52 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:38:52 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:38:52 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:38:52 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:38:52 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:38:52 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:38:52 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:38:52 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:38:52 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:38:52 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:38:52 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:38:52 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:38:52 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:38:52 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:39:38 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 501/918 [03:38<01:36,  4.31it/s]
10/08/2021 15:39:38 - INFO - trainer -     Num examples = 3258
10/08/2021 15:39:38 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.11it/s]
10/08/2021 15:39:45 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:39:45 - INFO - trainer -     T-F1 = 0.9907558455682436| 51/51 [00:07<00:00,  7.18it/s]
10/08/2021 15:39:45 - INFO - trainer -     T-F1(C) = 0.9757489300998574
10/08/2021 15:39:45 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:39:45 - INFO - trainer -     T-F1(O) = 0.9963083604777416
10/08/2021 15:39:45 - INFO - trainer -     T-F1(P) = 0.9976649153531816
10/08/2021 15:39:45 - INFO - trainer -     T-F1(S) = 0.9875976113918237
10/08/2021 15:39:45 - INFO - trainer -     T-F1(T) = 0.9846153846153847
10/08/2021 15:39:45 - INFO - trainer -     U-F1(A) = 0.72
10/08/2021 15:39:45 - INFO - trainer -     U-F1(E) = 0.7481481481481482
10/08/2021 15:39:45 - INFO - trainer -     U-F1(I) = 0.24390243902439024
10/08/2021 15:39:45 - INFO - trainer -     U-F1(O) = 0.9635571054925894
10/08/2021 15:39:45 - INFO - trainer -     intent_acc = 0.9337016574585635
10/08/2021 15:39:45 - INFO - trainer -     loss = 0.47040310553183745
10/08/2021 15:39:45 - INFO - trainer -     semantic_frame_acc = 0.9165131982811541
10/08/2021 15:39:45 - INFO - trainer -     slot_f1 = 0.9905450500556174
10/08/2021 15:39:45 - INFO - trainer -     slot_precision = 0.9864303517031293
10/08/2021 15:39:45 - INFO - trainer -     slot_recall = 0.9946942194917621

10/08/2021 15:39:45 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:39:45 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:39:45 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:39:45 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:39:45 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:39:45 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:39:45 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:39:45 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:39:45 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:39:45 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:39:45 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:39:45 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:39:45 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:39:45 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:39:45 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:39:45 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:39:45 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:40:32 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 701/918 [04:32<00:50,  4.29it/s]
10/08/2021 15:40:32 - INFO - trainer -     Num examples = 3258
10/08/2021 15:40:32 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.04it/s]
10/08/2021 15:40:40 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:40:40 - INFO - trainer -     T-F1 = 0.9907331698010357| 51/51 [00:07<00:00,  7.15it/s]
10/08/2021 15:40:40 - INFO - trainer -     T-F1(C) = 0.9784172661870504
10/08/2021 15:40:40 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 15:40:40 - INFO - trainer -     T-F1(O) = 0.9963119644213039
10/08/2021 15:40:40 - INFO - trainer -     T-F1(P) = 0.9970794392523364
10/08/2021 15:40:40 - INFO - trainer -     T-F1(S) = 0.988929889298893
10/08/2021 15:40:40 - INFO - trainer -     T-F1(T) = 0.9790209790209791
10/08/2021 15:40:40 - INFO - trainer -     U-F1(A) = 0.6888888888888889
10/08/2021 15:40:40 - INFO - trainer -     U-F1(E) = 0.7442748091603053
10/08/2021 15:40:40 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/08/2021 15:40:40 - INFO - trainer -     U-F1(O) = 0.9632582322357018
10/08/2021 15:40:40 - INFO - trainer -     intent_acc = 0.9333947206875384
10/08/2021 15:40:40 - INFO - trainer -     loss = 0.4607623532122257
10/08/2021 15:40:40 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/08/2021 15:40:40 - INFO - trainer -     slot_f1 = 0.990521327014218
10/08/2021 15:40:40 - INFO - trainer -     slot_precision = 0.9888672418591706
10/08/2021 15:40:40 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 15:40:40 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:40:40 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:40:40 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:40:40 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:40:40 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:40:40 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:40:40 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:40:40 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:40:40 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:40:40 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:40:40 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:40:40 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:40:40 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:40:40 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:40:40 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:40:40 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:40:40 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:41:27 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 901/918 [05:27<00:03,  4.29it/s]
10/08/2021 15:41:27 - INFO - trainer -     Num examples = 3258
10/08/2021 15:41:27 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  7.00it/s]
10/08/2021 15:41:34 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:41:34 - INFO - trainer -     T-F1 = 0.9911359607254875| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 15:41:34 - INFO - trainer -     T-F1(C) = 0.9741379310344828
10/08/2021 15:41:34 - INFO - trainer -     T-F1(L) = 0.9793510324483775
10/08/2021 15:41:34 - INFO - trainer -     T-F1(O) = 0.9964756276093911
10/08/2021 15:41:34 - INFO - trainer -     T-F1(P) = 0.9973722627737226
10/08/2021 15:41:34 - INFO - trainer -     T-F1(S) = 0.9921259842519685
10/08/2021 15:41:34 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 15:41:34 - INFO - trainer -     U-F1(A) = 0.7
10/08/2021 15:41:34 - INFO - trainer -     U-F1(E) = 0.7467652495378928
10/08/2021 15:41:34 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 15:41:34 - INFO - trainer -     U-F1(O) = 0.9627955493741308
10/08/2021 15:41:34 - INFO - trainer -     intent_acc = 0.932780847145488
10/08/2021 15:41:34 - INFO - trainer -     loss = 0.4761766831518388
10/08/2021 15:41:34 - INFO - trainer -     semantic_frame_acc = 0.9165131982811541
10/08/2021 15:41:34 - INFO - trainer -     slot_f1 = 0.9909331845445669
10/08/2021 15:41:34 - INFO - trainer -     slot_precision = 0.9899665551839465
10/08/2021 15:41:34 - INFO - trainer -     slot_recall = 0.9919017034347948

10/08/2021 15:41:34 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:41:34 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:41:34 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:41:34 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:41:34 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:41:34 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:41:34 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:41:34 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:41:34 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:41:34 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:41:34 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:41:34 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:41:34 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:41:34 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:41:34 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:41:34 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:41:34 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [05:39<00:00,  2.71it/s]
Epoch:  60%|                            | 12/20 [1:17:23<1:01:02, 457.86s/it]10/08/2021 15:42:21 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 183/918 [00:43<02:52,  4.25it/s]
10/08/2021 15:42:21 - INFO - trainer -     Num examples = 3258
10/08/2021 15:42:21 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.99it/s]
10/08/2021 15:42:29 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:42:29 - INFO - trainer -     T-F1 = 0.9911262798634813| 51/51 [00:07<00:00,  7.11it/s]
10/08/2021 15:42:29 - INFO - trainer -     T-F1(C) = 0.9769452449567723
10/08/2021 15:42:29 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/08/2021 15:42:29 - INFO - trainer -     T-F1(O) = 0.9964771557097176
10/08/2021 15:42:29 - INFO - trainer -     T-F1(P) = 0.9961999415375621
10/08/2021 15:42:29 - INFO - trainer -     T-F1(S) = 0.9907493061979649
10/08/2021 15:42:29 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 15:42:29 - INFO - trainer -     U-F1(A) = 0.7216494845360824
10/08/2021 15:42:29 - INFO - trainer -     U-F1(E) = 0.7335766423357662
10/08/2021 15:42:29 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/08/2021 15:42:29 - INFO - trainer -     U-F1(O) = 0.9616189811584089
10/08/2021 15:42:29 - INFO - trainer -     intent_acc = 0.9306322897483118
10/08/2021 15:42:29 - INFO - trainer -     loss = 0.47553207024055366
10/08/2021 15:42:29 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/08/2021 15:42:29 - INFO - trainer -     slot_f1 = 0.9909230554391845
10/08/2021 15:42:29 - INFO - trainer -     slot_precision = 0.9910614525139665
10/08/2021 15:42:29 - INFO - trainer -     slot_recall = 0.9907846970120078

10/08/2021 15:42:29 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:42:29 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:42:29 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:42:29 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:42:29 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:42:29 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:42:29 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:42:29 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:42:29 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:42:29 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:42:29 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:42:29 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:42:29 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:42:29 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:42:29 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:42:29 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:42:29 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:43:16 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 383/918 [01:37<02:04,  4.28it/s]
10/08/2021 15:43:16 - INFO - trainer -     Num examples = 3258
10/08/2021 15:43:16 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.98it/s]
10/08/2021 15:43:24 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:43:24 - INFO - trainer -     T-F1 = 0.991268758526603| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 15:43:24 - INFO - trainer -     T-F1(C) = 0.9768115942028986
10/08/2021 15:43:24 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:43:24 - INFO - trainer -     T-F1(O) = 0.9965304130976905
10/08/2021 15:43:24 - INFO - trainer -     T-F1(P) = 0.9964973730297724
10/08/2021 15:43:24 - INFO - trainer -     T-F1(S) = 0.9926062846580406
10/08/2021 15:43:24 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:43:24 - INFO - trainer -     U-F1(A) = 0.6358381502890174
10/08/2021 15:43:24 - INFO - trainer -     U-F1(E) = 0.7362428842504743
10/08/2021 15:43:24 - INFO - trainer -     U-F1(I) = 0.24390243902439024
10/08/2021 15:43:24 - INFO - trainer -     U-F1(O) = 0.9613852813852815
10/08/2021 15:43:24 - INFO - trainer -     intent_acc = 0.9300184162062615
10/08/2021 15:43:24 - INFO - trainer -     loss = 0.4687110490950884
10/08/2021 15:43:24 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/08/2021 15:43:24 - INFO - trainer -     slot_f1 = 0.9910689366452694
10/08/2021 15:43:24 - INFO - trainer -     slot_precision = 0.9905160390516039
10/08/2021 15:43:24 - INFO - trainer -     slot_recall = 0.991622451829098

10/08/2021 15:43:24 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:43:24 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:43:24 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:43:24 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:43:24 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:43:24 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:43:24 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:43:24 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:43:24 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:43:24 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:43:24 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:43:24 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:43:24 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:43:24 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:43:24 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:43:24 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:43:24 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:44:11 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 583/918 [02:32<01:19,  4.23it/s]
10/08/2021 15:44:11 - INFO - trainer -     Num examples = 3258
10/08/2021 15:44:11 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 15:44:19 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:44:19 - INFO - trainer -     T-F1 = 0.9901693063899508| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 15:44:19 - INFO - trainer -     T-F1(C) = 0.9697841726618704
10/08/2021 15:44:19 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:44:19 - INFO - trainer -     T-F1(O) = 0.9960979839583786
10/08/2021 15:44:19 - INFO - trainer -     T-F1(P) = 0.997081144191477
10/08/2021 15:44:19 - INFO - trainer -     T-F1(S) = 0.9911832946635731
10/08/2021 15:44:19 - INFO - trainer -     T-F1(T) = 0.9774647887323944
10/08/2021 15:44:19 - INFO - trainer -     U-F1(A) = 0.7428571428571428
10/08/2021 15:44:19 - INFO - trainer -     U-F1(E) = 0.72552783109405
10/08/2021 15:44:19 - INFO - trainer -     U-F1(I) = 0.22222222222222224
10/08/2021 15:44:19 - INFO - trainer -     U-F1(O) = 0.9623693379790941
10/08/2021 15:44:19 - INFO - trainer -     intent_acc = 0.9312461632903621
10/08/2021 15:44:19 - INFO - trainer -     loss = 0.4715034183772171
10/08/2021 15:44:19 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/08/2021 15:44:19 - INFO - trainer -     slot_f1 = 0.9898058930316994
10/08/2021 15:44:19 - INFO - trainer -     slot_precision = 0.9899441340782122
10/08/2021 15:44:19 - INFO - trainer -     slot_recall = 0.9896676905892209

10/08/2021 15:44:19 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:44:19 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:44:19 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:44:19 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:44:19 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:44:19 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:44:19 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:44:19 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:44:19 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:44:19 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:44:19 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:44:19 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:44:19 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:44:19 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:44:19 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:44:19 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:44:19 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:45:06 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 783/918 [03:27<00:31,  4.25it/s]
10/08/2021 15:45:06 - INFO - trainer -     Num examples = 3258
10/08/2021 15:45:06 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.97it/s]
10/08/2021 15:45:13 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:45:13 - INFO - trainer -     T-F1 = 0.9903256574465186| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 15:45:13 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/08/2021 15:45:13 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:45:13 - INFO - trainer -     T-F1(O) = 0.9961490481097793
10/08/2021 15:45:13 - INFO - trainer -     T-F1(P) = 0.9964953271028038
10/08/2021 15:45:13 - INFO - trainer -     T-F1(S) = 0.9902912621359223
10/08/2021 15:45:13 - INFO - trainer -     T-F1(T) = 0.9831932773109243
10/08/2021 15:45:13 - INFO - trainer -     U-F1(A) = 0.7058823529411765
10/08/2021 15:45:13 - INFO - trainer -     U-F1(E) = 0.7513812154696133
10/08/2021 15:45:13 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 15:45:13 - INFO - trainer -     U-F1(O) = 0.9626266154383515
10/08/2021 15:45:13 - INFO - trainer -     intent_acc = 0.9321669736034377
10/08/2021 15:45:13 - INFO - trainer -     loss = 0.4845263352435009
10/08/2021 15:45:13 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/08/2021 15:45:13 - INFO - trainer -     slot_f1 = 0.9901045296167248
10/08/2021 15:45:13 - INFO - trainer -     slot_precision = 0.988313856427379
10/08/2021 15:45:13 - INFO - trainer -     slot_recall = 0.9919017034347948

10/08/2021 15:45:13 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:45:13 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:45:13 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:45:13 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:45:13 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:45:13 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:45:13 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:45:13 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:45:13 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:45:13 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:45:13 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:45:13 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:45:13 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:45:13 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:45:13 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:45:13 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:45:13 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:06<00:00,  3.72it/s]
Epoch:  65%|                         | 13/20 [1:21:30<45:57, 393.94s/it]10/08/2021 15:46:00 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 65/918 [00:15<03:20,  4.25it/s]
10/08/2021 15:46:00 - INFO - trainer -     Num examples = 3258
10/08/2021 15:46:00 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 15:46:08 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:46:08 - INFO - trainer -     T-F1 = 0.9909934497816593| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 15:46:08 - INFO - trainer -     T-F1(C) = 0.9782293178519593
10/08/2021 15:46:08 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:46:08 - INFO - trainer -     T-F1(O) = 0.9964223764093669
10/08/2021 15:46:08 - INFO - trainer -     T-F1(P) = 0.9970777323202805
10/08/2021 15:46:08 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 15:46:08 - INFO - trainer -     T-F1(T) = 0.9818181818181818
10/08/2021 15:46:08 - INFO - trainer -     U-F1(A) = 0.6593406593406593
10/08/2021 15:46:08 - INFO - trainer -     U-F1(E) = 0.7368421052631579
10/08/2021 15:46:08 - INFO - trainer -     U-F1(I) = 0.27272727272727276
10/08/2021 15:46:08 - INFO - trainer -     U-F1(O) = 0.9617922889892324
10/08/2021 15:46:08 - INFO - trainer -     intent_acc = 0.9303253529772867
10/08/2021 15:46:08 - INFO - trainer -     loss = 0.48251620753138674
10/08/2021 15:46:08 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/08/2021 15:46:08 - INFO - trainer -     slot_f1 = 0.990787269681742
10/08/2021 15:46:08 - INFO - trainer -     slot_precision = 0.9905107451855987
10/08/2021 15:46:08 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 15:46:08 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:46:08 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:46:08 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:46:08 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:46:08 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:46:08 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:46:08 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:46:08 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:46:08 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:46:08 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:46:08 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:46:08 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:46:08 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:46:08 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:46:08 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:46:08 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:46:08 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:46:55 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 265/918 [01:10<02:32,  4.27it/s]
10/08/2021 15:46:55 - INFO - trainer -     Num examples = 3258
10/08/2021 15:46:55 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 15:47:03 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:47:03 - INFO - trainer -     T-F1 = 0.990450204638472| 51/51 [00:07<00:00,  7.10it/s]
10/08/2021 15:47:03 - INFO - trainer -     T-F1(C) = 0.9783549783549784
10/08/2021 15:47:03 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:47:03 - INFO - trainer -     T-F1(O) = 0.996205139325599
10/08/2021 15:47:03 - INFO - trainer -     T-F1(P) = 0.9964973730297724
10/08/2021 15:47:03 - INFO - trainer -     T-F1(S) = 0.9898336414048059
10/08/2021 15:47:03 - INFO - trainer -     T-F1(T) = 0.9788434414668548
10/08/2021 15:47:03 - INFO - trainer -     U-F1(A) = 0.7184466019417476
10/08/2021 15:47:03 - INFO - trainer -     U-F1(E) = 0.7198443579766537
10/08/2021 15:47:03 - INFO - trainer -     U-F1(I) = 0.27272727272727276
10/08/2021 15:47:03 - INFO - trainer -     U-F1(O) = 0.9627955493741308
10/08/2021 15:47:03 - INFO - trainer -     intent_acc = 0.9312461632903621
10/08/2021 15:47:03 - INFO - trainer -     loss = 0.5221115248460396
10/08/2021 15:47:03 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/08/2021 15:47:03 - INFO - trainer -     slot_f1 = 0.9900934840239988
10/08/2021 15:47:03 - INFO - trainer -     slot_precision = 0.9894032348020078
10/08/2021 15:47:03 - INFO - trainer -     slot_recall = 0.9907846970120078

10/08/2021 15:47:03 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:47:03 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:47:03 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:47:03 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:47:03 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:47:03 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:47:03 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:47:03 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:47:03 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:47:03 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:47:03 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:47:03 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:47:03 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:47:03 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:47:03 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:47:03 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:47:03 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:47:50 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 465/918 [02:04<01:46,  4.24it/s]
10/08/2021 15:47:50 - INFO - trainer -     Num examples = 3258
10/08/2021 15:47:50 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:47:58 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:47:58 - INFO - trainer -     T-F1 = 0.9900585591720006| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 15:47:58 - INFO - trainer -     T-F1(C) = 0.9769452449567723
10/08/2021 15:47:58 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:47:58 - INFO - trainer -     T-F1(O) = 0.9960397113871859
10/08/2021 15:47:58 - INFO - trainer -     T-F1(P) = 0.9962043795620438
10/08/2021 15:47:58 - INFO - trainer -     T-F1(S) = 0.9884951679705476
10/08/2021 15:47:58 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 15:47:58 - INFO - trainer -     U-F1(A) = 0.6974358974358974
10/08/2021 15:47:58 - INFO - trainer -     U-F1(E) = 0.7476635514018691
10/08/2021 15:47:58 - INFO - trainer -     U-F1(I) = 0.22222222222222224
10/08/2021 15:47:58 - INFO - trainer -     U-F1(O) = 0.9628984497474307
10/08/2021 15:47:58 - INFO - trainer -     intent_acc = 0.9321669736034377
10/08/2021 15:47:58 - INFO - trainer -     loss = 0.48839161924871743
10/08/2021 15:47:58 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/08/2021 15:47:58 - INFO - trainer -     slot_f1 = 0.9898314528485862
10/08/2021 15:47:58 - INFO - trainer -     slot_precision = 0.9874930516953864
10/08/2021 15:47:58 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 15:47:58 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:47:58 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:47:58 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:47:58 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:47:58 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:47:58 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:47:58 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:47:58 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:47:58 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:47:58 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:47:58 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:47:58 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:47:58 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:47:58 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:47:58 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:47:58 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:47:58 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:48:45 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 665/918 [02:59<00:59,  4.23it/s]
10/08/2021 15:48:45 - INFO - trainer -     Num examples = 3258
10/08/2021 15:48:45 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:48:53 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:48:53 - INFO - trainer -     T-F1 = 0.9914087003954726| 51/51 [00:07<00:00,  7.06it/s]
10/08/2021 15:48:53 - INFO - trainer -     T-F1(C) = 0.9769452449567723
10/08/2021 15:48:53 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 15:48:53 - INFO - trainer -     T-F1(O) = 0.9965840698367944
10/08/2021 15:48:53 - INFO - trainer -     T-F1(P) = 0.9967864446392054
10/08/2021 15:48:53 - INFO - trainer -     T-F1(S) = 0.9912321181356715
10/08/2021 15:48:53 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 15:48:53 - INFO - trainer -     U-F1(A) = 0.7106598984771575
10/08/2021 15:48:53 - INFO - trainer -     U-F1(E) = 0.7364485981308411
10/08/2021 15:48:53 - INFO - trainer -     U-F1(I) = 0.2608695652173913
10/08/2021 15:48:53 - INFO - trainer -     U-F1(O) = 0.9620076681770652
10/08/2021 15:48:53 - INFO - trainer -     intent_acc = 0.930939226519337
10/08/2021 15:48:53 - INFO - trainer -     loss = 0.5323092905329723
10/08/2021 15:48:53 - INFO - trainer -     semantic_frame_acc = 0.9158993247391037
10/08/2021 15:48:53 - INFO - trainer -     slot_f1 = 0.9912121634816571
10/08/2021 15:48:53 - INFO - trainer -     slot_precision = 0.9902452619843924
10/08/2021 15:48:53 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 15:48:53 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:48:53 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:48:53 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:48:53 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:48:53 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:48:53 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:48:53 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:48:53 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:48:53 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:48:53 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:48:53 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:48:53 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:48:53 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:48:53 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:48:53 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:48:53 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:48:53 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:49:40 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 865/918 [03:54<00:12,  4.20it/s]
10/08/2021 15:49:40 - INFO - trainer -     Num examples = 3258
10/08/2021 15:49:40 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:49:48 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:49:48 - INFO - trainer -     T-F1 = 0.9907281156258523| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 15:49:48 - INFO - trainer -     T-F1(C) = 0.9728958630527819
10/08/2021 15:49:48 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 15:49:48 - INFO - trainer -     T-F1(O) = 0.9963127643422622
10/08/2021 15:49:48 - INFO - trainer -     T-F1(P) = 0.9959112149532712
10/08/2021 15:49:48 - INFO - trainer -     T-F1(S) = 0.9916666666666667
10/08/2021 15:49:48 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 15:49:48 - INFO - trainer -     U-F1(A) = 0.7300000000000001
10/08/2021 15:49:48 - INFO - trainer -     U-F1(E) = 0.7500000000000001
10/08/2021 15:49:48 - INFO - trainer -     U-F1(I) = 0.27272727272727276
10/08/2021 15:49:48 - INFO - trainer -     U-F1(O) = 0.9640864714086472
10/08/2021 15:49:48 - INFO - trainer -     intent_acc = 0.934622467771639
10/08/2021 15:49:48 - INFO - trainer -     loss = 0.5022022911146575
10/08/2021 15:49:48 - INFO - trainer -     semantic_frame_acc = 0.9174340085942296
10/08/2021 15:49:48 - INFO - trainer -     slot_f1 = 0.990516039051604
10/08/2021 15:49:48 - INFO - trainer -     slot_precision = 0.989412092504876
10/08/2021 15:49:48 - INFO - trainer -     slot_recall = 0.991622451829098

10/08/2021 15:49:48 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:49:48 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:49:48 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:49:48 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:49:48 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:49:48 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:49:48 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:49:48 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:49:48 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:49:48 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:49:48 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:49:48 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:49:48 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:49:48 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:49:48 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:49:48 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:49:48 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:14<00:00,  3.60it/s]
Epoch:  70%|                     | 14/20 [1:25:44<35:11, 351.97s/it]10/08/2021 15:50:35 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 147/918 [00:34<03:01,  4.26it/s]
10/08/2021 15:50:35 - INFO - trainer -     Num examples = 3258
10/08/2021 15:50:35 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.94it/s]
10/08/2021 15:50:42 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:50:42 - INFO - trainer -     T-F1 = 0.9904632152588556| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 15:50:42 - INFO - trainer -     T-F1(C) = 0.9715909090909091
10/08/2021 15:50:42 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 15:50:42 - INFO - trainer -     T-F1(O) = 0.996203080928618
10/08/2021 15:50:42 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 15:50:42 - INFO - trainer -     T-F1(S) = 0.9907663896583563
10/08/2021 15:50:42 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:50:42 - INFO - trainer -     U-F1(A) = 0.6448087431693988
10/08/2021 15:50:42 - INFO - trainer -     U-F1(E) = 0.7293666026871402
10/08/2021 15:50:42 - INFO - trainer -     U-F1(I) = 0.25
10/08/2021 15:50:42 - INFO - trainer -     U-F1(O) = 0.9614850798056905
10/08/2021 15:50:42 - INFO - trainer -     intent_acc = 0.9287906691221608
10/08/2021 15:50:42 - INFO - trainer -     loss = 0.509169909883948
10/08/2021 15:50:42 - INFO - trainer -     semantic_frame_acc = 0.9106813996316758
10/08/2021 15:50:42 - INFO - trainer -     slot_f1 = 0.9902452619843924
10/08/2021 15:50:42 - INFO - trainer -     slot_precision = 0.988317107093185
10/08/2021 15:50:42 - INFO - trainer -     slot_recall = 0.9921809550404915

10/08/2021 15:50:42 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:50:42 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:50:42 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:50:42 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:50:42 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:50:42 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:50:42 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:50:42 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:50:42 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:50:42 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:50:42 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:50:42 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:50:42 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:50:42 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:50:42 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:50:42 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:50:42 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:51:30 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 347/918 [01:29<02:14,  4.26it/s]
10/08/2021 15:51:30 - INFO - trainer -     Num examples = 3258
10/08/2021 15:51:30 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 15:51:37 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:51:37 - INFO - trainer -     T-F1 = 0.9900558507015393| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 15:51:37 - INFO - trainer -     T-F1(C) = 0.9783549783549784
10/08/2021 15:51:37 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:51:37 - INFO - trainer -     T-F1(O) = 0.9960401410360727
10/08/2021 15:51:37 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 15:51:37 - INFO - trainer -     T-F1(S) = 0.9875976113918237
10/08/2021 15:51:37 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:51:37 - INFO - trainer -     U-F1(A) = 0.6436781609195402
10/08/2021 15:51:37 - INFO - trainer -     U-F1(E) = 0.7286245353159851
10/08/2021 15:51:37 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 15:51:37 - INFO - trainer -     U-F1(O) = 0.9616386044089568
10/08/2021 15:51:37 - INFO - trainer -     intent_acc = 0.929097605893186
10/08/2021 15:51:37 - INFO - trainer -     loss = 0.5316214179437534
10/08/2021 15:51:37 - INFO - trainer -     semantic_frame_acc = 0.9116022099447514
10/08/2021 15:51:37 - INFO - trainer -     slot_f1 = 0.989828619200223
10/08/2021 15:51:37 - INFO - trainer -     slot_precision = 0.9877641824249166
10/08/2021 15:51:37 - INFO - trainer -     slot_recall = 0.9919017034347948

10/08/2021 15:51:37 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:51:37 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:51:37 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:51:37 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:51:37 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:51:37 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:51:37 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:51:37 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:51:37 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:51:37 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:51:37 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:51:37 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:51:37 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:51:37 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:51:37 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:51:37 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:51:37 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:52:24 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 547/918 [02:24<01:28,  4.21it/s]
10/08/2021 15:52:24 - INFO - trainer -     Num examples = 3258
10/08/2021 15:52:24 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:52:32 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:52:32 - INFO - trainer -     T-F1 = 0.9922141783909302| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 15:52:32 - INFO - trainer -     T-F1(C) = 0.9784172661870504
10/08/2021 15:52:32 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 15:52:32 - INFO - trainer -     T-F1(O) = 0.9969114061230019
10/08/2021 15:52:32 - INFO - trainer -     T-F1(P) = 0.9970760233918129
10/08/2021 15:52:32 - INFO - trainer -     T-F1(S) = 0.9930523390458547
10/08/2021 15:52:32 - INFO - trainer -     T-F1(T) = 0.9817158931082982
10/08/2021 15:52:32 - INFO - trainer -     U-F1(A) = 0.6703910614525139
10/08/2021 15:52:32 - INFO - trainer -     U-F1(E) = 0.7467652495378928
10/08/2021 15:52:32 - INFO - trainer -     U-F1(I) = 0.2608695652173913
10/08/2021 15:52:32 - INFO - trainer -     U-F1(O) = 0.962086956521739
10/08/2021 15:52:32 - INFO - trainer -     intent_acc = 0.9312461632903621
10/08/2021 15:52:32 - INFO - trainer -     loss = 0.541858262802456
10/08/2021 15:52:32 - INFO - trainer -     semantic_frame_acc = 0.9165131982811541
10/08/2021 15:52:32 - INFO - trainer -     slot_f1 = 0.9920357691770295
10/08/2021 15:52:32 - INFO - trainer -     slot_precision = 0.9927293064876958
10/08/2021 15:52:32 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 15:52:32 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:52:32 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:52:32 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:52:32 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:52:32 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:52:32 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:52:32 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:52:32 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:52:32 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:52:32 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:52:32 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:52:32 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:52:32 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:52:32 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:52:32 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:52:32 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:52:32 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:53:19 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 747/918 [03:19<00:40,  4.27it/s]
10/08/2021 15:53:19 - INFO - trainer -     Num examples = 3258
10/08/2021 15:53:19 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:53:27 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:53:27 - INFO - trainer -     T-F1 = 0.9905904813855174| 51/51 [00:07<00:00,  6.88it/s]
10/08/2021 15:53:27 - INFO - trainer -     T-F1(C) = 0.9783549783549784
10/08/2021 15:53:27 - INFO - trainer -     T-F1(L) = 0.9793510324483775
10/08/2021 15:53:27 - INFO - trainer -     T-F1(O) = 0.9962587431545844
10/08/2021 15:53:27 - INFO - trainer -     T-F1(P) = 0.9959112149532712
10/08/2021 15:53:27 - INFO - trainer -     T-F1(S) = 0.9912159038372631
10/08/2021 15:53:27 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 15:53:27 - INFO - trainer -     U-F1(A) = 0.6938775510204082
10/08/2021 15:53:27 - INFO - trainer -     U-F1(E) = 0.7540983606557378
10/08/2021 15:53:27 - INFO - trainer -     U-F1(I) = 0.30434782608695654
10/08/2021 15:53:27 - INFO - trainer -     U-F1(O) = 0.9638427947598254
10/08/2021 15:53:27 - INFO - trainer -     intent_acc = 0.9333947206875384
10/08/2021 15:53:27 - INFO - trainer -     loss = 0.5147238888723009
10/08/2021 15:53:27 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/08/2021 15:53:27 - INFO - trainer -     slot_f1 = 0.9903752266703864
10/08/2021 15:53:27 - INFO - trainer -     slot_precision = 0.9894091415830546
10/08/2021 15:53:27 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 15:53:27 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:53:27 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:53:27 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:53:27 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:53:27 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:53:27 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:53:27 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:53:27 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:53:27 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:53:27 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:53:27 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:53:27 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:53:27 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:53:27 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:53:27 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:53:27 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:53:27 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:07<00:00,  3.71it/s]
Epoch:  75%|                  | 15/20 [1:29:52<26:41, 320.37s/it]10/08/2021 15:54:14 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 29/918 [00:06<03:28,  4.27it/s]
10/08/2021 15:54:14 - INFO - trainer -     Num examples = 3258
10/08/2021 15:54:14 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:54:22 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:54:22 - INFO - trainer -     T-F1 = 0.9912639912639912| 51/51 [00:07<00:00,  7.04it/s]
10/08/2021 15:54:22 - INFO - trainer -     T-F1(C) = 0.9754689754689755
10/08/2021 15:54:22 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:54:22 - INFO - trainer -     T-F1(O) = 0.9965311653116531
10/08/2021 15:54:22 - INFO - trainer -     T-F1(P) = 0.9967845659163987
10/08/2021 15:54:22 - INFO - trainer -     T-F1(S) = 0.9912159038372631
10/08/2021 15:54:22 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:54:22 - INFO - trainer -     U-F1(A) = 0.6836734693877551
10/08/2021 15:54:22 - INFO - trainer -     U-F1(E) = 0.7639639639639638
10/08/2021 15:54:22 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 15:54:22 - INFO - trainer -     U-F1(O) = 0.964348130024467
10/08/2021 15:54:22 - INFO - trainer -     intent_acc = 0.9340085942295887
10/08/2021 15:54:22 - INFO - trainer -     loss = 0.5205610462044384
10/08/2021 15:54:22 - INFO - trainer -     semantic_frame_acc = 0.9168201350521793
10/08/2021 15:54:22 - INFO - trainer -     slot_f1 = 0.9910639486177045
10/08/2021 15:54:22 - INFO - trainer -     slot_precision = 0.9910639486177045
10/08/2021 15:54:22 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 15:54:22 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:54:22 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:54:22 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:54:22 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:54:22 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:54:22 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:54:22 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:54:22 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:54:22 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:54:22 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:54:22 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:54:22 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:54:22 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:54:22 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:54:22 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:54:22 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:54:22 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:55:09 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 229/918 [01:01<02:42,  4.24it/s]
10/08/2021 15:55:09 - INFO - trainer -     Num examples = 3258
10/08/2021 15:55:09 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:55:17 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:55:17 - INFO - trainer -     T-F1 = 0.9909959072305594| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 15:55:17 - INFO - trainer -     T-F1(C) = 0.9784172661870504
10/08/2021 15:55:17 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:55:17 - INFO - trainer -     T-F1(O) = 0.9964219885069934
10/08/2021 15:55:17 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 15:55:17 - INFO - trainer -     T-F1(S) = 0.9907578558225508
10/08/2021 15:55:17 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:55:17 - INFO - trainer -     U-F1(A) = 0.7035175879396984
10/08/2021 15:55:17 - INFO - trainer -     U-F1(E) = 0.7527675276752769
10/08/2021 15:55:17 - INFO - trainer -     U-F1(I) = 0.21739130434782608
10/08/2021 15:55:17 - INFO - trainer -     U-F1(O) = 0.9635189387327632
10/08/2021 15:55:17 - INFO - trainer -     intent_acc = 0.932780847145488
10/08/2021 15:55:17 - INFO - trainer -     loss = 0.5377366342673114
10/08/2021 15:55:17 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/08/2021 15:55:17 - INFO - trainer -     slot_f1 = 0.990789840915434
10/08/2021 15:55:17 - INFO - trainer -     slot_precision = 0.9902370990237099
10/08/2021 15:55:17 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 15:55:17 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:55:17 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:55:17 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:55:17 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:55:17 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:55:17 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:55:17 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:55:17 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:55:17 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:55:17 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:55:17 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:55:17 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:55:17 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:55:17 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:55:17 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:55:17 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:55:17 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:56:04 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 429/918 [01:56<01:55,  4.25it/s]
10/08/2021 15:56:04 - INFO - trainer -     Num examples = 3258
10/08/2021 15:56:04 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 15:56:12 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:56:12 - INFO - trainer -     T-F1 = 0.9907281156258523| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 15:56:12 - INFO - trainer -     T-F1(C) = 0.9769452449567723
10/08/2021 15:56:12 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 15:56:12 - INFO - trainer -     T-F1(O) = 0.9963127643422622
10/08/2021 15:56:12 - INFO - trainer -     T-F1(P) = 0.9956178790534619
10/08/2021 15:56:12 - INFO - trainer -     T-F1(S) = 0.9907749077490775
10/08/2021 15:56:12 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 15:56:12 - INFO - trainer -     U-F1(A) = 0.7219512195121951
10/08/2021 15:56:12 - INFO - trainer -     U-F1(E) = 0.756052141527002
10/08/2021 15:56:12 - INFO - trainer -     U-F1(I) = 0.18604651162790697
10/08/2021 15:56:12 - INFO - trainer -     U-F1(O) = 0.9649275868085849
10/08/2021 15:56:12 - INFO - trainer -     intent_acc = 0.9349294045426643
10/08/2021 15:56:12 - INFO - trainer -     loss = 0.5567745528384751
10/08/2021 15:56:12 - INFO - trainer -     semantic_frame_acc = 0.9174340085942296
10/08/2021 15:56:12 - INFO - trainer -     slot_f1 = 0.990516039051604
10/08/2021 15:56:12 - INFO - trainer -     slot_precision = 0.989412092504876
10/08/2021 15:56:12 - INFO - trainer -     slot_recall = 0.991622451829098

10/08/2021 15:56:12 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:56:12 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:56:12 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:56:12 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:56:12 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:56:12 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:56:12 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:56:12 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:56:12 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:56:12 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:56:12 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:56:12 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:56:12 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:56:12 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:56:12 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:56:12 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:56:12 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:56:59 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 629/918 [02:51<01:09,  4.19it/s]
10/08/2021 15:56:59 - INFO - trainer -     Num examples = 3258
10/08/2021 15:56:59 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 15:57:06 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:57:06 - INFO - trainer -     T-F1 = 0.9905956112852664| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 15:57:06 - INFO - trainer -     T-F1(C) = 0.9742857142857144
10/08/2021 15:57:06 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:57:06 - INFO - trainer -     T-F1(O) = 0.9962579315581106
10/08/2021 15:57:06 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/08/2021 15:57:06 - INFO - trainer -     T-F1(S) = 0.9921478060046189
10/08/2021 15:57:06 - INFO - trainer -     T-F1(T) = 0.9803921568627452
10/08/2021 15:57:06 - INFO - trainer -     U-F1(A) = 0.6521739130434782
10/08/2021 15:57:06 - INFO - trainer -     U-F1(E) = 0.758364312267658
10/08/2021 15:57:06 - INFO - trainer -     U-F1(I) = 0.3137254901960785
10/08/2021 15:57:06 - INFO - trainer -     U-F1(O) = 0.9636078704509838
10/08/2021 15:57:06 - INFO - trainer -     intent_acc = 0.932780847145488
10/08/2021 15:57:06 - INFO - trainer -     loss = 0.5481550215798265
10/08/2021 15:57:06 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/08/2021 15:57:06 - INFO - trainer -     slot_f1 = 0.9903805938937683
10/08/2021 15:57:06 - INFO - trainer -     slot_precision = 0.9888641425389755
10/08/2021 15:57:06 - INFO - trainer -     slot_recall = 0.9919017034347948

10/08/2021 15:57:06 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:57:06 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:57:06 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:57:06 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:57:06 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:57:06 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:57:06 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:57:06 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:57:06 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:57:06 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:57:06 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:57:06 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:57:06 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:57:06 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:57:06 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:57:06 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:57:06 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:57:54 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 829/918 [03:46<00:21,  4.23it/s]
10/08/2021 15:57:54 - INFO - trainer -     Num examples = 3258
10/08/2021 15:57:54 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.94it/s]
10/08/2021 15:58:01 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:58:01 - INFO - trainer -     T-F1 = 0.9908632210555026| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 15:58:01 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/08/2021 15:58:01 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:58:01 - INFO - trainer -     T-F1(O) = 0.9963671853819878
10/08/2021 15:58:01 - INFO - trainer -     T-F1(P) = 0.9961999415375621
10/08/2021 15:58:01 - INFO - trainer -     T-F1(S) = 0.9921478060046189
10/08/2021 15:58:01 - INFO - trainer -     T-F1(T) = 0.9817158931082982
10/08/2021 15:58:01 - INFO - trainer -     U-F1(A) = 0.7238095238095239
10/08/2021 15:58:01 - INFO - trainer -     U-F1(E) = 0.7683823529411765
10/08/2021 15:58:01 - INFO - trainer -     U-F1(I) = 0.29787234042553196
10/08/2021 15:58:01 - INFO - trainer -     U-F1(O) = 0.9662292213473317
10/08/2021 15:58:01 - INFO - trainer -     intent_acc = 0.9370779619398404
10/08/2021 15:58:01 - INFO - trainer -     loss = 0.5435300292629822
10/08/2021 15:58:01 - INFO - trainer -     semantic_frame_acc = 0.9186617556783303
10/08/2021 15:58:01 - INFO - trainer -     slot_f1 = 0.9906542056074766
10/08/2021 15:58:01 - INFO - trainer -     slot_precision = 0.9896878483835005
10/08/2021 15:58:01 - INFO - trainer -     slot_recall = 0.991622451829098

10/08/2021 15:58:01 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:58:01 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:58:01 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:58:01 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:58:01 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:58:01 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:58:01 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:58:01 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:58:01 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:58:01 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:58:01 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:58:01 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:58:01 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:58:01 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:58:01 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:58:01 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:58:01 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:14<00:00,  3.60it/s]
Epoch:  80%|              | 16/20 [1:34:07<20:02, 300.68s/it]10/08/2021 15:58:48 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 111/918 [00:26<03:09,  4.25it/s]
10/08/2021 15:58:48 - INFO - trainer -     Num examples = 3258
10/08/2021 15:58:48 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.94it/s]
10/08/2021 15:58:56 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:58:56 - INFO - trainer -     T-F1 = 0.9905879143363797| 51/51 [00:07<00:00,  7.10it/s]
10/08/2021 15:58:56 - INFO - trainer -     T-F1(C) = 0.9741379310344828
10/08/2021 15:58:56 - INFO - trainer -     T-F1(L) = 0.9793510324483775
10/08/2021 15:58:56 - INFO - trainer -     T-F1(O) = 0.9962591488208187
10/08/2021 15:58:56 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 15:58:56 - INFO - trainer -     T-F1(S) = 0.9921405455386038
10/08/2021 15:58:56 - INFO - trainer -     T-F1(T) = 0.9817158931082982
10/08/2021 15:58:56 - INFO - trainer -     U-F1(A) = 0.6907216494845361
10/08/2021 15:58:56 - INFO - trainer -     U-F1(E) = 0.7352380952380954
10/08/2021 15:58:56 - INFO - trainer -     U-F1(I) = 0.30434782608695654
10/08/2021 15:58:56 - INFO - trainer -     U-F1(O) = 0.9626151973569814
10/08/2021 15:58:56 - INFO - trainer -     intent_acc = 0.9315531000613874
10/08/2021 15:58:56 - INFO - trainer -     loss = 0.5765095881971658
10/08/2021 15:58:56 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/08/2021 15:58:56 - INFO - trainer -     slot_f1 = 0.9903725408120552
10/08/2021 15:58:56 - INFO - trainer -     slot_precision = 0.9896820970440602
10/08/2021 15:58:56 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 15:58:56 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:58:56 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:58:56 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:58:56 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:58:56 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:58:56 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:58:56 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:58:56 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:58:56 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:58:56 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:58:56 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:58:56 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:58:56 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:58:56 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:58:56 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:58:56 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:58:56 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 15:59:43 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 311/918 [01:20<02:22,  4.25it/s]
10/08/2021 15:59:43 - INFO - trainer -     Num examples = 3258
10/08/2021 15:59:43 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 15:59:51 - INFO - trainer -   ***** Eval results *****
10/08/2021 15:59:51 - INFO - trainer -     T-F1 = 0.9907281156258523| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 15:59:51 - INFO - trainer -     T-F1(C) = 0.9770114942528736
10/08/2021 15:59:51 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 15:59:51 - INFO - trainer -     T-F1(O) = 0.9963127643422622
10/08/2021 15:59:51 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 15:59:51 - INFO - trainer -     T-F1(S) = 0.9912321181356715
10/08/2021 15:59:51 - INFO - trainer -     T-F1(T) = 0.9817158931082982
10/08/2021 15:59:51 - INFO - trainer -     U-F1(A) = 0.6524064171122995
10/08/2021 15:59:51 - INFO - trainer -     U-F1(E) = 0.7201565557729941
10/08/2021 15:59:51 - INFO - trainer -     U-F1(I) = 0.26666666666666666
10/08/2021 15:59:51 - INFO - trainer -     U-F1(O) = 0.9613719036895895
10/08/2021 15:59:51 - INFO - trainer -     intent_acc = 0.9287906691221608
10/08/2021 15:59:51 - INFO - trainer -     loss = 0.576040203664817
10/08/2021 15:59:51 - INFO - trainer -     semantic_frame_acc = 0.9112952731737262
10/08/2021 15:59:51 - INFO - trainer -     slot_f1 = 0.990516039051604
10/08/2021 15:59:51 - INFO - trainer -     slot_precision = 0.989412092504876
10/08/2021 15:59:51 - INFO - trainer -     slot_recall = 0.991622451829098

10/08/2021 15:59:51 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 15:59:51 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 15:59:51 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 15:59:51 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 15:59:51 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 15:59:51 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 15:59:51 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 15:59:51 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 15:59:51 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 15:59:51 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 15:59:51 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 15:59:51 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 15:59:51 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 15:59:51 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 15:59:51 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 15:59:51 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 15:59:51 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:00:38 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 511/918 [02:15<01:36,  4.21it/s]
10/08/2021 16:00:38 - INFO - trainer -     Num examples = 3258
10/08/2021 16:00:38 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 16:00:46 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:00:46 - INFO - trainer -     T-F1 = 0.9904528096017458| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 16:00:46 - INFO - trainer -     T-F1(C) = 0.972818311874106
10/08/2021 16:00:46 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/08/2021 16:00:46 - INFO - trainer -     T-F1(O) = 0.9962047278247669
10/08/2021 16:00:46 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 16:00:46 - INFO - trainer -     T-F1(S) = 0.9912077741786209
10/08/2021 16:00:46 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:00:46 - INFO - trainer -     U-F1(A) = 0.6859903381642511
10/08/2021 16:00:46 - INFO - trainer -     U-F1(E) = 0.7574626865671642
10/08/2021 16:00:46 - INFO - trainer -     U-F1(I) = 0.2608695652173913
10/08/2021 16:00:46 - INFO - trainer -     U-F1(O) = 0.963855421686747
10/08/2021 16:00:46 - INFO - trainer -     intent_acc = 0.9330877839165131
10/08/2021 16:00:46 - INFO - trainer -     loss = 0.577410661268468
10/08/2021 16:00:46 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/08/2021 16:00:46 - INFO - trainer -     slot_f1 = 0.9902343750000001
10/08/2021 16:00:46 - INFO - trainer -     slot_precision = 0.9894061890158907
10/08/2021 16:00:46 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 16:00:46 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:00:46 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:00:46 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:00:46 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:00:46 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:00:46 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:00:46 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:00:46 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:00:46 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:00:46 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:00:46 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:00:46 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:00:46 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:00:46 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:00:46 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:00:46 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:00:46 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:01:33 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 711/918 [03:10<00:48,  4.25it/s]
10/08/2021 16:01:33 - INFO - trainer -     Num examples = 3258
10/08/2021 16:01:33 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 16:01:41 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:01:41 - INFO - trainer -     T-F1 = 0.9911287020608708| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 16:01:41 - INFO - trainer -     T-F1(C) = 0.976878612716763
10/08/2021 16:01:41 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 16:01:41 - INFO - trainer -     T-F1(O) = 0.9964767738088784
10/08/2021 16:01:41 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 16:01:41 - INFO - trainer -     T-F1(S) = 0.9916820702402956
10/08/2021 16:01:41 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:01:41 - INFO - trainer -     U-F1(A) = 0.7029702970297029
10/08/2021 16:01:41 - INFO - trainer -     U-F1(E) = 0.7647058823529412
10/08/2021 16:01:41 - INFO - trainer -     U-F1(I) = 0.26666666666666666
10/08/2021 16:01:41 - INFO - trainer -     U-F1(O) = 0.9645414847161572
10/08/2021 16:01:41 - INFO - trainer -     intent_acc = 0.9349294045426643
10/08/2021 16:01:41 - INFO - trainer -     loss = 0.5668909995287073
10/08/2021 16:01:41 - INFO - trainer -     semantic_frame_acc = 0.9186617556783303
10/08/2021 16:01:41 - INFO - trainer -     slot_f1 = 0.9909255898366607
10/08/2021 16:01:41 - INFO - trainer -     slot_precision = 0.990787269681742
10/08/2021 16:01:41 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 16:01:41 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:01:41 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:01:41 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:01:41 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:01:41 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:01:41 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:01:41 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:01:41 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:01:41 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:01:41 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:01:41 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:01:41 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:01:41 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:01:41 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:01:41 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:01:41 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:01:41 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:02:28 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 911/918 [04:05<00:01,  4.26it/s]
10/08/2021 16:02:28 - INFO - trainer -     Num examples = 3258
10/08/2021 16:02:28 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 16:02:35 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:02:35 - INFO - trainer -     T-F1 = 0.9915300546448088| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 16:02:35 - INFO - trainer -     T-F1(C) = 0.976878612716763
10/08/2021 16:02:35 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:02:35 - INFO - trainer -     T-F1(O) = 0.996640658864326
10/08/2021 16:02:35 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/08/2021 16:02:35 - INFO - trainer -     T-F1(S) = 0.9921259842519685
10/08/2021 16:02:35 - INFO - trainer -     T-F1(T) = 0.9817158931082982
10/08/2021 16:02:35 - INFO - trainer -     U-F1(A) = 0.6829268292682926
10/08/2021 16:02:35 - INFO - trainer -     U-F1(E) = 0.7244701348747592
10/08/2021 16:02:35 - INFO - trainer -     U-F1(I) = 0.25
10/08/2021 16:02:35 - INFO - trainer -     U-F1(O) = 0.9613509749303621
10/08/2021 16:02:35 - INFO - trainer -     intent_acc = 0.9284837323511357
10/08/2021 16:02:35 - INFO - trainer -     loss = 0.5761966933222378
10/08/2021 16:02:35 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/08/2021 16:02:35 - INFO - trainer -     slot_f1 = 0.9913359418669648
10/08/2021 16:02:35 - INFO - trainer -     slot_precision = 0.9921678321678322
10/08/2021 16:02:35 - INFO - trainer -     slot_recall = 0.9905054454063111

10/08/2021 16:02:35 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:02:35 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:02:35 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:02:35 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:02:35 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:02:35 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:02:35 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:02:35 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:02:35 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:02:35 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:02:35 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:02:35 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:02:35 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:02:35 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:02:35 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:02:35 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:02:35 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:14<00:00,  3.60it/s]
Epoch:  85%|          | 17/20 [1:38:21<14:20, 286.85s/it]10/08/2021 16:03:22 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 193/918 [00:45<02:49,  4.28it/s]
10/08/2021 16:03:22 - INFO - trainer -     Num examples = 3258
10/08/2021 16:03:22 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.97it/s]
10/08/2021 16:03:30 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:03:30 - INFO - trainer -     T-F1 = 0.9911214314984292| 51/51 [00:07<00:00,  7.11it/s]
10/08/2021 16:03:30 - INFO - trainer -     T-F1(C) = 0.9754689754689755
10/08/2021 16:03:30 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:03:30 - INFO - trainer -     T-F1(O) = 0.9964779192630724
10/08/2021 16:03:30 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/08/2021 16:03:30 - INFO - trainer -     T-F1(S) = 0.9921259842519685
10/08/2021 16:03:30 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:03:30 - INFO - trainer -     U-F1(A) = 0.6701030927835052
10/08/2021 16:03:30 - INFO - trainer -     U-F1(E) = 0.7368421052631579
10/08/2021 16:03:30 - INFO - trainer -     U-F1(I) = 0.25531914893617025
10/08/2021 16:03:30 - INFO - trainer -     U-F1(O) = 0.9615183701897962
10/08/2021 16:03:30 - INFO - trainer -     intent_acc = 0.9294045426642111
10/08/2021 16:03:30 - INFO - trainer -     loss = 0.5769316340778389
10/08/2021 16:03:30 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/08/2021 16:03:30 - INFO - trainer -     slot_f1 = 0.9909179823948582
10/08/2021 16:03:30 - INFO - trainer -     slot_precision = 0.9916107382550335
10/08/2021 16:03:30 - INFO - trainer -     slot_recall = 0.9902261938006144

10/08/2021 16:03:30 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:03:30 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:03:30 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:03:30 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:03:30 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:03:30 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:03:30 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:03:30 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:03:30 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:03:30 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:03:30 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:03:30 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:03:30 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:03:30 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:03:30 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:03:30 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:03:30 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:04:17 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 393/918 [01:39<02:03,  4.25it/s]
10/08/2021 16:04:17 - INFO - trainer -     Num examples = 3258
10/08/2021 16:04:17 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.98it/s]
10/08/2021 16:04:25 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:04:25 - INFO - trainer -     T-F1 = 0.990988530857455| 51/51 [00:07<00:00,  7.10it/s]
10/08/2021 16:04:25 - INFO - trainer -     T-F1(C) = 0.9754689754689755
10/08/2021 16:04:25 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 16:04:25 - INFO - trainer -     T-F1(O) = 0.996423151961847
10/08/2021 16:04:25 - INFO - trainer -     T-F1(P) = 0.9964953271028038
10/08/2021 16:04:25 - INFO - trainer -     T-F1(S) = 0.9911914696337505
10/08/2021 16:04:25 - INFO - trainer -     T-F1(T) = 0.9817671809256663
10/08/2021 16:04:25 - INFO - trainer -     U-F1(A) = 0.6836734693877551
10/08/2021 16:04:25 - INFO - trainer -     U-F1(E) = 0.7272727272727273
10/08/2021 16:04:25 - INFO - trainer -     U-F1(I) = 0.25531914893617025
10/08/2021 16:04:25 - INFO - trainer -     U-F1(O) = 0.9615317667536989
10/08/2021 16:04:25 - INFO - trainer -     intent_acc = 0.929097605893186
10/08/2021 16:04:25 - INFO - trainer -     loss = 0.5928433908551347
10/08/2021 16:04:25 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/08/2021 16:04:25 - INFO - trainer -     slot_f1 = 0.990782122905028
10/08/2021 16:04:25 - INFO - trainer -     slot_precision = 0.9910589550153674
10/08/2021 16:04:25 - INFO - trainer -     slot_recall = 0.9905054454063111

10/08/2021 16:04:25 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:04:25 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:04:25 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:04:25 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:04:25 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:04:25 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:04:25 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:04:25 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:04:25 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:04:25 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:04:25 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:04:25 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:04:25 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:04:25 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:04:25 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:04:25 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:04:25 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:05:11 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 593/918 [02:34<01:16,  4.27it/s]
10/08/2021 16:05:11 - INFO - trainer -     Num examples = 3258
10/08/2021 16:05:11 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.98it/s]
10/08/2021 16:05:19 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:05:19 - INFO - trainer -     T-F1 = 0.990983606557377| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 16:05:19 - INFO - trainer -     T-F1(C) = 0.9754689754689755
10/08/2021 16:05:19 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 16:05:19 - INFO - trainer -     T-F1(O) = 0.9964239271781534
10/08/2021 16:05:19 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/08/2021 16:05:19 - INFO - trainer -     T-F1(S) = 0.9916589434661723
10/08/2021 16:05:19 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:05:19 - INFO - trainer -     U-F1(A) = 0.6735751295336788
10/08/2021 16:05:19 - INFO - trainer -     U-F1(E) = 0.7145631067961166
10/08/2021 16:05:19 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/08/2021 16:05:19 - INFO - trainer -     U-F1(O) = 0.961151578217135
10/08/2021 16:05:19 - INFO - trainer -     intent_acc = 0.9284837323511357
10/08/2021 16:05:19 - INFO - trainer -     loss = 0.6250288670553881
10/08/2021 16:05:19 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/08/2021 16:05:19 - INFO - trainer -     slot_f1 = 0.9907769703745111
10/08/2021 16:05:19 - INFO - trainer -     slot_precision = 0.9916083916083916
10/08/2021 16:05:19 - INFO - trainer -     slot_recall = 0.9899469421949176

10/08/2021 16:05:19 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:05:19 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:05:19 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:05:19 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:05:19 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:05:19 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:05:19 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:05:19 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:05:19 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:05:19 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:05:19 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:05:19 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:05:19 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:05:19 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:05:19 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:05:19 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:05:19 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:06:06 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 793/918 [03:29<00:29,  4.23it/s]
10/08/2021 16:06:06 - INFO - trainer -     Num examples = 3258
10/08/2021 16:06:06 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 16:06:14 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:06:14 - INFO - trainer -     T-F1 = 0.9908557390473589| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 16:06:14 - INFO - trainer -     T-F1(C) = 0.9726618705035971
10/08/2021 16:06:14 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/08/2021 16:06:14 - INFO - trainer -     T-F1(O) = 0.9963683668491518
10/08/2021 16:06:14 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 16:06:14 - INFO - trainer -     T-F1(S) = 0.9921332716335031
10/08/2021 16:06:14 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:06:14 - INFO - trainer -     U-F1(A) = 0.6735751295336788
10/08/2021 16:06:14 - INFO - trainer -     U-F1(E) = 0.7386363636363638
10/08/2021 16:06:14 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 16:06:14 - INFO - trainer -     U-F1(O) = 0.9621001390820585
10/08/2021 16:06:14 - INFO - trainer -     intent_acc = 0.9306322897483118
10/08/2021 16:06:14 - INFO - trainer -     loss = 0.5987302593740762
10/08/2021 16:06:14 - INFO - trainer -     semantic_frame_acc = 0.9134438305709024
10/08/2021 16:06:14 - INFO - trainer -     slot_f1 = 0.9906463772162502
10/08/2021 16:06:14 - INFO - trainer -     slot_precision = 0.9905080960357342
10/08/2021 16:06:14 - INFO - trainer -     slot_recall = 0.9907846970120078

10/08/2021 16:06:14 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:06:14 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:06:14 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:06:14 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:06:14 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:06:14 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:06:14 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:06:14 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:06:14 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:06:14 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:06:14 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:06:14 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:06:14 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:06:14 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:06:14 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:06:14 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:06:14 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:06<00:00,  3.73it/s]
Epoch:  90%|       | 18/20 [1:42:27<09:09, 274.62s/it]10/08/2021 16:07:01 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 75/918 [00:17<03:18,  4.26it/s]
10/08/2021 16:07:01 - INFO - trainer -     Num examples = 3258
10/08/2021 16:07:01 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.94it/s]
10/08/2021 16:07:08 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:07:08 - INFO - trainer -     T-F1 = 0.9911262798634813| 51/51 [00:07<00:00,  7.02it/s]
10/08/2021 16:07:08 - INFO - trainer -     T-F1(C) = 0.9754689754689755
10/08/2021 16:07:08 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:07:08 - INFO - trainer -     T-F1(O) = 0.9964771557097176
10/08/2021 16:07:08 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/08/2021 16:07:08 - INFO - trainer -     T-F1(S) = 0.9916743755781684
10/08/2021 16:07:08 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:07:08 - INFO - trainer -     U-F1(A) = 0.6907216494845361
10/08/2021 16:07:08 - INFO - trainer -     U-F1(E) = 0.7348484848484849
10/08/2021 16:07:08 - INFO - trainer -     U-F1(I) = 0.25531914893617025
10/08/2021 16:07:08 - INFO - trainer -     U-F1(O) = 0.9615451539933878
10/08/2021 16:07:08 - INFO - trainer -     intent_acc = 0.9300184162062615
10/08/2021 16:07:08 - INFO - trainer -     loss = 0.5986253376684937
10/08/2021 16:07:08 - INFO - trainer -     semantic_frame_acc = 0.9134438305709024
10/08/2021 16:07:08 - INFO - trainer -     slot_f1 = 0.9909230554391845
10/08/2021 16:07:08 - INFO - trainer -     slot_precision = 0.9910614525139665
10/08/2021 16:07:08 - INFO - trainer -     slot_recall = 0.9907846970120078

10/08/2021 16:07:08 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:07:08 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:07:08 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:07:08 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:07:08 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:07:08 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:07:08 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:07:08 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:07:08 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:07:08 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:07:08 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:07:08 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:07:08 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:07:08 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:07:08 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:07:08 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:07:08 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:07:55 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 275/918 [01:12<02:31,  4.25it/s]
10/08/2021 16:07:55 - INFO - trainer -     Num examples = 3258
10/08/2021 16:07:55 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.91it/s]
10/08/2021 16:08:03 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:08:03 - INFO - trainer -     T-F1 = 0.990990990990991| 51/51 [00:07<00:00,  6.85it/s]
10/08/2021 16:08:03 - INFO - trainer -     T-F1(C) = 0.9726618705035971
10/08/2021 16:08:03 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:08:03 - INFO - trainer -     T-F1(O) = 0.9964227642276423
10/08/2021 16:08:03 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:08:03 - INFO - trainer -     T-F1(S) = 0.9916666666666667
10/08/2021 16:08:03 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:08:03 - INFO - trainer -     U-F1(A) = 0.6938775510204082
10/08/2021 16:08:03 - INFO - trainer -     U-F1(E) = 0.7304015296367112
10/08/2021 16:08:03 - INFO - trainer -     U-F1(I) = 0.24390243902439024
10/08/2021 16:08:03 - INFO - trainer -     U-F1(O) = 0.962821403752606
10/08/2021 16:08:03 - INFO - trainer -     intent_acc = 0.9315531000613874
10/08/2021 16:08:03 - INFO - trainer -     loss = 0.6050566544427591
10/08/2021 16:08:03 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/08/2021 16:08:03 - INFO - trainer -     slot_f1 = 0.9907846970120078
10/08/2021 16:08:03 - INFO - trainer -     slot_precision = 0.9907846970120078
10/08/2021 16:08:03 - INFO - trainer -     slot_recall = 0.9907846970120078

10/08/2021 16:08:03 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:08:03 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:08:03 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:08:03 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:08:03 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:08:03 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:08:03 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:08:03 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:08:03 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:08:03 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:08:03 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:08:03 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:08:03 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:08:03 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:08:03 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:08:03 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:08:03 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:08:50 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 475/918 [02:07<01:44,  4.24it/s]
10/08/2021 16:08:50 - INFO - trainer -     Num examples = 3258
10/08/2021 16:08:50 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.93it/s]
10/08/2021 16:08:58 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:08:58 - INFO - trainer -     T-F1 = 0.990858234411243| 51/51 [00:07<00:00,  7.07it/s]
10/08/2021 16:08:58 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/08/2021 16:08:58 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:08:58 - INFO - trainer -     T-F1(O) = 0.9963679731121592
10/08/2021 16:08:58 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:08:58 - INFO - trainer -     T-F1(S) = 0.9912077741786209
10/08/2021 16:08:58 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:08:58 - INFO - trainer -     U-F1(A) = 0.6804123711340206
10/08/2021 16:08:58 - INFO - trainer -     U-F1(E) = 0.7304015296367112
10/08/2021 16:08:58 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 16:08:58 - INFO - trainer -     U-F1(O) = 0.9617790132036135
10/08/2021 16:08:58 - INFO - trainer -     intent_acc = 0.9300184162062615
10/08/2021 16:08:58 - INFO - trainer -     loss = 0.6116917735221339
10/08/2021 16:08:58 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/08/2021 16:08:58 - INFO - trainer -     slot_f1 = 0.990648988136776
10/08/2021 16:08:58 - INFO - trainer -     slot_precision = 0.990234375
10/08/2021 16:08:58 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 16:08:58 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:08:58 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:08:58 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:08:58 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:08:58 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:08:58 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:08:58 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:08:58 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:08:58 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:08:58 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:08:58 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:08:58 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:08:58 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:08:58 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:08:58 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:08:58 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:08:58 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:09:45 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 675/918 [03:02<00:56,  4.26it/s]
10/08/2021 16:09:45 - INFO - trainer -     Num examples = 3258
10/08/2021 16:09:45 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 16:09:53 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:09:53 - INFO - trainer -     T-F1 = 0.9905904813855174| 51/51 [00:07<00:00,  7.03it/s]
10/08/2021 16:09:53 - INFO - trainer -     T-F1(C) = 0.9713467048710601
10/08/2021 16:09:53 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:09:53 - INFO - trainer -     T-F1(O) = 0.9962587431545844
10/08/2021 16:09:53 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:09:53 - INFO - trainer -     T-F1(S) = 0.9907578558225508
10/08/2021 16:09:53 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:09:53 - INFO - trainer -     U-F1(A) = 0.6965174129353233
10/08/2021 16:09:53 - INFO - trainer -     U-F1(E) = 0.7314285714285715
10/08/2021 16:09:53 - INFO - trainer -     U-F1(I) = 0.26666666666666666
10/08/2021 16:09:53 - INFO - trainer -     U-F1(O) = 0.9622280243690164
10/08/2021 16:09:53 - INFO - trainer -     intent_acc = 0.9306322897483118
10/08/2021 16:09:53 - INFO - trainer -     loss = 0.6058830896428987
10/08/2021 16:09:53 - INFO - trainer -     semantic_frame_acc = 0.9134438305709024
10/08/2021 16:09:53 - INFO - trainer -     slot_f1 = 0.9903752266703864
10/08/2021 16:09:53 - INFO - trainer -     slot_precision = 0.9894091415830546
10/08/2021 16:09:53 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 16:09:53 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:09:53 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:09:53 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:09:53 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:09:53 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:09:53 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:09:53 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:09:53 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:09:53 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:09:53 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:09:53 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:09:53 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:09:53 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:09:53 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:09:53 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:09:53 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:09:53 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:10:40 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 875/918 [03:56<00:10,  4.26it/s]
10/08/2021 16:10:40 - INFO - trainer -     Num examples = 3258
10/08/2021 16:10:40 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.94it/s]
10/08/2021 16:10:48 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:10:48 - INFO - trainer -     T-F1 = 0.9905904813855174| 51/51 [00:07<00:00,  7.05it/s]
10/08/2021 16:10:48 - INFO - trainer -     T-F1(C) = 0.9699570815450644
10/08/2021 16:10:48 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:10:48 - INFO - trainer -     T-F1(O) = 0.9962587431545844
10/08/2021 16:10:48 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:10:48 - INFO - trainer -     T-F1(S) = 0.9912159038372631
10/08/2021 16:10:48 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:10:48 - INFO - trainer -     U-F1(A) = 0.6526315789473685
10/08/2021 16:10:48 - INFO - trainer -     U-F1(E) = 0.7279693486590038
10/08/2021 16:10:48 - INFO - trainer -     U-F1(I) = 0.2608695652173913
10/08/2021 16:10:48 - INFO - trainer -     U-F1(O) = 0.9607502605071206
10/08/2021 16:10:48 - INFO - trainer -     intent_acc = 0.9281767955801105
10/08/2021 16:10:48 - INFO - trainer -     loss = 0.6171513521203807
10/08/2021 16:10:48 - INFO - trainer -     semantic_frame_acc = 0.9106813996316758
10/08/2021 16:10:48 - INFO - trainer -     slot_f1 = 0.9903752266703864
10/08/2021 16:10:48 - INFO - trainer -     slot_precision = 0.9894091415830546
10/08/2021 16:10:48 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 16:10:48 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:10:48 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:10:48 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:10:48 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:10:48 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:10:48 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:10:48 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:10:48 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:10:48 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:10:48 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:10:48 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:10:48 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:10:48 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:10:48 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:10:48 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:10:48 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:10:48 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:14<00:00,  3.60it/s]
Epoch:  95%|   | 19/20 [1:46:42<04:28, 268.62s/it]10/08/2021 16:11:35 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 157/918 [00:36<02:59,  4.23it/s]
10/08/2021 16:11:35 - INFO - trainer -     Num examples = 3258
10/08/2021 16:11:35 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 16:11:42 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:11:42 - INFO - trainer -     T-F1 = 0.9911311229362805| 51/51 [00:07<00:00,  7.11it/s]
10/08/2021 16:11:42 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/08/2021 16:11:42 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:11:42 - INFO - trainer -     T-F1(O) = 0.996476391825229
10/08/2021 16:11:42 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:11:42 - INFO - trainer -     T-F1(S) = 0.9921332716335031
10/08/2021 16:11:42 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:11:42 - INFO - trainer -     U-F1(A) = 0.676923076923077
10/08/2021 16:11:42 - INFO - trainer -     U-F1(E) = 0.7265774378585087
10/08/2021 16:11:42 - INFO - trainer -     U-F1(I) = 0.26666666666666666
10/08/2021 16:11:42 - INFO - trainer -     U-F1(O) = 0.9615852598644186
10/08/2021 16:11:42 - INFO - trainer -     intent_acc = 0.9294045426642111
10/08/2021 16:11:42 - INFO - trainer -     loss = 0.6205721284244575
10/08/2021 16:11:42 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/08/2021 16:11:42 - INFO - trainer -     slot_f1 = 0.9909281228192602
10/08/2021 16:11:42 - INFO - trainer -     slot_precision = 0.9905133928571429
10/08/2021 16:11:42 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 16:11:42 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:11:42 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:11:42 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:11:42 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:11:42 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:11:42 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:11:42 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:11:42 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:11:42 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:11:42 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:11:42 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:11:42 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:11:42 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:11:42 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:11:42 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:11:42 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:11:42 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:12:29 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 357/918 [01:31<02:11,  4.28it/s]
10/08/2021 16:12:29 - INFO - trainer -     Num examples = 3258
10/08/2021 16:12:29 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.94it/s]
10/08/2021 16:12:37 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:12:37 - INFO - trainer -     T-F1 = 0.9911311229362805| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 16:12:37 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/08/2021 16:12:37 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:12:37 - INFO - trainer -     T-F1(O) = 0.996476391825229
10/08/2021 16:12:37 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:12:37 - INFO - trainer -     T-F1(S) = 0.9921332716335031
10/08/2021 16:12:37 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:12:37 - INFO - trainer -     U-F1(A) = 0.6735751295336788
10/08/2021 16:12:37 - INFO - trainer -     U-F1(E) = 0.7290076335877863
10/08/2021 16:12:37 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/08/2021 16:12:37 - INFO - trainer -     U-F1(O) = 0.9614315496872827
10/08/2021 16:12:37 - INFO - trainer -     intent_acc = 0.9294045426642111
10/08/2021 16:12:37 - INFO - trainer -     loss = 0.6209140355680504
10/08/2021 16:12:37 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/08/2021 16:12:37 - INFO - trainer -     slot_f1 = 0.9909281228192602
10/08/2021 16:12:37 - INFO - trainer -     slot_precision = 0.9905133928571429
10/08/2021 16:12:37 - INFO - trainer -     slot_recall = 0.9913432002234013

10/08/2021 16:12:37 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:12:37 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:12:37 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:12:37 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:12:37 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:12:37 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:12:37 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:12:37 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:12:37 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:12:37 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:12:37 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:12:37 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:12:37 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:12:37 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:12:37 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:12:37 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:12:37 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:13:24 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 557/918 [02:26<01:24,  4.26it/s]
10/08/2021 16:13:24 - INFO - trainer -     Num examples = 3258
10/08/2021 16:13:24 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.95it/s]
10/08/2021 16:13:32 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:13:32 - INFO - trainer -     T-F1 = 0.9911287020608708| 51/51 [00:07<00:00,  7.08it/s]
10/08/2021 16:13:32 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/08/2021 16:13:32 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:13:32 - INFO - trainer -     T-F1(O) = 0.9964767738088784
10/08/2021 16:13:32 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:13:32 - INFO - trainer -     T-F1(S) = 0.9921259842519685
10/08/2021 16:13:32 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:13:32 - INFO - trainer -     U-F1(A) = 0.676923076923077
10/08/2021 16:13:32 - INFO - trainer -     U-F1(E) = 0.7314285714285715
10/08/2021 16:13:32 - INFO - trainer -     U-F1(I) = 0.22727272727272727
10/08/2021 16:13:32 - INFO - trainer -     U-F1(O) = 0.9617524339360223
10/08/2021 16:13:32 - INFO - trainer -     intent_acc = 0.9297114794352364
10/08/2021 16:13:32 - INFO - trainer -     loss = 0.6114123077953563
10/08/2021 16:13:32 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/08/2021 16:13:32 - INFO - trainer -     slot_f1 = 0.9909255898366607
10/08/2021 16:13:32 - INFO - trainer -     slot_precision = 0.990787269681742
10/08/2021 16:13:32 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 16:13:32 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:13:32 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:13:32 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:13:32 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:13:32 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:13:32 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:13:32 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:13:32 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:13:32 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:13:32 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:13:32 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:13:32 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:13:32 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:13:32 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:13:32 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:13:32 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:13:32 - INFO - trainer -     slot_recall = 0.9891091873778274
                                                                                                                       10/08/2021 16:14:19 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 757/918 [03:21<00:37,  4.24it/s]
10/08/2021 16:14:19 - INFO - trainer -     Num examples = 3258
10/08/2021 16:14:19 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:07<00:00,  6.96it/s]
10/08/2021 16:14:26 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:14:26 - INFO - trainer -     T-F1 = 0.9911287020608708| 51/51 [00:07<00:00,  7.09it/s]
10/08/2021 16:14:26 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/08/2021 16:14:26 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/08/2021 16:14:26 - INFO - trainer -     T-F1(O) = 0.9964767738088784
10/08/2021 16:14:26 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/08/2021 16:14:26 - INFO - trainer -     T-F1(S) = 0.9921259842519685
10/08/2021 16:14:26 - INFO - trainer -     T-F1(T) = 0.9831460674157303
10/08/2021 16:14:26 - INFO - trainer -     U-F1(A) = 0.676923076923077
10/08/2021 16:14:26 - INFO - trainer -     U-F1(E) = 0.7265774378585087
10/08/2021 16:14:26 - INFO - trainer -     U-F1(I) = 0.22222222222222224
10/08/2021 16:14:26 - INFO - trainer -     U-F1(O) = 0.9612376151573092
10/08/2021 16:14:26 - INFO - trainer -     intent_acc = 0.9287906691221608
10/08/2021 16:14:26 - INFO - trainer -     loss = 0.6194616287362342
10/08/2021 16:14:26 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/08/2021 16:14:26 - INFO - trainer -     slot_f1 = 0.9909255898366607
10/08/2021 16:14:26 - INFO - trainer -     slot_precision = 0.990787269681742
10/08/2021 16:14:26 - INFO - trainer -     slot_recall = 0.9910639486177045

10/08/2021 16:14:26 - INFO - trainer -   ***** Current best eval results based on JSA *****
10/08/2021 16:14:26 - INFO - trainer -     T-F1 = 0.9923329682365826
10/08/2021 16:14:26 - INFO - trainer -     T-F1(C) = 0.9810771470160116
10/08/2021 16:14:26 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/08/2021 16:14:26 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/08/2021 16:14:26 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/08/2021 16:14:26 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/08/2021 16:14:26 - INFO - trainer -     T-F1(T) = 0.9803370786516853
10/08/2021 16:14:26 - INFO - trainer -     U-F1(A) = 0.7657657657657658
10/08/2021 16:14:26 - INFO - trainer -     U-F1(E) = 0.7635726795096321
10/08/2021 16:14:26 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/08/2021 16:14:26 - INFO - trainer -     U-F1(O) = 0.966619817287421
10/08/2021 16:14:26 - INFO - trainer -     intent_acc = 0.937998772252916
10/08/2021 16:14:26 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/08/2021 16:14:26 - INFO - trainer -     slot_f1 = 0.9921568627450981
10/08/2021 16:14:26 - INFO - trainer -     slot_precision = 0.9952233773531891
10/08/2021 16:14:26 - INFO - trainer -     slot_recall = 0.9891091873778274
Iteration: 100%|| 918/918 [04:06<00:00,  3.73it/s]
Epoch: 100%|| 20/20 [1:50:48<00:00, 332.45s/it]
10/08/2021 16:15:04 - INFO - transformers.configuration_utils -   loading configuration file final_low_bert_dg_model\config.json
10/08/2021 16:15:04 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "JointBERT"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "low",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

10/08/2021 16:15:04 - INFO - transformers.modeling_utils -   loading weights file final_low_bert_dg_model\pytorch_model.bin
10/08/2021 16:15:05 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing JointBERT.

10/08/2021 16:15:05 - INFO - transformers.modeling_utils -   All the weights of JointBERT were initialized from the model checkpoint at final_low_bert_dg_model.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use JointBERT for predictions without further training.
10/08/2021 16:15:06 - INFO - trainer -   ***** Model Loaded *****
10/08/2021 16:15:06 - INFO - trainer -   ***** Running evaluation on test dataset *****
10/08/2021 16:15:06 - INFO - trainer -     Num examples = 3628
10/08/2021 16:15:06 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 57/57 [00:08<00:00,  7.01it/s]
10/08/2021 16:15:14 - INFO - trainer -   ***** Eval results *****
10/08/2021 16:15:14 - INFO - trainer -     T-F1 = 0.9894867037724182
10/08/2021 16:15:14 - INFO - trainer -     T-F1(C) = 0.9730496453900709
10/08/2021 16:15:14 - INFO - trainer -     T-F1(L) = 0.9770642201834864
10/08/2021 16:15:14 - INFO - trainer -     T-F1(O) = 0.9958955043700807
10/08/2021 16:15:14 - INFO - trainer -     T-F1(P) = 0.999216505615043
10/08/2021 16:15:14 - INFO - trainer -     T-F1(S) = 0.9877997475809845
10/08/2021 16:15:14 - INFO - trainer -     T-F1(T) = 0.967479674796748
10/08/2021 16:15:14 - INFO - trainer -     U-F1(A) = 0.794392523364486
10/08/2021 16:15:14 - INFO - trainer -     U-F1(E) = 0.7513611615245008
10/08/2021 16:15:14 - INFO - trainer -     U-F1(I) = 0.3181818181818182
10/08/2021 16:15:14 - INFO - trainer -     U-F1(O) = 0.9706840390879478
10/08/2021 16:15:14 - INFO - trainer -     intent_acc = 0.9448732083792724
10/08/2021 16:15:14 - INFO - trainer -     loss = 0.2228722524616802
10/08/2021 16:15:14 - INFO - trainer -     semantic_frame_acc = 0.9258544652701213
10/08/2021 16:15:14 - INFO - trainer -     slot_f1 = 0.989192625556262
10/08/2021 16:15:14 - INFO - trainer -     slot_precision = 0.9910828025477707
10/08/2021 16:15:14 - INFO - trainer -     slot_recall = 0.9873096446700508
