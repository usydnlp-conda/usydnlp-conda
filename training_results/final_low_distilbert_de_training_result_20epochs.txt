 2.45000000e-01  5.47000000e-01  3.00000000e-03 -2.83000000e-01
  3.60000000e-01 -3.53000000e-01  2.21000000e-01  1.00000000e-03
  7.59000000e-01  2.92000000e-01 -8.70000000e-02  3.59000000e-01
  3.78000000e-01 -4.06000000e-01 -8.24000000e-01  4.61000000e-01
  3.93000000e-01 -1.96000000e-01  2.71000000e-01  4.74000000e-01
 -1.39000000e-01  1.02400000e+00 -3.14000000e-01 -3.85000000e-01
  9.20000000e-02 -5.61000000e-01 -3.53000000e-01  1.66000000e-01
 -1.25000000e-01 -1.94000000e-01  1.26900000e+00 -4.43000000e-01
 -9.90000000e-02 -6.96000000e-01  2.33000000e-01  3.49000000e-01
 -4.22000000e-01  3.11000000e-01  7.08000000e-01  1.73000000e-01
  9.06000000e-01  1.50000000e-02 -3.18000000e-01 -5.15000000e-01
 -2.72000000e-01 -6.40000000e-01  1.00000000e-03  3.72000000e-01
  3.37000000e-01 -1.40000000e+00  1.00000000e-02 -3.71000000e-01
 -8.50000000e-02 -6.14000000e-01 -5.00000000e-01  3.39000000e-01
  4.00000000e-02 -3.65000000e-01  7.77000000e-01  4.12000000e-01
  2.02000000e-01  1.77000000e-01 -2.84000000e-01 -3.29000000e-01
  1.36800000e+00 -1.36000000e-01  1.69000000e-01  1.17000000e-01
 -3.10000000e-02  7.47000000e-01 -2.58000000e-01 -1.57000000e-01
  4.62000000e-01 -2.90000000e-02 -5.01000000e-01 -3.54000000e-01
 -3.67000000e-01 -1.99000000e-01 -1.12000000e-01  4.60000000e-02
  5.07000000e-01 -2.26000000e-01  5.60000000e-02  4.70000000e-02
 -4.15000000e-01  4.01000000e-01  1.17000000e-01  4.55000000e-01
  2.12000000e-01  3.45000000e-01  2.29000000e-01  9.82000000e-01
 -5.95000000e-01 -1.83000000e-01 -3.51000000e-01  6.36000000e-01
  5.42000000e-01 -6.65000000e-01  6.40000000e-01 -8.11000000e-01
 -4.48000000e-01  1.69000000e-01  6.06000000e-01  4.08000000e-01
  6.26000000e-01  2.66000000e-01 -3.94000000e-01 -1.25000000e-01
  3.20000000e-01  3.66000000e-01 -3.64000000e-01  2.28000000e-01
 -1.55000000e-01 -4.55000000e-01  4.68000000e-01 -4.77000000e-01
 -3.50000000e-02  4.46000000e-01 -3.73000000e-01 -5.65000000e-01
 -6.03000000e-01  8.38000000e-01  4.60000000e-01 -7.20000000e-02
 -3.48000000e-01 -2.26000000e-01  4.00000000e-03 -2.87000000e-01
  3.30000000e-02 -2.89000000e-01 -3.73000000e-01  6.56000000e-01
  1.60000000e-02 -6.68000000e-01  1.20000000e-01 -8.90000000e-02
 -1.31000000e-01 -2.73000000e-01 -6.60000000e-02 -4.16000000e-01
  6.76000000e-01 -4.67000000e-01  3.31000000e-01  3.06000000e-01
  3.21000000e-01  2.10000000e-01 -2.38000000e-01 -7.86000000e-01
  4.94000000e-01 -2.17000000e-01  3.67000000e-01 -1.26000000e-01
 -2.04000000e-01  1.40000000e-02 -4.50000000e-02  8.61000000e-01
  6.05000000e-01 -5.10000000e-02  2.21000000e-01  1.12000000e-01
 -2.93000000e-01 -6.30000000e-01 -2.69000000e-01  3.92000000e-01
  2.53000000e-01 -2.31000000e-01 -2.82000000e-01  3.83000000e-01
  2.02000000e-01  2.22000000e-01  1.01000000e+00  1.90000000e-01
  8.40000000e-02  7.46000000e-01 -6.37000000e-01 -3.62000000e-01
 -6.04000000e-01 -7.68000000e-01  4.40000000e-02  3.82000000e-01
 -1.57000000e-01  4.67000000e-01 -3.59000000e-01  6.10000000e-01
 -6.84000000e-01 -1.51000000e-01  3.25000000e-01  1.97000000e-01
 -2.41000000e-01  5.70000000e-02  1.84000000e-01 -2.52000000e-01
  2.53000000e-01  5.80000000e-01 -6.70000000e-02 -9.24000000e-01
 -2.92000000e-01 -8.90000000e-02 -6.63000000e-01 -4.14000000e-01
 -4.36000000e-01  1.62000000e-01 -8.04000000e-01  3.83000000e-01
 -2.80000000e-02  3.67000000e-01 -2.05000000e-01 -5.43000000e-01
  1.56000000e-01  9.31000000e-01 -1.31000000e-01  5.95000000e-01
  1.27000000e-01 -5.83000000e-01  6.54000000e-01 -1.48000000e-01
  2.98000000e-01  3.81000000e-01  1.34000000e-01  9.00000000e-02
  1.73000000e-01  2.20000000e-01  1.89000000e-01  4.51000000e-01
 -9.32000000e-01 -4.30000000e-02  3.91000000e-01  2.49000000e-01
  1.90000000e-01 -2.47000000e-01  6.52000000e-01  7.20000000e-02
  1.70000000e-01 -5.82000000e-01  4.41000000e-01 -6.14000000e-01
  6.50000000e-01  2.06000000e-01  3.02000000e-01  1.11000000e-01
  3.31000000e-01 -1.87000000e-01 -1.04000000e-01  1.24000000e-01
 -4.76000000e-01  1.29000000e-01  2.87000000e-01 -3.24000000e-01
 -2.47000000e-01 -3.07000000e-01  4.50000000e-02  1.77000000e-01
 -4.80000000e-02  2.06000000e-01 -2.00000000e-01  3.72000000e-01
 -4.87000000e-01 -1.14000000e-01 -7.35000000e-01  4.00000000e-02
 -1.72000000e-01  4.01000000e-01 -7.11000000e-01  4.90000000e-01
  5.48000000e-01  4.25000000e-01 -4.47000000e-01  6.78000000e-01
  2.74000000e-01 -3.40000000e-02  2.33000000e-01  6.60000000e-02
 -5.23000000e-01  5.98000000e-01 -1.10000000e-02 -5.90000000e-02
 -3.50000000e-02  1.81000000e-01 -9.60000000e-02 -9.00000000e-03
 -4.64000000e-01  1.67000000e-01  1.82000000e-01  4.38000000e-01
  2.89000000e-01  6.26000000e-01  4.08000000e-01 -4.03000000e-01
 -4.44000000e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/09/2021 10:16:39 - INFO - data_loader -   *** Example ***
10/09/2021 10:16:39 - INFO - data_loader -   guid: test-1
10/09/2021 10:16:39 - INFO - data_loader -   tokens: [CLS] how dare we go 0 - 2 feeder ##s bronze kids [SEP]
10/09/2021 10:16:39 - INFO - data_loader -   input_ids: 101 2129 8108 2057 2175 1014 1011 1016 21429 2015 4421 4268 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   intent_label: 3 (id = 3)
10/09/2021 10:16:39 - INFO - data_loader -   slot_labels: 0 4 4 5 6 4 0 0 4 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   ner_embeds: [ 1.46634594e-01 -5.26245758e-02 -9.95037891e-03 -9.02985111e-02
  1.22633725e-01 -1.46261543e-01  2.04309840e-02 -1.69067800e-01
  3.29489075e-02  1.92000000e-01  3.00000000e-02 -5.41000000e-01
 -2.49000000e-01  8.70000000e-02 -4.46000000e-01  3.70000000e-01
  4.68000000e-01 -1.50000000e-02 -3.35000000e-01  8.40000000e-02
  2.48000000e-01 -1.15000000e-01 -2.24000000e-01 -1.04000000e-01
 -2.34000000e-01 -4.87000000e-01  4.34000000e-01 -1.94000000e-01
 -4.70000000e-02  2.05000000e-01 -2.13000000e-01 -3.94000000e-01
  4.73000000e-01 -4.53000000e-01  3.11000000e-01 -1.39000000e-01
  5.00000000e-01 -2.43000000e-01 -6.13000000e-01  6.62000000e-01
  5.02000000e-01 -1.15000000e-01 -6.00000000e-02 -5.62000000e-01
  4.21000000e-01  1.01000000e-01 -4.60000000e-02  4.17000000e-01
 -1.92000000e-01 -4.67000000e-01  4.93000000e-01  4.42000000e-01
  1.30000000e-02 -2.28000000e-01 -2.03000000e-01 -1.41000000e-01
  2.71000000e-01  1.18000000e-01 -3.38000000e-01  3.52000000e-01
  2.73000000e-01 -4.71000000e-01 -5.99000000e-01 -4.63000000e-01
 -2.47000000e-01  4.56000000e-01  1.35000000e-01  9.40000000e-02
  1.00700000e+00  6.66000000e-01 -8.60000000e-02 -1.05000000e-01
 -1.01000000e+00  4.70000000e-02  7.10000000e-02 -6.45000000e-01
 -5.60000000e-02 -6.30000000e-02  1.00000000e-03  2.21000000e-01
  1.97000000e-01  2.00000000e-03  3.97000000e-01  5.38000000e-01
  9.70000000e-02 -1.10000000e-01  6.21000000e-01  3.56000000e-01
  4.54000000e-01  1.09000000e-01 -1.18000000e-01  8.80000000e-02
  1.50000000e-01 -3.39000000e-01 -1.92000000e-01 -4.03000000e-01
 -3.47000000e-01  2.93000000e-01  5.40000000e-02  1.51000000e-01
  3.64000000e-01  1.86000000e-01 -6.30000000e-01 -1.04000000e-01
 -7.13000000e-01  2.08000000e-01  4.06000000e-01 -3.94000000e-01
  6.60000000e-02  2.45000000e-01  4.08000000e-01  1.03300000e+00
  2.93000000e-01  9.50000000e-02 -8.00000000e-03  4.32000000e-01
 -3.22000000e-01 -5.65000000e-01 -9.60000000e-02 -3.08000000e-01
 -5.73000000e-01 -4.07000000e-01  2.75000000e-01  1.48000000e-01
 -1.80000000e-02  7.28000000e-01 -5.44000000e-01 -1.05200000e+00
  1.10000000e-02  5.48000000e-01 -1.49000000e-01  1.25000000e-01
 -3.13000000e-01 -3.50000000e-02  3.75000000e-01 -7.79000000e-01
 -3.37000000e-01  2.20000000e-02 -4.77000000e-01 -6.28000000e-01
  3.62000000e-01  6.60000000e-02  1.41000000e-01 -2.86000000e-01
  2.92000000e-01 -6.90000000e-02  1.10000000e-02 -5.11000000e-01
 -1.43000000e-01 -1.24000000e-01  5.52000000e-01 -2.40000000e-01
 -1.52000000e-01 -6.00000000e-03 -2.08000000e-01  8.30000000e-02
 -1.27000000e-01 -1.99000000e-01 -1.50000000e-01 -1.87000000e-01
 -3.20000000e-02 -3.70000000e-02 -2.48000000e-01 -3.86000000e-01
 -1.28500000e+00  2.65000000e-01 -4.45000000e-01 -1.56000000e-01
  3.11000000e-01  9.00000000e-03 -7.97000000e-01 -2.30000000e-02
 -1.27000000e-01 -5.87000000e-01  3.30000000e-01  4.85000000e-01
  4.66000000e-01 -6.44000000e-01 -3.77000000e-01 -2.55000000e-01
  7.00000000e-03 -2.55000000e-01  6.24000000e-01  2.89000000e-01
 -1.08000000e-01 -3.26000000e-01  3.38000000e-01 -2.09000000e-01
 -3.15000000e-01  6.17000000e-01 -7.30000000e-02 -1.43000000e-01
  3.33000000e-01 -1.95000000e-01  2.34000000e-01  8.35000000e-01
  1.35000000e-01 -2.53000000e-01  3.40000000e-02  5.29000000e-01
 -2.62000000e-01 -9.70000000e-02  2.98000000e-01  5.29000000e-01
  2.40000000e-02 -3.21000000e-01  4.54000000e-01 -4.95000000e-01
  4.05000000e-01 -3.84000000e-01  4.15000000e-01  5.03000000e-01
 -1.39000000e-01 -2.59000000e-01 -1.34000000e-01 -7.32000000e-01
 -2.70000000e-01 -1.26000000e-01  5.95000000e-01  2.06000000e-01
 -2.77000000e-01  6.90000000e-02 -1.01800000e+00 -2.20000000e-02
  2.90000000e-02 -2.99000000e-01  3.42000000e-01 -3.03000000e-01
 -8.00000000e-02  7.42000000e-01 -3.80000000e-02  1.46000000e-01
 -3.71000000e-01 -7.46000000e-01 -4.52000000e-01 -1.20000000e-02
  2.84000000e-01  3.18000000e-01 -7.60000000e-01 -2.80000000e-01
 -1.16000000e-01  6.60000000e-02  4.19000000e-01 -2.28000000e-01
  1.48000000e-01 -5.00000000e-02  3.20000000e-01 -9.70000000e-02
 -4.52000000e-01  7.45000000e-01 -9.40000000e-02  7.33000000e-01
 -4.37000000e-01  4.89000000e-01 -3.09000000e-01  2.21000000e-01
 -3.14000000e-01 -2.21000000e-01 -7.70000000e-01  7.30000000e-02
  5.16000000e-01  7.95000000e-01 -4.28000000e-01  2.95000000e-01
 -5.41000000e-01  5.65000000e-01 -3.18000000e-01 -3.05000000e-01
 -3.64000000e-01 -5.40000000e-02 -9.98000000e-01 -4.05000000e-01
 -3.30000000e-01 -2.52000000e-01 -4.55000000e-01 -1.16000000e-01
  1.40000000e-02  6.80000000e-02 -6.90000000e-02 -8.40000000e-02
  2.22000000e-01  4.01000000e-01 -6.30000000e-02  1.85000000e-01
  6.60000000e-01  1.52000000e-01 -9.00000000e-03  6.19000000e-01
 -3.46000000e-01  7.70000000e-02 -8.50000000e-02  6.60000000e-02
  5.58000000e-01  2.95000000e-01 -2.67000000e-01 -2.70000000e-01
 -7.50000000e-02  1.60000000e-01 -4.59000000e-01 -4.39000000e-01
 -2.71000000e-01  4.49000000e-01  3.45000000e-01 -2.73000000e-01
 -4.57000000e-01  2.84000000e-01 -3.90000000e-02 -2.00000000e-03
  8.70000000e-02] [ 0.31645012  0.23681979  0.02408778 -0.10382108  0.05711119 -0.21169041
 -0.13343655 -0.11978559  0.10702279 -0.17        0.043      -0.414
  0.205      -0.013       0.178      -0.067      -0.053      -0.069
  0.236      -0.246      -0.056      -0.073       0.019      -0.076
  0.14       -0.153       0.018      -0.057       0.018      -0.058
  0.153      -0.061      -0.044      -0.129      -0.096       0.164
 -0.049       0.119       0.098      -0.154      -0.205       0.292
 -0.008      -0.287       0.052       0.017      -0.082      -0.014
  0.03       -0.043       0.164      -0.451       0.157       0.06
 -0.274       0.434      -0.142      -0.055      -0.012      -0.278
 -0.325       0.096      -0.134      -0.003      -0.018      -0.002
 -0.234      -0.177       0.124       0.198       0.155      -0.147
 -0.453       0.069       0.292      -0.132       0.022       0.202
 -0.276      -0.326      -0.078      -0.01        0.464      -0.112
 -0.002      -0.2         0.228      -0.051       0.069      -0.247
 -0.061       0.586       0.122      -0.16        0.138      -0.061
 -0.402      -0.551      -0.065      -0.245       0.135       0.067
 -0.107      -0.327      -0.199      -0.019      -0.108      -0.466
 -0.224       0.017      -0.073       0.048      -0.097       0.035
 -0.483       0.167      -0.133      -0.225      -0.125      -0.182
 -0.144      -0.134      -0.183       0.068      -0.195      -0.26
 -0.202       0.077       0.105      -0.073      -0.55        0.229
  0.249      -0.118       0.128      -0.105       0.125       0.152
 -0.116      -0.053       0.32       -0.316       0.062       0.478
 -0.114       0.21       -0.039       0.122       0.486      -0.185
  0.121       0.24       -0.323       0.201      -0.294       0.117
 -0.228      -0.673      -0.139      -0.207       0.091      -0.541
 -0.013      -0.104      -0.02       -0.045       0.153      -0.388
 -0.211       0.63       -0.272      -0.096      -0.077       0.381
  0.058       0.328      -0.145      -0.065       0.279       0.103
  0.303      -0.595       0.353      -0.048      -0.124      -0.34
 -0.06        0.491       0.449      -0.084       0.083       0.304
  0.313      -0.328      -0.161       0.099       0.109      -0.097
  0.17       -0.297      -0.15        0.024       0.086       0.213
 -0.41        0.535       0.486      -0.095       0.166       0.623
  0.024      -0.044       0.048      -0.231      -0.021      -0.23
  0.043       0.026       0.103       0.2        -0.069      -0.404
  0.096       0.427      -0.349       0.003       0.256       0.118
  0.402       0.069      -0.015      -0.12       -0.078       0.254
 -0.407      -0.127      -0.138       0.284       0.281       0.237
  0.307      -0.022       0.103       0.294       0.015      -0.277
 -0.148      -0.138       0.076       0.121       0.066       0.241
  0.246      -0.001       0.123      -0.126      -0.259      -0.016
  0.092       0.098       0.018      -0.064       0.136      -0.137
 -0.417       0.284      -0.051       0.198       0.086       0.117
 -0.076       0.258       0.407       0.066       0.124      -0.165
 -0.194      -0.412       0.162      -0.573      -0.226      -0.067
  0.002      -0.145       0.183       0.171       0.006       0.211
 -0.154       0.2        -0.036      -0.368      -0.181       0.203
 -0.314      -0.101       0.08       -0.139      -0.664      -0.029
  0.205      -0.275       0.035      -0.141       0.2        -0.011
  0.025       0.139       0.166     ] [ 0.12179235  0.17170192  0.09281767  0.03441127 -0.04047123 -0.25426403
 -0.07613952 -0.24388608  0.03252     0.121       0.388      -0.389
  0.014      -0.189       0.07       -0.078       0.02        0.095
 -0.092      -0.517      -0.198      -0.162       0.268       0.176
  0.106       0.65        0.226      -0.169      -0.072      -0.239
  0.228      -0.06       -0.398      -0.485       0.128      -0.02
 -0.014      -0.066      -0.069       0.266       0.606      -0.01
 -0.297       0.041       0.208       0.27        0.083       0.377
 -0.197      -0.062       0.302      -0.308      -0.196       0.096
  0.241       0.115       0.115      -0.21       -0.054       0.03
  0.014      -0.383      -0.632       0.056       0.273      -0.55
  0.166      -0.014       0.12       -0.099       0.005      -0.216
 -0.039       0.003       0.115       0.004      -0.23        0.156
 -0.235      -0.283       0.161       0.203       0.24        0.415
 -0.056      -0.34        0.061       0.202      -0.017      -0.252
 -0.503      -0.136       0.097      -0.115      -0.366      -0.059
  0.235      -0.321       0.335      -0.421      -0.085      -0.3
 -0.341      -0.188      -0.389      -0.007       0.376       0.006
 -0.366       0.249       0.377       0.222      -0.249       0.016
 -0.602      -0.224       0.266      -0.219       0.143       0.365
  0.34       -0.208      -0.078      -0.112      -0.145       0.128
 -0.361      -0.252       0.125      -0.232       0.17        0.539
 -0.641      -0.075      -0.277       0.186       0.226       0.196
 -0.293      -0.125      -0.427       0.085       0.093      -0.432
 -0.038      -0.175      -0.259       0.073       0.102      -0.256
  0.35        0.128      -0.23        0.062      -0.199       0.2
  0.2        -0.139      -0.003      -0.385       0.176      -0.274
 -0.069       0.07       -0.088      -0.061      -0.087      -0.221
  0.318       0.201       0.013       0.148       0.031       0.24
 -0.211       0.136      -0.154      -0.074      -0.171      -0.015
 -0.094      -0.076       0.315       0.083      -0.105      -0.416
 -0.239      -0.276       0.279       0.459       0.186      -0.159
 -0.335       0.154       0.004      -0.158      -0.024      -0.026
 -0.151      -0.176      -0.261       0.063      -0.032      -0.045
  0.225      -0.295      -0.039       0.049      -0.13       -0.035
  0.219       0.124      -0.507      -0.18        0.234       0.179
 -0.106      -0.27        0.209       0.035      -0.418       0.267
 -0.066       0.147       0.015       0.142      -0.258      -0.337
  0.289       0.742       0.176       0.124       0.236      -0.062
 -0.169       0.157       0.063       0.23        0.24       -0.238
 -0.381      -0.231       0.143      -0.316      -0.185      -0.234
  0.141       0.198       0.199       0.017      -0.59       -0.092
 -0.168      -0.134       0.015      -0.248       0.216      -0.343
  0.001       0.173      -0.097       0.424      -0.268      -0.088
  0.159      -0.181      -0.064      -0.085      -0.104       0.264
 -0.371       0.053      -0.19       -0.275       0.106      -0.027
 -0.027       0.262       0.005      -0.158       0.058       0.148
 -0.231       0.029      -0.374       0.03        0.245       0.026
 -0.033       0.235       0.234      -0.158       0.305       0.389
 -0.178       0.092      -0.005      -0.062      -0.204      -0.331
 -0.12       -0.038       0.56       -0.328       0.226       0.086
  0.172       0.015       0.369     ] [ 3.21291417e-01  3.42758745e-01  2.52079163e-02 -1.70695886e-01
 -2.50976924e-02 -2.46103391e-01 -1.49478525e-01 -6.77057654e-02
  4.77589481e-02 -9.60000000e-02  5.89000000e-01 -5.20000000e-02
  8.40000000e-02  4.00000000e-03 -2.26000000e-01  1.05000000e-01
 -1.68000000e-01 -9.50000000e-02  6.00000000e-02 -8.70000000e-02
  2.67000000e-01 -2.41000000e-01  1.92000000e-01  2.71000000e-01
 -1.30000000e-02 -2.26000000e-01  2.07000000e-01 -3.60000000e-01
  1.92000000e-01 -1.84000000e-01  3.84000000e-01  1.02000000e-01
  1.46000000e-01  2.18000000e-01  1.68000000e-01  1.89000000e-01
 -4.20000000e-02 -2.39000000e-01 -1.50000000e-01  1.81000000e-01
 -6.70000000e-02  2.70000000e-02  6.80000000e-02 -2.40000000e-02
  9.50000000e-02  2.28000000e-01  5.52000000e-01  2.00000000e-01
  3.40000000e-01 -8.50000000e-02  1.02000000e-01 -2.14000000e-01
 -9.60000000e-02  4.20000000e-02  2.21000000e-01 -2.38000000e-01
  3.51000000e-01  2.57000000e-01 -1.32000000e-01  1.13000000e-01
 -1.00000000e-03 -2.63000000e-01 -2.73000000e-01  2.20000000e-01
  1.29000000e-01 -3.03000000e-01  3.32000000e-01 -2.53000000e-01
 -3.50000000e-02  1.52000000e-01  7.90000000e-02  9.50000000e-02
 -4.99000000e-01  6.20000000e-02  1.99000000e-01 -6.02000000e-01
  1.43000000e-01 -2.25000000e-01  3.49000000e-01 -2.27000000e-01
  6.10000000e-02  3.20000000e-02  1.66000000e-01 -1.78000000e-01
  2.00000000e-03 -6.00000000e-02 -1.85000000e-01  1.09000000e-01
 -9.80000000e-02  3.00000000e-01 -1.79000000e-01  3.17000000e-01
 -8.50000000e-02 -2.43000000e-01 -8.80000000e-02  3.30000000e-02
 -9.40000000e-02 -4.46000000e-01  1.60000000e-02 -3.16000000e-01
 -1.00000000e-03  1.82000000e-01  8.60000000e-02 -7.00000000e-02
 -2.87000000e-01  6.30000000e-02 -5.50000000e-02  8.30000000e-02
  2.91000000e-01 -1.31000000e-01 -2.20000000e-02 -2.60000000e-02
  3.15000000e-01 -4.80000000e-02 -2.60000000e-01 -6.00000000e-02
 -2.96000000e-01 -1.28000000e-01 -5.06000000e-01 -1.27000000e-01
 -2.19000000e-01 -4.60000000e-02  4.90000000e-02 -2.29000000e-01
 -1.30000000e-01 -1.32000000e-01 -1.41000000e-01 -2.18000000e-01
 -1.26000000e-01 -1.32000000e-01  7.20000000e-02  5.17000000e-01
  7.20000000e-02  4.10000000e-02  1.24000000e-01 -2.03000000e-01
  4.70000000e-02  2.71000000e-01 -3.15000000e-01 -2.45000000e-01
 -3.14000000e-01 -9.70000000e-02  1.24000000e-01 -1.94000000e-01
 -1.21000000e-01 -4.05000000e-01  2.60000000e-01  5.90000000e-02
  4.00000000e-02 -5.90000000e-02  3.45000000e-01  3.40000000e-02
  1.30000000e-02  9.00000000e-02 -4.88000000e-01  2.14000000e-01
 -1.40000000e-01  2.01000000e-01  1.43000000e-01 -2.61000000e-01
  3.42000000e-01 -1.85000000e-01 -1.90000000e-01 -3.84000000e-01
  7.00000000e-02  1.13000000e-01  3.40000000e-02  4.30000000e-02
 -1.06000000e-01 -1.22000000e-01 -3.10000000e-02 -2.37000000e-01
 -4.90000000e-02 -1.07000000e-01 -6.40000000e-02 -3.29000000e-01
 -3.10000000e-02 -2.23000000e-01  1.41000000e-01  1.99000000e-01
 -3.32000000e-01 -3.08000000e-01  1.79000000e-01  3.50000000e-02
 -2.38000000e-01 -4.25000000e-01 -3.73000000e-01  3.48000000e-01
  1.65000000e-01  3.02000000e-01 -2.32000000e-01  2.07000000e-01
  3.00000000e-02  1.09000000e-01 -1.90000000e-01 -1.52000000e-01
  1.72000000e-01  5.70000000e-02  1.80000000e-02 -1.35000000e-01
  1.31000000e-01 -5.52000000e-01 -1.39000000e-01  6.60000000e-02
  2.91000000e-01  2.57000000e-01 -1.67000000e-01  2.55000000e-01
  2.67000000e-01  2.47000000e-01  3.77000000e-01  3.38000000e-01
 -8.90000000e-02  2.87000000e-01  1.16000000e-01  2.42000000e-01
  1.25000000e-01  1.10000000e-01  1.50000000e-02  5.06000000e-01
 -1.67000000e-01  3.80000000e-02 -3.35000000e-01  1.05000000e+00
 -5.97000000e-01 -4.60000000e-02 -3.27000000e-01 -2.30000000e-02
  1.18000000e-01  1.77000000e-01 -2.50000000e-02  1.52000000e-01
 -4.00000000e-01 -1.30000000e-02 -3.25000000e-01 -9.80000000e-02
  1.70000000e-02  1.19000000e-01  2.20000000e-01  7.00000000e-03
 -1.58000000e-01 -4.01000000e-01 -1.73000000e-01  3.19000000e-01
 -3.41000000e-01  2.40000000e-01  8.70000000e-02 -1.34000000e-01
 -3.15000000e-01 -1.78000000e-01 -5.40000000e-02 -1.09000000e-01
 -8.20000000e-02  2.96000000e-01  1.91000000e-01 -2.40000000e-02
  1.00000000e-03  3.40000000e-01  1.38000000e-01 -1.38000000e-01
 -4.70000000e-02  2.35000000e-01 -4.50000000e-02  3.60000000e-02
 -2.57000000e-01  4.29000000e-01 -1.21000000e-01  1.51000000e-01
  3.08000000e-01  4.53000000e-01 -3.49000000e-01  0.00000000e+00
  3.63000000e-01 -4.00000000e-03  1.97000000e-01  9.90000000e-02
 -1.24000000e-01 -2.77000000e-01  5.60000000e-02 -5.89000000e-01
 -3.17000000e-01  3.98000000e-01  1.98000000e-01  8.70000000e-02
 -3.27000000e-01  3.52000000e-01 -1.46000000e-01  8.70000000e-02
 -3.61000000e-01 -1.22000000e-01  2.10000000e-01 -5.10000000e-02
  3.20000000e-02  2.45000000e-01  1.13000000e-01 -3.12000000e-01
 -1.87000000e-01 -1.66000000e-01 -3.48000000e-01 -1.83000000e-01
  8.20000000e-02  4.39000000e-01  2.62000000e-01 -3.63000000e-01
  3.61000000e-01 -9.00000000e-03 -1.73000000e-01  2.58000000e-01
 -2.74000000e-01] [ 0.2695964   0.19175911  0.07458551 -0.08295608 -0.03780138 -0.22242071
 -0.07209244 -0.09776788  0.08011486  0.304       0.41       -0.053
  0.088      -0.216       0.481       0.262      -0.166      -0.631
 -0.107      -0.468       0.137       0.472      -0.172       0.259
  0.336       0.408      -0.809      -0.465       0.347       0.411
  0.433       0.285      -0.073       0.214      -0.067       0.509
  0.361      -0.192       0.133       0.34       -0.052      -0.153
  0.133       0.3         0.468      -0.435      -0.163       0.662
  0.07       -0.226      -0.1        -0.076       0.09        0.186
  0.315       0.058       0.246      -0.264      -0.108       0.352
 -0.435      -0.365      -0.036      -0.075       0.14       -0.478
 -0.142       0.021      -0.525      -0.015       0.325       0.018
 -0.148      -0.297      -0.319      -0.026       0.138      -0.042
 -0.038      -0.267       0.076       0.096       0.691      -0.389
 -0.223      -0.224      -0.335       0.195       0.173       0.255
 -0.713       0.044      -0.162      -0.081      -0.401      -0.339
  0.493      -0.328       0.212      -0.245      -0.101      -0.308
 -0.319      -0.27        0.117       0.481       0.023       0.604
 -0.199      -0.32       -0.463       0.428      -0.127       0.337
 -0.435      -0.053      -0.52       -0.16       -0.527       0.205
 -0.14       -0.092       0.17        0.458       0.513       0.122
 -0.348       0.166      -0.037       0.62        0.65        0.176
  0.184      -0.18        0.079       0.177       0.046      -0.674
 -0.508       0.116       0.248       0.355       0.093      -0.016
  0.096       0.07        0.342      -0.163       0.121      -0.311
  0.231       0.095      -0.008       0.073       0.195      -0.359
  0.047      -0.38       -0.14       -0.292       0.415      -0.554
 -0.101      -0.688       0.003       0.45       -0.083       0.113
 -0.151       0.561      -0.274      -0.078      -0.22       -0.401
 -0.323       0.008      -0.022      -0.528      -0.232      -0.433
 -0.13       -0.002      -0.765      -0.014      -0.42       -0.372
 -0.302      -0.031       0.033       0.234       0.474      -0.036
  0.138       0.442      -0.16       -0.507      -0.218       0.26
  0.655      -0.184      -0.345      -0.011      -0.122       0.169
 -0.477      -0.361       0.39        0.039      -0.233       0.362
  0.123       0.311       0.126       0.364      -0.07       -0.182
 -0.235      -0.069      -0.357       0.276       0.031       0.158
 -0.516       0.247       0.236      -0.184      -0.033       0.402
  0.438      -0.042      -0.054       0.53       -0.547       0.445
 -0.206       0.029      -0.116       0.432      -0.187      -0.19
 -0.253      -0.808       0.038      -0.101      -0.38       -0.081
  0.17        0.034       0.07        0.392      -0.418       0.037
 -0.281       0.591      -0.837      -0.06       -0.415      -0.231
 -0.238       0.037       0.433      -0.179      -0.398       0.11
  0.422       0.14       -0.383       0.04       -0.256       0.236
 -0.457      -0.217      -0.543      -0.164       0.192      -0.645
 -0.658       0.298      -0.384      -0.198      -0.174      -0.011
 -0.129      -0.027       0.152      -0.03        0.229      -0.475
 -0.468       0.45        0.302       0.024      -0.177       0.359
 -0.36       -0.665      -0.09        0.133      -0.65       -0.512
  0.052       0.075       0.41       -0.015       0.541      -0.075
  0.728       0.156       0.318     ] [ 0.4795565   0.1805498   0.13075891 -0.06237778 -0.10000283 -0.13903876
  0.0653518  -0.31374249  0.1547047   0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ] [ 0.08702344  0.18458235  0.37562189 -0.01208748 -0.44048437 -0.12599725
 -0.00877414 -0.29612905  0.07801971  0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ] [ 0.30658117  0.11490601  0.13285208  0.03179812 -0.0184346  -0.04639965
  0.00652742 -0.12284825  0.022705    0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.        ] [ 0.13725726 -0.08411375  0.13283202 -0.05049944 -0.06270097 -0.03976175
  0.04712682 -0.21629179  0.15849967  0.321      -0.135       0.294
 -0.326      -0.179       0.248      -0.269       0.177      -0.092
  0.103       0.188      -0.768       0.387      -0.157      -0.772
 -0.223       0.118       0.037       0.361      -0.235      -0.227
  0.571       0.605       0.114       0.135      -0.016      -0.161
  0.357      -0.582      -0.142      -0.247      -0.101      -0.55
  0.052      -0.181      -0.048       0.187       0.019       0.257
  0.355      -0.002      -0.43        0.241       0.231      -0.035
  0.493      -0.321      -0.138      -0.223      -0.035       0.605
  0.242       0.438      -0.286      -0.547      -0.075      -0.135
  0.044       0.401       0.25       -0.382       0.039      -0.023
 -0.246       0.695      -0.507      -0.049      -0.275      -0.7
 -0.549       0.258       0.024       0.24       -0.043      -0.009
  0.421       0.214       0.181      -0.073       0.15        0.654
 -0.25       -0.152      -0.234       0.025       0.037       0.102
  0.446      -0.828       0.03       -0.252      -0.032       0.121
 -0.168      -0.791      -0.238       0.266       0.222      -0.608
 -0.142      -0.197      -0.063       0.13       -0.223       0.21
 -0.274      -0.321      -0.052       0.513      -0.25        0.189
  0.138      -0.416      -0.158       0.961       0.132      -0.055
 -0.17       -0.232      -0.398       0.106      -0.074       0.372
  0.446      -0.013       0.099       0.571      -0.064       0.057
  0.424       0.562       0.024      -0.277      -0.019       0.353
 -0.337      -0.406       0.255       0.587       0.122      -0.235
 -0.111      -0.284       0.416      -0.205       0.153       0.704
 -0.491      -0.044      -0.116      -0.082       0.089      -0.072
  0.151      -0.331      -0.244       0.037       0.258      -0.089
 -0.895      -0.016      -0.204       0.644       0.264      -0.337
 -0.253       0.397       0.187      -0.338       0.105       0.321
 -0.484      -0.151       0.259       0.145       0.004      -0.033
 -0.332       0.245       0.101       0.137      -0.677       0.076
  0.688       0.436       0.411      -0.336       0.019      -0.263
  0.204       0.047       0.186       0.042       0.236       0.567
  0.157      -0.018      -0.105      -0.352      -0.174       0.378
  0.126      -0.068       0.246       0.496       0.069      -0.397
 -0.409       0.417      -0.03        0.135       0.065       0.815
  0.005      -0.111      -0.483      -0.071       0.325       0.308
  0.504      -0.141      -0.599       0.146       0.322      -0.67
  0.031      -0.285       0.073       0.003       0.29        0.161
 -0.233      -0.518      -0.396      -0.518      -0.165       0.414
  0.566      -0.639      -0.464       0.911      -0.335       0.898
 -0.204      -0.23       -0.395      -0.466      -0.645       0.79
 -0.337       0.222       0.471       0.593      -0.286       0.18
 -0.478      -0.163      -0.214      -0.139      -0.574       0.177
 -0.497       0.19        0.409      -0.244      -0.568       0.353
 -0.293      -0.139      -0.262      -0.03       -0.612       0.066
 -0.084       0.171      -0.5        -0.201       0.184      -0.269
 -0.376       0.758      -0.086      -0.227      -0.018       0.55
 -0.092      -0.999      -0.315      -0.22        0.608       0.286
 -0.021      -0.021      -0.544       0.055       0.035      -0.076
  0.109      -0.002       0.042     ] [ 0.14539956  0.15284954  0.11732712 -0.00734479 -0.01622459 -0.19360994
  0.05985836 -0.23592828 -0.03114882  0.642      -0.27        0.588
 -0.652      -0.358       0.496      -0.538       0.354      -0.184
  0.206       0.376      -1.536       0.774      -0.314      -1.544
 -0.446       0.236       0.074       0.722      -0.47       -0.454
  1.142       1.21        0.228       0.27       -0.032      -0.322
  0.714      -1.164      -0.284      -0.494      -0.202      -1.1
  0.104      -0.362      -0.096       0.374       0.038       0.514
  0.71       -0.004      -0.86        0.482       0.462      -0.07
  0.986      -0.642      -0.276      -0.446      -0.07        1.21
  0.484       0.876      -0.572      -1.094      -0.15       -0.27
  0.088       0.802       0.5        -0.764       0.078      -0.046
 -0.492       1.39       -1.014      -0.098      -0.55       -1.4
 -1.098       0.516       0.048       0.48       -0.086      -0.018
  0.842       0.428       0.362      -0.146       0.3         1.308
 -0.5        -0.304      -0.468       0.05        0.074       0.204
  0.892      -1.656       0.06       -0.504      -0.064       0.242
 -0.336      -1.582      -0.476       0.532       0.444      -1.216
 -0.284      -0.394      -0.126       0.26       -0.446       0.42
 -0.548      -0.642      -0.104       1.026      -0.5         0.378
  0.276      -0.832      -0.316       1.922       0.264      -0.11
 -0.34       -0.464      -0.796       0.212      -0.148       0.744
  0.892      -0.026       0.198       1.142      -0.128       0.114
  0.848       1.124       0.048      -0.554      -0.038       0.706
 -0.674      -0.812       0.51        1.174       0.244      -0.47
 -0.222      -0.568       0.832      -0.41        0.306       1.408
 -0.982      -0.088      -0.232      -0.164       0.178      -0.144
  0.302      -0.662      -0.488       0.074       0.516      -0.178
 -1.79       -0.032      -0.408       1.288       0.528      -0.674
 -0.506       0.794       0.374      -0.676       0.21        0.642
 -0.968      -0.302       0.518       0.29        0.008      -0.066
 -0.664       0.49        0.202       0.274      -1.354       0.152
  1.376       0.872       0.822      -0.672       0.038      -0.526
  0.408       0.094       0.372       0.084       0.472       1.134
  0.314      -0.036      -0.21       -0.704      -0.348       0.756
  0.252      -0.136       0.492       0.992       0.138      -0.794
 -0.818       0.834      -0.06        0.27        0.13        1.63
  0.01       -0.222      -0.966      -0.142       0.65        0.616
  1.008      -0.282      -1.198       0.292       0.644      -1.34
  0.062      -0.57        0.146       0.006       0.58        0.322
 -0.466      -1.036      -0.792      -1.036      -0.33        0.828
  1.132      -1.278      -0.928       1.822      -0.67        1.796
 -0.408      -0.46       -0.79       -0.932      -1.29        1.58
 -0.674       0.444       0.942       1.186      -0.572       0.36
 -0.956      -0.326      -0.428      -0.278      -1.148       0.354
 -0.994       0.38        0.818      -0.488      -1.136       0.706
 -0.586      -0.278      -0.524      -0.06       -1.224       0.132
 -0.168       0.342      -1.         -0.402       0.368      -0.538
 -0.752       1.516      -0.172      -0.454      -0.036       1.1
 -0.184      -1.998      -0.63       -0.44        1.216       0.572
 -0.042      -0.042      -1.088       0.11        0.07       -0.152
  0.218      -0.004       0.084     ] [ 0.0978981   0.01783093  0.05027778 -0.14408033  0.13288936 -0.14443889
  0.07469472  0.03034516 -0.06471247  0.357      -0.257       0.595
 -0.295       0.09        0.218       0.013       0.138      -0.253
  0.321       0.397       0.336       0.035      -0.465       0.068
 -0.05       -0.07        0.234       0.202       0.052       0.238
 -0.264       0.234       0.351      -0.426       0.023      -0.256
  0.513      -0.106       0.481       0.006       0.491      -0.164
  0.236       0.088      -0.429      -0.149       0.123       0.41
 -0.318      -0.28        0.223       0.336      -0.054       0.535
  0.342       0.173       0.255       0.142      -0.065       0.118
  0.251       0.346      -0.142      -0.349      -0.249      -0.216
  0.047      -0.611      -0.131       0.118       0.419       0.104
  0.571      -0.49        0.024       0.079       0.389       0.047
  0.011      -0.177      -0.185      -0.554      -0.371      -0.196
  0.305       0.434      -0.081       0.256      -0.425      -0.183
  0.113       0.041       0.347      -0.748       0.111      -0.304
 -0.222      -0.377      -0.005       0.348      -0.073      -0.277
 -0.206      -0.21        0.216      -0.498      -0.079       0.061
 -0.443       0.233      -0.079       0.597       0.504       0.276
 -0.062      -0.327      -0.099       0.129       0.053      -0.306
  0.614       0.146      -0.093       0.337      -0.543       0.822
 -0.118       0.422       0.177      -0.216       0.565       0.075
 -0.394       0.179       0.025       0.129       0.302      -0.146
 -0.123      -0.175      -0.507      -0.013       0.302       0.187
 -0.294      -0.168       0.104       0.054       0.022      -0.241
  0.258      -0.066       0.487       0.247       0.076       0.582
 -0.033      -0.281      -0.296      -0.574      -0.311      -0.078
 -0.105      -0.246      -0.301       0.135       0.276       0.151
 -0.373       0.118       0.474       0.055       0.054      -0.223
  0.378       0.374       0.256      -0.144       0.026      -0.027
  0.402      -0.017       0.527       0.208      -0.169      -0.45
  0.304      -0.248       0.532       0.509      -0.373      -0.02
 -0.188      -0.346      -0.296      -0.246       0.066       0.115
  0.21        0.825       0.183      -0.206      -0.087       0.711
 -0.114       0.274      -0.167      -0.108       0.321       0.461
  0.696       0.334       0.161      -0.059      -0.56        0.172
  0.29        0.47       -0.274       0.162      -0.26        0.202
 -0.419       0.211      -0.552      -0.013       0.499       0.374
 -0.359       0.019      -0.085       0.157      -0.073      -0.598
 -0.13        0.43        0.334       0.222       0.268       0.424
  0.039      -0.331      -0.297       0.099      -0.495      -0.106
 -0.148       0.071       0.109       0.388       0.09       -0.307
 -0.218      -0.21       -0.089       0.328       0.186      -0.204
 -0.427      -0.127       0.305      -0.333      -0.414      -0.066
  0.109      -0.095       0.356       0.08       -0.189      -0.107
 -0.491      -0.003       0.203       0.186       0.345      -0.061
  0.208       0.089      -0.104      -0.452      -0.133       0.327
  0.208      -0.193      -0.126       0.315      -0.007       0.235
 -0.652      -0.373       0.264      -0.048       0.301       0.338
  0.341      -0.022       0.097      -0.596      -0.148       0.299
  0.793      -0.506      -0.107       0.018      -0.026      -0.727
 -0.324       0.035       0.416     ] [ 0.41695985  0.47033483  0.13023524 -0.09870124 -0.016159   -0.40092546
 -0.19421062 -0.28585118  0.0988782  -0.217       0.063       0.183
 -0.148      -0.186      -0.39        0.13        0.066      -0.087
 -0.007      -0.375      -0.085      -0.062      -0.314      -0.155
 -0.129       0.379       0.206       0.006       0.096      -0.071
  0.409       0.374       0.099      -0.075       0.17       -0.28
  0.032      -0.186       0.207       0.126      -0.021       0.048
 -0.065       0.175       0.156       0.117       0.102       0.068
  0.126      -0.036       0.375      -0.091      -0.202      -0.153
  0.105      -0.125      -0.072      -0.024      -0.398       0.282
 -0.087      -0.092      -0.125      -0.238       0.066       0.246
 -0.052      -0.091       0.154      -0.243       0.088       0.359
 -0.052       0.069       0.236      -0.107      -0.331      -0.047
 -0.23        0.026       0.024      -0.139       0.022      -0.284
 -0.014      -0.171       0.196       0.132      -0.11       -0.071
 -0.262       0.526       0.338      -0.234      -0.231       0.029
  0.005       0.006       0.032      -0.336      -0.073      -0.168
 -0.532      -0.248      -0.046      -0.132       0.483      -0.141
 -0.517      -0.073       0.272       0.274       0.018       0.083
 -0.183       0.288       0.151      -0.112       0.072       0.277
  0.086       0.053      -0.214       0.477       0.053       0.077
 -0.145      -0.239      -0.341      -0.311       0.09       -0.001
 -0.279      -0.096      -0.01        0.043       0.41        0.007
 -0.25       -0.048      -0.617       0.314       0.117      -0.236
 -0.304      -0.134      -0.168       0.018      -0.39       -0.386
 -0.01       -0.299      -0.307      -0.19        0.188       0.242
 -0.303      -0.152       0.027      -0.605       0.041      -0.102
 -0.193      -0.004      -0.046       0.356       0.254      -0.31
 -0.136       0.365      -0.098       0.061      -0.065       0.176
  0.118      -0.075       0.036      -0.055      -0.035      -0.013
 -0.057      -0.234      -0.163      -0.115       0.089      -0.124
 -0.235      -0.08        0.492       0.071      -0.003       0.2
  0.263       0.107      -0.075       0.142       0.057      -0.282
 -0.054      -0.073       0.112      -0.203      -0.158       0.451
  0.031      -0.094      -0.081       0.183       0.254       0.151
  0.593       0.131       0.077       0.03       -0.194       0.029
  0.249      -0.136       0.059      -0.062       0.116      -0.276
 -0.082       0.463      -0.386      -0.275      -0.123      -0.055
  0.078       0.135       0.194      -0.295       0.189      -0.116
 -0.171      -0.001       0.239       0.384       0.129      -0.
 -0.212       0.098      -0.11        0.27       -0.186      -0.11
  0.082      -0.315       0.011       0.269      -0.169       0.203
 -0.229      -0.025      -0.203      -0.034      -0.145       0.207
 -0.246       0.081       0.063       0.242       0.338      -0.256
  0.056      -0.423      -0.292       0.047      -0.032       0.182
 -0.4         0.443      -0.017       0.035       0.093      -0.29
  0.13       -0.346       0.172       0.033      -0.07       -0.038
 -0.053      -0.459      -0.362       0.248       0.08        0.05
 -0.283       0.036       0.169      -0.2         0.051       0.232
  0.215       0.123       0.047       0.249      -0.292      -0.149
 -0.157       0.224      -0.062       0.033      -0.259       0.048
  0.12        0.349      -0.03      ] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  1.19000000e-01 -3.60000000e-02  4.26000000e-01
  1.67000000e-01 -4.56000000e-01  9.12000000e-01  4.56000000e-01
  4.57000000e-01 -2.29000000e-01 -6.00000000e-01  5.30000000e-02
  2.45000000e-01  5.47000000e-01  3.00000000e-03 -2.83000000e-01
  3.60000000e-01 -3.53000000e-01  2.21000000e-01  1.00000000e-03
  7.59000000e-01  2.92000000e-01 -8.70000000e-02  3.59000000e-01
  3.78000000e-01 -4.06000000e-01 -8.24000000e-01  4.61000000e-01
  3.93000000e-01 -1.96000000e-01  2.71000000e-01  4.74000000e-01
 -1.39000000e-01  1.02400000e+00 -3.14000000e-01 -3.85000000e-01
  9.20000000e-02 -5.61000000e-01 -3.53000000e-01  1.66000000e-01
 -1.25000000e-01 -1.94000000e-01  1.26900000e+00 -4.43000000e-01
 -9.90000000e-02 -6.96000000e-01  2.33000000e-01  3.49000000e-01
 -4.22000000e-01  3.11000000e-01  7.08000000e-01  1.73000000e-01
  9.06000000e-01  1.50000000e-02 -3.18000000e-01 -5.15000000e-01
 -2.72000000e-01 -6.40000000e-01  1.00000000e-03  3.72000000e-01
  3.37000000e-01 -1.40000000e+00  1.00000000e-02 -3.71000000e-01
 -8.50000000e-02 -6.14000000e-01 -5.00000000e-01  3.39000000e-01
  4.00000000e-02 -3.65000000e-01  7.77000000e-01  4.12000000e-01
  2.02000000e-01  1.77000000e-01 -2.84000000e-01 -3.29000000e-01
  1.36800000e+00 -1.36000000e-01  1.69000000e-01  1.17000000e-01
 -3.10000000e-02  7.47000000e-01 -2.58000000e-01 -1.57000000e-01
  4.62000000e-01 -2.90000000e-02 -5.01000000e-01 -3.54000000e-01
 -3.67000000e-01 -1.99000000e-01 -1.12000000e-01  4.60000000e-02
  5.07000000e-01 -2.26000000e-01  5.60000000e-02  4.70000000e-02
 -4.15000000e-01  4.01000000e-01  1.17000000e-01  4.55000000e-01
  2.12000000e-01  3.45000000e-01  2.29000000e-01  9.82000000e-01
 -5.95000000e-01 -1.83000000e-01 -3.51000000e-01  6.36000000e-01
  5.42000000e-01 -6.65000000e-01  6.40000000e-01 -8.11000000e-01
 -4.48000000e-01  1.69000000e-01  6.06000000e-01  4.08000000e-01
  6.26000000e-01  2.66000000e-01 -3.94000000e-01 -1.25000000e-01
  3.20000000e-01  3.66000000e-01 -3.64000000e-01  2.28000000e-01
 -1.55000000e-01 -4.55000000e-01  4.68000000e-01 -4.77000000e-01
 -3.50000000e-02  4.46000000e-01 -3.73000000e-01 -5.65000000e-01
 -6.03000000e-01  8.38000000e-01  4.60000000e-01 -7.20000000e-02
 -3.48000000e-01 -2.26000000e-01  4.00000000e-03 -2.87000000e-01
  3.30000000e-02 -2.89000000e-01 -3.73000000e-01  6.56000000e-01
  1.60000000e-02 -6.68000000e-01  1.20000000e-01 -8.90000000e-02
 -1.31000000e-01 -2.73000000e-01 -6.60000000e-02 -4.16000000e-01
  6.76000000e-01 -4.67000000e-01  3.31000000e-01  3.06000000e-01
  3.21000000e-01  2.10000000e-01 -2.38000000e-01 -7.86000000e-01
  4.94000000e-01 -2.17000000e-01  3.67000000e-01 -1.26000000e-01
 -2.04000000e-01  1.40000000e-02 -4.50000000e-02  8.61000000e-01
  6.05000000e-01 -5.10000000e-02  2.21000000e-01  1.12000000e-01
 -2.93000000e-01 -6.30000000e-01 -2.69000000e-01  3.92000000e-01
  2.53000000e-01 -2.31000000e-01 -2.82000000e-01  3.83000000e-01
  2.02000000e-01  2.22000000e-01  1.01000000e+00  1.90000000e-01
  8.40000000e-02  7.46000000e-01 -6.37000000e-01 -3.62000000e-01
 -6.04000000e-01 -7.68000000e-01  4.40000000e-02  3.82000000e-01
 -1.57000000e-01  4.67000000e-01 -3.59000000e-01  6.10000000e-01
 -6.84000000e-01 -1.51000000e-01  3.25000000e-01  1.97000000e-01
 -2.41000000e-01  5.70000000e-02  1.84000000e-01 -2.52000000e-01
  2.53000000e-01  5.80000000e-01 -6.70000000e-02 -9.24000000e-01
 -2.92000000e-01 -8.90000000e-02 -6.63000000e-01 -4.14000000e-01
 -4.36000000e-01  1.62000000e-01 -8.04000000e-01  3.83000000e-01
 -2.80000000e-02  3.67000000e-01 -2.05000000e-01 -5.43000000e-01
  1.56000000e-01  9.31000000e-01 -1.31000000e-01  5.95000000e-01
  1.27000000e-01 -5.83000000e-01  6.54000000e-01 -1.48000000e-01
  2.98000000e-01  3.81000000e-01  1.34000000e-01  9.00000000e-02
  1.73000000e-01  2.20000000e-01  1.89000000e-01  4.51000000e-01
 -9.32000000e-01 -4.30000000e-02  3.91000000e-01  2.49000000e-01
  1.90000000e-01 -2.47000000e-01  6.52000000e-01  7.20000000e-02
  1.70000000e-01 -5.82000000e-01  4.41000000e-01 -6.14000000e-01
  6.50000000e-01  2.06000000e-01  3.02000000e-01  1.11000000e-01
  3.31000000e-01 -1.87000000e-01 -1.04000000e-01  1.24000000e-01
 -4.76000000e-01  1.29000000e-01  2.87000000e-01 -3.24000000e-01
 -2.47000000e-01 -3.07000000e-01  4.50000000e-02  1.77000000e-01
 -4.80000000e-02  2.06000000e-01 -2.00000000e-01  3.72000000e-01
 -4.87000000e-01 -1.14000000e-01 -7.35000000e-01  4.00000000e-02
 -1.72000000e-01  4.01000000e-01 -7.11000000e-01  4.90000000e-01
  5.48000000e-01  4.25000000e-01 -4.47000000e-01  6.78000000e-01
  2.74000000e-01 -3.40000000e-02  2.33000000e-01  6.60000000e-02
 -5.23000000e-01  5.98000000e-01 -1.10000000e-02 -5.90000000e-02
 -3.50000000e-02  1.81000000e-01 -9.60000000e-02 -9.00000000e-03
 -4.64000000e-01  1.67000000e-01  1.82000000e-01  4.38000000e-01
  2.89000000e-01  6.26000000e-01  4.08000000e-01 -4.03000000e-01
 -4.44000000e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/09/2021 10:16:39 - INFO - data_loader -   *** Example ***
10/09/2021 10:16:39 - INFO - data_loader -   guid: test-2
10/09/2021 10:16:39 - INFO - data_loader -   tokens: [CLS] g ##j re ##tar ##d [SEP]
10/09/2021 10:16:39 - INFO - data_loader -   input_ids: 101 1043 3501 2128 7559 2094 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   intent_label: 2 (id = 2)
10/09/2021 10:16:39 - INFO - data_loader -   slot_labels: 0 6 0 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   ner_embeds: [ 1.46634594e-01 -5.26245758e-02 -9.95037891e-03 -9.02985111e-02
  1.22633725e-01 -1.46261543e-01  2.04309840e-02 -1.69067800e-01
  3.29489075e-02  1.92000000e-01  3.00000000e-02 -5.41000000e-01
 -2.49000000e-01  8.70000000e-02 -4.46000000e-01  3.70000000e-01
  4.68000000e-01 -1.50000000e-02 -3.35000000e-01  8.40000000e-02
  2.48000000e-01 -1.15000000e-01 -2.24000000e-01 -1.04000000e-01
 -2.34000000e-01 -4.87000000e-01  4.34000000e-01 -1.94000000e-01
 -4.70000000e-02  2.05000000e-01 -2.13000000e-01 -3.94000000e-01
  4.73000000e-01 -4.53000000e-01  3.11000000e-01 -1.39000000e-01
  5.00000000e-01 -2.43000000e-01 -6.13000000e-01  6.62000000e-01
  5.02000000e-01 -1.15000000e-01 -6.00000000e-02 -5.62000000e-01
  4.21000000e-01  1.01000000e-01 -4.60000000e-02  4.17000000e-01
 -1.92000000e-01 -4.67000000e-01  4.93000000e-01  4.42000000e-01
  1.30000000e-02 -2.28000000e-01 -2.03000000e-01 -1.41000000e-01
  2.71000000e-01  1.18000000e-01 -3.38000000e-01  3.52000000e-01
  2.73000000e-01 -4.71000000e-01 -5.99000000e-01 -4.63000000e-01
 -2.47000000e-01  4.56000000e-01  1.35000000e-01  9.40000000e-02
  1.00700000e+00  6.66000000e-01 -8.60000000e-02 -1.05000000e-01
 -1.01000000e+00  4.70000000e-02  7.10000000e-02 -6.45000000e-01
 -5.60000000e-02 -6.30000000e-02  1.00000000e-03  2.21000000e-01
  1.97000000e-01  2.00000000e-03  3.97000000e-01  5.38000000e-01
  9.70000000e-02 -1.10000000e-01  6.21000000e-01  3.56000000e-01
  4.54000000e-01  1.09000000e-01 -1.18000000e-01  8.80000000e-02
  1.50000000e-01 -3.39000000e-01 -1.92000000e-01 -4.03000000e-01
 -3.47000000e-01  2.93000000e-01  5.40000000e-02  1.51000000e-01
  3.64000000e-01  1.86000000e-01 -6.30000000e-01 -1.04000000e-01
 -7.13000000e-01  2.08000000e-01  4.06000000e-01 -3.94000000e-01
  6.60000000e-02  2.45000000e-01  4.08000000e-01  1.03300000e+00
  2.93000000e-01  9.50000000e-02 -8.00000000e-03  4.32000000e-01
 -3.22000000e-01 -5.65000000e-01 -9.60000000e-02 -3.08000000e-01
 -5.73000000e-01 -4.07000000e-01  2.75000000e-01  1.48000000e-01
 -1.80000000e-02  7.28000000e-01 -5.44000000e-01 -1.05200000e+00
  1.10000000e-02  5.48000000e-01 -1.49000000e-01  1.25000000e-01
 -3.13000000e-01 -3.50000000e-02  3.75000000e-01 -7.79000000e-01
 -3.37000000e-01  2.20000000e-02 -4.77000000e-01 -6.28000000e-01
  3.62000000e-01  6.60000000e-02  1.41000000e-01 -2.86000000e-01
  2.92000000e-01 -6.90000000e-02  1.10000000e-02 -5.11000000e-01
 -1.43000000e-01 -1.24000000e-01  5.52000000e-01 -2.40000000e-01
 -1.52000000e-01 -6.00000000e-03 -2.08000000e-01  8.30000000e-02
 -1.27000000e-01 -1.99000000e-01 -1.50000000e-01 -1.87000000e-01
 -3.20000000e-02 -3.70000000e-02 -2.48000000e-01 -3.86000000e-01
 -1.28500000e+00  2.65000000e-01 -4.45000000e-01 -1.56000000e-01
  3.11000000e-01  9.00000000e-03 -7.97000000e-01 -2.30000000e-02
 -1.27000000e-01 -5.87000000e-01  3.30000000e-01  4.85000000e-01
  4.66000000e-01 -6.44000000e-01 -3.77000000e-01 -2.55000000e-01
  7.00000000e-03 -2.55000000e-01  6.24000000e-01  2.89000000e-01
 -1.08000000e-01 -3.26000000e-01  3.38000000e-01 -2.09000000e-01
 -3.15000000e-01  6.17000000e-01 -7.30000000e-02 -1.43000000e-01
  3.33000000e-01 -1.95000000e-01  2.34000000e-01  8.35000000e-01
  1.35000000e-01 -2.53000000e-01  3.40000000e-02  5.29000000e-01
 -2.62000000e-01 -9.70000000e-02  2.98000000e-01  5.29000000e-01
  2.40000000e-02 -3.21000000e-01  4.54000000e-01 -4.95000000e-01
  4.05000000e-01 -3.84000000e-01  4.15000000e-01  5.03000000e-01
 -1.39000000e-01 -2.59000000e-01 -1.34000000e-01 -7.32000000e-01
 -2.70000000e-01 -1.26000000e-01  5.95000000e-01  2.06000000e-01
 -2.77000000e-01  6.90000000e-02 -1.01800000e+00 -2.20000000e-02
  2.90000000e-02 -2.99000000e-01  3.42000000e-01 -3.03000000e-01
 -8.00000000e-02  7.42000000e-01 -3.80000000e-02  1.46000000e-01
 -3.71000000e-01 -7.46000000e-01 -4.52000000e-01 -1.20000000e-02
  2.84000000e-01  3.18000000e-01 -7.60000000e-01 -2.80000000e-01
 -1.16000000e-01  6.60000000e-02  4.19000000e-01 -2.28000000e-01
  1.48000000e-01 -5.00000000e-02  3.20000000e-01 -9.70000000e-02
 -4.52000000e-01  7.45000000e-01 -9.40000000e-02  7.33000000e-01
 -4.37000000e-01  4.89000000e-01 -3.09000000e-01  2.21000000e-01
 -3.14000000e-01 -2.21000000e-01 -7.70000000e-01  7.30000000e-02
  5.16000000e-01  7.95000000e-01 -4.28000000e-01  2.95000000e-01
 -5.41000000e-01  5.65000000e-01 -3.18000000e-01 -3.05000000e-01
 -3.64000000e-01 -5.40000000e-02 -9.98000000e-01 -4.05000000e-01
 -3.30000000e-01 -2.52000000e-01 -4.55000000e-01 -1.16000000e-01
  1.40000000e-02  6.80000000e-02 -6.90000000e-02 -8.40000000e-02
  2.22000000e-01  4.01000000e-01 -6.30000000e-02  1.85000000e-01
  6.60000000e-01  1.52000000e-01 -9.00000000e-03  6.19000000e-01
 -3.46000000e-01  7.70000000e-02 -8.50000000e-02  6.60000000e-02
  5.58000000e-01  2.95000000e-01 -2.67000000e-01 -2.70000000e-01
 -7.50000000e-02  1.60000000e-01 -4.59000000e-01 -4.39000000e-01
 -2.71000000e-01  4.49000000e-01  3.45000000e-01 -2.73000000e-01
 -4.57000000e-01  2.84000000e-01 -3.90000000e-02 -2.00000000e-03
  8.70000000e-02] [ 0.11572831  0.17759714  0.09786654  0.00935022 -0.01359184 -0.22684969
  0.01183829 -0.25695458 -0.05139022  0.024       0.072      -0.091
 -0.337      -0.393       0.222      -0.391      -0.447      -0.088
 -0.022       0.012       0.266       0.259      -0.076       0.293
 -0.147      -0.048      -0.079      -0.092      -0.057      -0.082
  0.369       0.376       0.064       0.117      -0.004       0.039
  0.442      -0.318      -0.133      -0.305      -0.093      -0.014
  0.448       0.199       0.01       -0.179      -0.134      -0.31
  0.043      -0.3        -0.089      -0.03        0.413       0.217
 -0.294      -0.217       0.179       0.088      -0.339      -0.124
  0.346       0.183      -0.309       0.201       0.126      -0.205
 -0.268       0.343       0.078      -0.          0.116       0.084
  0.116       0.081      -0.256      -0.093      -0.323      -0.194
 -0.179      -0.379       0.129      -0.125       0.342       0.212
 -0.096       0.017      -0.006      -0.011      -0.104       0.217
  0.331      -0.184       0.443      -0.163       0.095      -0.398
 -0.251      -0.04        0.376      -0.146       0.215       0.035
 -0.009       0.115      -0.272       0.162       0.15       -0.042
 -0.255      -0.188      -0.258       0.041       0.146      -0.253
 -0.279       0.009      -0.19       -0.342       0.046      -0.042
 -0.147       0.059      -0.377       0.247      -0.006       0.038
 -0.243       0.213       0.287      -0.035      -0.008       0.379
 -0.434      -0.001      -0.144      -0.087      -0.181       0.263
  0.021      -0.015      -0.241      -0.092       0.189      -0.005
 -0.256      -0.         -0.064       0.375       0.281       0.015
  0.367      -0.147       0.245      -0.063      -0.257      -0.252
  0.075       0.136      -0.093       0.13        0.108       0.065
 -0.039      -0.172       0.027       0.488       0.527       0.021
  0.11        0.048       0.03        0.132      -0.143      -0.008
  0.07        0.349      -0.139      -0.003       0.005       0.065
 -0.131       0.194       0.181      -0.006      -0.098      -0.109
 -0.126       0.217      -0.018      -0.129       0.048       0.22
  0.359      -0.381      -0.114       0.384       0.334      -0.138
 -0.25       -0.014       0.009      -0.003       0.294       0.058
  0.417      -0.183      -0.166      -0.255      -0.234      -0.005
  0.215       0.322      -0.151      -0.004       0.113      -0.
  0.012       0.256      -0.236      -0.022      -0.017      -0.099
 -0.093      -0.117      -0.605      -0.117      -0.072       0.444
  0.288      -0.002       0.116       0.242      -0.074      -0.235
 -0.024      -0.235       0.181       0.053       0.372       0.073
 -0.01       -0.106       0.283      -0.06       -0.158      -0.098
 -0.118       0.336      -0.441       0.138      -0.185      -0.373
 -0.003       0.103      -0.572      -0.388      -0.166      -0.223
 -0.148       0.068       0.434       0.018      -0.372       0.197
 -0.085      -0.014      -0.048       0.097       0.196      -0.038
 -0.009      -0.164       0.222       0.031       0.061       0.332
  0.033      -0.085      -0.288      -0.183      -0.248       0.113
 -0.109       0.064      -0.556       0.271       0.026       0.144
  0.319      -0.004       0.136      -0.127      -0.106       0.511
  0.204       0.123       0.338       0.066      -0.085      -0.091
  0.041      -0.076      -0.159       0.097       0.442       0.076
  0.123      -0.339       0.191     ] [ 1.70408890e-01  1.44198924e-01  9.75735113e-02 -4.20796610e-02
  9.23748128e-04 -2.41188765e-01  3.76508720e-02 -1.96202636e-01
 -2.92958114e-02  4.80000000e-02  1.44000000e-01 -1.82000000e-01
 -6.74000000e-01 -7.86000000e-01  4.44000000e-01 -7.82000000e-01
 -8.94000000e-01 -1.76000000e-01 -4.40000000e-02  2.40000000e-02
  5.32000000e-01  5.18000000e-01 -1.52000000e-01  5.86000000e-01
 -2.94000000e-01 -9.60000000e-02 -1.58000000e-01 -1.84000000e-01
 -1.14000000e-01 -1.64000000e-01  7.38000000e-01  7.52000000e-01
  1.28000000e-01  2.34000000e-01 -8.00000000e-03  7.80000000e-02
  8.84000000e-01 -6.36000000e-01 -2.66000000e-01 -6.10000000e-01
 -1.86000000e-01 -2.80000000e-02  8.96000000e-01  3.98000000e-01
  2.00000000e-02 -3.58000000e-01 -2.68000000e-01 -6.20000000e-01
  8.60000000e-02 -6.00000000e-01 -1.78000000e-01 -6.00000000e-02
  8.26000000e-01  4.34000000e-01 -5.88000000e-01 -4.34000000e-01
  3.58000000e-01  1.76000000e-01 -6.78000000e-01 -2.48000000e-01
  6.92000000e-01  3.66000000e-01 -6.18000000e-01  4.02000000e-01
  2.52000000e-01 -4.10000000e-01 -5.36000000e-01  6.86000000e-01
  1.56000000e-01 -0.00000000e+00  2.32000000e-01  1.68000000e-01
  2.32000000e-01  1.62000000e-01 -5.12000000e-01 -1.86000000e-01
 -6.46000000e-01 -3.88000000e-01 -3.58000000e-01 -7.58000000e-01
  2.58000000e-01 -2.50000000e-01  6.84000000e-01  4.24000000e-01
 -1.92000000e-01  3.40000000e-02 -1.20000000e-02 -2.20000000e-02
 -2.08000000e-01  4.34000000e-01  6.62000000e-01 -3.68000000e-01
  8.86000000e-01 -3.26000000e-01  1.90000000e-01 -7.96000000e-01
 -5.02000000e-01 -8.00000000e-02  7.52000000e-01 -2.92000000e-01
  4.30000000e-01  7.00000000e-02 -1.80000000e-02  2.30000000e-01
 -5.44000000e-01  3.24000000e-01  3.00000000e-01 -8.40000000e-02
 -5.10000000e-01 -3.76000000e-01 -5.16000000e-01  8.20000000e-02
  2.92000000e-01 -5.06000000e-01 -5.58000000e-01  1.80000000e-02
 -3.80000000e-01 -6.84000000e-01  9.20000000e-02 -8.40000000e-02
 -2.94000000e-01  1.18000000e-01 -7.54000000e-01  4.94000000e-01
 -1.20000000e-02  7.60000000e-02 -4.86000000e-01  4.26000000e-01
  5.74000000e-01 -7.00000000e-02 -1.60000000e-02  7.58000000e-01
 -8.68000000e-01 -2.00000000e-03 -2.88000000e-01 -1.74000000e-01
 -3.62000000e-01  5.26000000e-01  4.20000000e-02 -3.00000000e-02
 -4.82000000e-01 -1.84000000e-01  3.78000000e-01 -1.00000000e-02
 -5.12000000e-01 -0.00000000e+00 -1.28000000e-01  7.50000000e-01
  5.62000000e-01  3.00000000e-02  7.34000000e-01 -2.94000000e-01
  4.90000000e-01 -1.26000000e-01 -5.14000000e-01 -5.04000000e-01
  1.50000000e-01  2.72000000e-01 -1.86000000e-01  2.60000000e-01
  2.16000000e-01  1.30000000e-01 -7.80000000e-02 -3.44000000e-01
  5.40000000e-02  9.76000000e-01  1.05400000e+00  4.20000000e-02
  2.20000000e-01  9.60000000e-02  6.00000000e-02  2.64000000e-01
 -2.86000000e-01 -1.60000000e-02  1.40000000e-01  6.98000000e-01
 -2.78000000e-01 -6.00000000e-03  1.00000000e-02  1.30000000e-01
 -2.62000000e-01  3.88000000e-01  3.62000000e-01 -1.20000000e-02
 -1.96000000e-01 -2.18000000e-01 -2.52000000e-01  4.34000000e-01
 -3.60000000e-02 -2.58000000e-01  9.60000000e-02  4.40000000e-01
  7.18000000e-01 -7.62000000e-01 -2.28000000e-01  7.68000000e-01
  6.68000000e-01 -2.76000000e-01 -5.00000000e-01 -2.80000000e-02
  1.80000000e-02 -6.00000000e-03  5.88000000e-01  1.16000000e-01
  8.34000000e-01 -3.66000000e-01 -3.32000000e-01 -5.10000000e-01
 -4.68000000e-01 -1.00000000e-02  4.30000000e-01  6.44000000e-01
 -3.02000000e-01 -8.00000000e-03  2.26000000e-01 -0.00000000e+00
  2.40000000e-02  5.12000000e-01 -4.72000000e-01 -4.40000000e-02
 -3.40000000e-02 -1.98000000e-01 -1.86000000e-01 -2.34000000e-01
 -1.21000000e+00 -2.34000000e-01 -1.44000000e-01  8.88000000e-01
  5.76000000e-01 -4.00000000e-03  2.32000000e-01  4.84000000e-01
 -1.48000000e-01 -4.70000000e-01 -4.80000000e-02 -4.70000000e-01
  3.62000000e-01  1.06000000e-01  7.44000000e-01  1.46000000e-01
 -2.00000000e-02 -2.12000000e-01  5.66000000e-01 -1.20000000e-01
 -3.16000000e-01 -1.96000000e-01 -2.36000000e-01  6.72000000e-01
 -8.82000000e-01  2.76000000e-01 -3.70000000e-01 -7.46000000e-01
 -6.00000000e-03  2.06000000e-01 -1.14400000e+00 -7.76000000e-01
 -3.32000000e-01 -4.46000000e-01 -2.96000000e-01  1.36000000e-01
  8.68000000e-01  3.60000000e-02 -7.44000000e-01  3.94000000e-01
 -1.70000000e-01 -2.80000000e-02 -9.60000000e-02  1.94000000e-01
  3.92000000e-01 -7.60000000e-02 -1.80000000e-02 -3.28000000e-01
  4.44000000e-01  6.20000000e-02  1.22000000e-01  6.64000000e-01
  6.60000000e-02 -1.70000000e-01 -5.76000000e-01 -3.66000000e-01
 -4.96000000e-01  2.26000000e-01 -2.18000000e-01  1.28000000e-01
 -1.11200000e+00  5.42000000e-01  5.20000000e-02  2.88000000e-01
  6.38000000e-01 -8.00000000e-03  2.72000000e-01 -2.54000000e-01
 -2.12000000e-01  1.02200000e+00  4.08000000e-01  2.46000000e-01
  6.76000000e-01  1.32000000e-01 -1.70000000e-01 -1.82000000e-01
  8.20000000e-02 -1.52000000e-01 -3.18000000e-01  1.94000000e-01
  8.84000000e-01  1.52000000e-01  2.46000000e-01 -6.78000000e-01
  3.82000000e-01] [ 0.16360979  0.1795129   0.16120473 -0.00198991 -0.07130799 -0.20982495
  0.07380834 -0.14492798  0.00642991 -0.218      -0.321      -0.051
  0.072      -0.376       0.016      -0.311      -0.459      -0.2
  0.031      -0.306       0.105       0.125      -0.074      -0.016
 -0.172      -0.167      -0.055      -0.019       0.377       0.032
  0.13        0.013       0.466       0.541      -0.165       0.217
  0.008       0.274      -0.151      -0.076       0.59        0.132
 -0.165      -0.351       0.317      -0.186       0.186      -0.246
  0.492       0.296       0.008      -0.379       0.418      -0.307
  0.188       0.056      -0.104      -0.066       0.031      -0.122
  0.254      -0.136      -0.415      -0.029       0.161      -0.487
 -0.275       0.446       0.028      -0.396       0.166       0.148
 -0.389       0.109       0.473       0.037      -0.287      -0.089
 -0.48       -0.399      -0.349      -0.187      -0.08        0.208
  0.044      -0.513       0.032       0.096       0.037       0.455
  0.189       0.209       0.276       0.006      -0.141       0.079
 -0.079      -0.177       0.18       -1.085      -0.101      -0.316
  0.          0.077       0.084       0.251       0.044       0.063
 -0.062       0.084       0.006      -0.167      -0.455       0.15
  0.022      -0.209      -0.331       0.056      -0.077      -0.026
 -0.578       0.097       0.109      -0.121      -0.363       0.413
 -0.168       0.197      -0.517      -0.138      -0.159       0.3
 -0.438       0.126       0.161      -0.006       0.171      -0.063
 -0.663      -0.288       0.006       0.356       0.429       0.389
  0.107      -0.496       0.069      -0.079       0.304       0.201
  0.181       0.296       0.028      -0.398      -0.078       0.132
 -0.172      -0.025      -0.235      -0.091       0.131       0.193
  0.312       0.11       -0.009      -0.245       0.156       0.175
 -0.48        0.603      -0.05       -0.171      -0.092       0.076
  0.344      -0.592       0.242       0.051       0.126      -0.396
 -0.044       0.604      -0.036       0.09        0.118      -0.083
 -0.239       0.241      -0.04        0.095       0.005       0.009
  0.062      -0.139      -0.135       0.451       0.147       0.147
 -0.093       0.557       0.166      -0.064       0.036      -0.257
  0.342       0.357       0.126       0.145       0.065      -0.047
 -0.083      -0.114      -0.058      -0.068      -0.164      -0.585
 -0.098       0.118       0.047      -0.272      -0.058       0.07
 -0.344       0.598      -0.17       -0.298      -0.387       0.288
  0.406       0.348      -0.004      -0.53        0.159      -0.197
 -0.191      -0.08        0.117       0.192       0.315      -0.25
 -0.349      -0.425       0.129      -0.08        0.037      -0.287
  0.183       0.006      -0.055       0.325      -0.111       0.383
 -0.203      -0.152      -0.303      -0.018      -0.079      -0.082
  0.024      -0.143       0.05        0.045       0.155      -0.265
  0.295       0.124      -0.083       0.31        0.032       0.604
 -0.678      -0.127      -0.351      -0.327      -0.129       0.093
  0.08       -0.196       0.017       0.267      -0.444       0.299
 -0.002      -0.12        0.176      -0.151       0.422      -0.142
 -0.355       0.311       0.099       0.21        0.421       0.024
  0.123      -0.038      -0.198      -0.177      -0.439       0.103
  0.219       0.07       -0.275       0.038      -0.179      -0.035
  0.07       -0.131      -0.07      ] [ 1.63994610e-01  6.05893545e-02  5.11902161e-02 -3.45100947e-02
  1.46517009e-01 -1.98174000e-01  1.87352672e-03 -2.67973125e-01
 -4.77913693e-02 -4.36000000e-01 -6.42000000e-01 -1.02000000e-01
  1.44000000e-01 -7.52000000e-01  3.20000000e-02 -6.22000000e-01
 -9.18000000e-01 -4.00000000e-01  6.20000000e-02 -6.12000000e-01
  2.10000000e-01  2.50000000e-01 -1.48000000e-01 -3.20000000e-02
 -3.44000000e-01 -3.34000000e-01 -1.10000000e-01 -3.80000000e-02
  7.54000000e-01  6.40000000e-02  2.60000000e-01  2.60000000e-02
  9.32000000e-01  1.08200000e+00 -3.30000000e-01  4.34000000e-01
  1.60000000e-02  5.48000000e-01 -3.02000000e-01 -1.52000000e-01
  1.18000000e+00  2.64000000e-01 -3.30000000e-01 -7.02000000e-01
  6.34000000e-01 -3.72000000e-01  3.72000000e-01 -4.92000000e-01
  9.84000000e-01  5.92000000e-01  1.60000000e-02 -7.58000000e-01
  8.36000000e-01 -6.14000000e-01  3.76000000e-01  1.12000000e-01
 -2.08000000e-01 -1.32000000e-01  6.20000000e-02 -2.44000000e-01
  5.08000000e-01 -2.72000000e-01 -8.30000000e-01 -5.80000000e-02
  3.22000000e-01 -9.74000000e-01 -5.50000000e-01  8.92000000e-01
  5.60000000e-02 -7.92000000e-01  3.32000000e-01  2.96000000e-01
 -7.78000000e-01  2.18000000e-01  9.46000000e-01  7.40000000e-02
 -5.74000000e-01 -1.78000000e-01 -9.60000000e-01 -7.98000000e-01
 -6.98000000e-01 -3.74000000e-01 -1.60000000e-01  4.16000000e-01
  8.80000000e-02 -1.02600000e+00  6.40000000e-02  1.92000000e-01
  7.40000000e-02  9.10000000e-01  3.78000000e-01  4.18000000e-01
  5.52000000e-01  1.20000000e-02 -2.82000000e-01  1.58000000e-01
 -1.58000000e-01 -3.54000000e-01  3.60000000e-01 -2.17000000e+00
 -2.02000000e-01 -6.32000000e-01  0.00000000e+00  1.54000000e-01
  1.68000000e-01  5.02000000e-01  8.80000000e-02  1.26000000e-01
 -1.24000000e-01  1.68000000e-01  1.20000000e-02 -3.34000000e-01
 -9.10000000e-01  3.00000000e-01  4.40000000e-02 -4.18000000e-01
 -6.62000000e-01  1.12000000e-01 -1.54000000e-01 -5.20000000e-02
 -1.15600000e+00  1.94000000e-01  2.18000000e-01 -2.42000000e-01
 -7.26000000e-01  8.26000000e-01 -3.36000000e-01  3.94000000e-01
 -1.03400000e+00 -2.76000000e-01 -3.18000000e-01  6.00000000e-01
 -8.76000000e-01  2.52000000e-01  3.22000000e-01 -1.20000000e-02
  3.42000000e-01 -1.26000000e-01 -1.32600000e+00 -5.76000000e-01
  1.20000000e-02  7.12000000e-01  8.58000000e-01  7.78000000e-01
  2.14000000e-01 -9.92000000e-01  1.38000000e-01 -1.58000000e-01
  6.08000000e-01  4.02000000e-01  3.62000000e-01  5.92000000e-01
  5.60000000e-02 -7.96000000e-01 -1.56000000e-01  2.64000000e-01
 -3.44000000e-01 -5.00000000e-02 -4.70000000e-01 -1.82000000e-01
  2.62000000e-01  3.86000000e-01  6.24000000e-01  2.20000000e-01
 -1.80000000e-02 -4.90000000e-01  3.12000000e-01  3.50000000e-01
 -9.60000000e-01  1.20600000e+00 -1.00000000e-01 -3.42000000e-01
 -1.84000000e-01  1.52000000e-01  6.88000000e-01 -1.18400000e+00
  4.84000000e-01  1.02000000e-01  2.52000000e-01 -7.92000000e-01
 -8.80000000e-02  1.20800000e+00 -7.20000000e-02  1.80000000e-01
  2.36000000e-01 -1.66000000e-01 -4.78000000e-01  4.82000000e-01
 -8.00000000e-02  1.90000000e-01  1.00000000e-02  1.80000000e-02
  1.24000000e-01 -2.78000000e-01 -2.70000000e-01  9.02000000e-01
  2.94000000e-01  2.94000000e-01 -1.86000000e-01  1.11400000e+00
  3.32000000e-01 -1.28000000e-01  7.20000000e-02 -5.14000000e-01
  6.84000000e-01  7.14000000e-01  2.52000000e-01  2.90000000e-01
  1.30000000e-01 -9.40000000e-02 -1.66000000e-01 -2.28000000e-01
 -1.16000000e-01 -1.36000000e-01 -3.28000000e-01 -1.17000000e+00
 -1.96000000e-01  2.36000000e-01  9.40000000e-02 -5.44000000e-01
 -1.16000000e-01  1.40000000e-01 -6.88000000e-01  1.19600000e+00
 -3.40000000e-01 -5.96000000e-01 -7.74000000e-01  5.76000000e-01
  8.12000000e-01  6.96000000e-01 -8.00000000e-03 -1.06000000e+00
  3.18000000e-01 -3.94000000e-01 -3.82000000e-01 -1.60000000e-01
  2.34000000e-01  3.84000000e-01  6.30000000e-01 -5.00000000e-01
 -6.98000000e-01 -8.50000000e-01  2.58000000e-01 -1.60000000e-01
  7.40000000e-02 -5.74000000e-01  3.66000000e-01  1.20000000e-02
 -1.10000000e-01  6.50000000e-01 -2.22000000e-01  7.66000000e-01
 -4.06000000e-01 -3.04000000e-01 -6.06000000e-01 -3.60000000e-02
 -1.58000000e-01 -1.64000000e-01  4.80000000e-02 -2.86000000e-01
  1.00000000e-01  9.00000000e-02  3.10000000e-01 -5.30000000e-01
  5.90000000e-01  2.48000000e-01 -1.66000000e-01  6.20000000e-01
  6.40000000e-02  1.20800000e+00 -1.35600000e+00 -2.54000000e-01
 -7.02000000e-01 -6.54000000e-01 -2.58000000e-01  1.86000000e-01
  1.60000000e-01 -3.92000000e-01  3.40000000e-02  5.34000000e-01
 -8.88000000e-01  5.98000000e-01 -4.00000000e-03 -2.40000000e-01
  3.52000000e-01 -3.02000000e-01  8.44000000e-01 -2.84000000e-01
 -7.10000000e-01  6.22000000e-01  1.98000000e-01  4.20000000e-01
  8.42000000e-01  4.80000000e-02  2.46000000e-01 -7.60000000e-02
 -3.96000000e-01 -3.54000000e-01 -8.78000000e-01  2.06000000e-01
  4.38000000e-01  1.40000000e-01 -5.50000000e-01  7.60000000e-02
 -3.58000000e-01 -7.00000000e-02  1.40000000e-01 -2.62000000e-01
 -1.40000000e-01] [ 0.22164738  0.16209549  0.16796629  0.04259393  0.00421151 -0.10542212
  0.00639763 -0.21556678  0.10225053 -0.654      -0.963      -0.153
  0.216      -1.128       0.048      -0.933      -1.377      -0.6
  0.093      -0.918       0.315       0.375      -0.222      -0.048
 -0.516      -0.501      -0.165      -0.057       1.131       0.096
  0.39        0.039       1.398       1.623      -0.495       0.651
  0.024       0.822      -0.453      -0.228       1.77        0.396
 -0.495      -1.053       0.951      -0.558       0.558      -0.738
  1.476       0.888       0.024      -1.137       1.254      -0.921
  0.564       0.168      -0.312      -0.198       0.093      -0.366
  0.762      -0.408      -1.245      -0.087       0.483      -1.461
 -0.825       1.338       0.084      -1.188       0.498       0.444
 -1.167       0.327       1.419       0.111      -0.861      -0.267
 -1.44       -1.197      -1.047      -0.561      -0.24        0.624
  0.132      -1.539       0.096       0.288       0.111       1.365
  0.567       0.627       0.828       0.018      -0.423       0.237
 -0.237      -0.531       0.54       -3.255      -0.303      -0.948
  0.          0.231       0.252       0.753       0.132       0.189
 -0.186       0.252       0.018      -0.501      -1.365       0.45
  0.066      -0.627      -0.993       0.168      -0.231      -0.078
 -1.734       0.291       0.327      -0.363      -1.089       1.239
 -0.504       0.591      -1.551      -0.414      -0.477       0.9
 -1.314       0.378       0.483      -0.018       0.513      -0.189
 -1.989      -0.864       0.018       1.068       1.287       1.167
  0.321      -1.488       0.207      -0.237       0.912       0.603
  0.543       0.888       0.084      -1.194      -0.234       0.396
 -0.516      -0.075      -0.705      -0.273       0.393       0.579
  0.936       0.33       -0.027      -0.735       0.468       0.525
 -1.44        1.809      -0.15       -0.513      -0.276       0.228
  1.032      -1.776       0.726       0.153       0.378      -1.188
 -0.132       1.812      -0.108       0.27        0.354      -0.249
 -0.717       0.723      -0.12        0.285       0.015       0.027
  0.186      -0.417      -0.405       1.353       0.441       0.441
 -0.279       1.671       0.498      -0.192       0.108      -0.771
  1.026       1.071       0.378       0.435       0.195      -0.141
 -0.249      -0.342      -0.174      -0.204      -0.492      -1.755
 -0.294       0.354       0.141      -0.816      -0.174       0.21
 -1.032       1.794      -0.51       -0.894      -1.161       0.864
  1.218       1.044      -0.012      -1.59        0.477      -0.591
 -0.573      -0.24        0.351       0.576       0.945      -0.75
 -1.047      -1.275       0.387      -0.24        0.111      -0.861
  0.549       0.018      -0.165       0.975      -0.333       1.149
 -0.609      -0.456      -0.909      -0.054      -0.237      -0.246
  0.072      -0.429       0.15        0.135       0.465      -0.795
  0.885       0.372      -0.249       0.93        0.096       1.812
 -2.034      -0.381      -1.053      -0.981      -0.387       0.279
  0.24       -0.588       0.051       0.801      -1.332       0.897
 -0.006      -0.36        0.528      -0.453       1.266      -0.426
 -1.065       0.933       0.297       0.63        1.263       0.072
  0.369      -0.114      -0.594      -0.531      -1.317       0.309
  0.657       0.21       -0.825       0.114      -0.537      -0.105
  0.21       -0.393      -0.21      ] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  1.19000000e-01 -3.60000000e-02  4.26000000e-01
  1.67000000e-01 -4.56000000e-01  9.12000000e-01  4.56000000e-01
  4.57000000e-01 -2.29000000e-01 -6.00000000e-01  5.30000000e-02
  2.45000000e-01  5.47000000e-01  3.00000000e-03 -2.83000000e-01
  3.60000000e-01 -3.53000000e-01  2.21000000e-01  1.00000000e-03
  7.59000000e-01  2.92000000e-01 -8.70000000e-02  3.59000000e-01
  3.78000000e-01 -4.06000000e-01 -8.24000000e-01  4.61000000e-01
  3.93000000e-01 -1.96000000e-01  2.71000000e-01  4.74000000e-01
 -1.39000000e-01  1.02400000e+00 -3.14000000e-01 -3.85000000e-01
  9.20000000e-02 -5.61000000e-01 -3.53000000e-01  1.66000000e-01
 -1.25000000e-01 -1.94000000e-01  1.26900000e+00 -4.43000000e-01
 -9.90000000e-02 -6.96000000e-01  2.33000000e-01  3.49000000e-01
 -4.22000000e-01  3.11000000e-01  7.08000000e-01  1.73000000e-01
  9.06000000e-01  1.50000000e-02 -3.18000000e-01 -5.15000000e-01
 -2.72000000e-01 -6.40000000e-01  1.00000000e-03  3.72000000e-01
  3.37000000e-01 -1.40000000e+00  1.00000000e-02 -3.71000000e-01
 -8.50000000e-02 -6.14000000e-01 -5.00000000e-01  3.39000000e-01
  4.00000000e-02 -3.65000000e-01  7.77000000e-01  4.12000000e-01
  2.02000000e-01  1.77000000e-01 -2.84000000e-01 -3.29000000e-01
  1.36800000e+00 -1.36000000e-01  1.69000000e-01  1.17000000e-01
 -3.10000000e-02  7.47000000e-01 -2.58000000e-01 -1.57000000e-01
  4.62000000e-01 -2.90000000e-02 -5.01000000e-01 -3.54000000e-01
 -3.67000000e-01 -1.99000000e-01 -1.12000000e-01  4.60000000e-02
  5.07000000e-01 -2.26000000e-01  5.60000000e-02  4.70000000e-02
 -4.15000000e-01  4.01000000e-01  1.17000000e-01  4.55000000e-01
  2.12000000e-01  3.45000000e-01  2.29000000e-01  9.82000000e-01
 -5.95000000e-01 -1.83000000e-01 -3.51000000e-01  6.36000000e-01
  5.42000000e-01 -6.65000000e-01  6.40000000e-01 -8.11000000e-01
 -4.48000000e-01  1.69000000e-01  6.06000000e-01  4.08000000e-01
  6.26000000e-01  2.66000000e-01 -3.94000000e-01 -1.25000000e-01
  3.20000000e-01  3.66000000e-01 -3.64000000e-01  2.28000000e-01
 -1.55000000e-01 -4.55000000e-01  4.68000000e-01 -4.77000000e-01
 -3.50000000e-02  4.46000000e-01 -3.73000000e-01 -5.65000000e-01
 -6.03000000e-01  8.38000000e-01  4.60000000e-01 -7.20000000e-02
 -3.48000000e-01 -2.26000000e-01  4.00000000e-03 -2.87000000e-01
  3.30000000e-02 -2.89000000e-01 -3.73000000e-01  6.56000000e-01
  1.60000000e-02 -6.68000000e-01  1.20000000e-01 -8.90000000e-02
 -1.31000000e-01 -2.73000000e-01 -6.60000000e-02 -4.16000000e-01
  6.76000000e-01 -4.67000000e-01  3.31000000e-01  3.06000000e-01
  3.21000000e-01  2.10000000e-01 -2.38000000e-01 -7.86000000e-01
  4.94000000e-01 -2.17000000e-01  3.67000000e-01 -1.26000000e-01
 -2.04000000e-01  1.40000000e-02 -4.50000000e-02  8.61000000e-01
  6.05000000e-01 -5.10000000e-02  2.21000000e-01  1.12000000e-01
 -2.93000000e-01 -6.30000000e-01 -2.69000000e-01  3.92000000e-01
  2.53000000e-01 -2.31000000e-01 -2.82000000e-01  3.83000000e-01
  2.02000000e-01  2.22000000e-01  1.01000000e+00  1.90000000e-01
  8.40000000e-02  7.46000000e-01 -6.37000000e-01 -3.62000000e-01
 -6.04000000e-01 -7.68000000e-01  4.40000000e-02  3.82000000e-01
 -1.57000000e-01  4.67000000e-01 -3.59000000e-01  6.10000000e-01
 -6.84000000e-01 -1.51000000e-01  3.25000000e-01  1.97000000e-01
 -2.41000000e-01  5.70000000e-02  1.84000000e-01 -2.52000000e-01
  2.53000000e-01  5.80000000e-01 -6.70000000e-02 -9.24000000e-01
 -2.92000000e-01 -8.90000000e-02 -6.63000000e-01 -4.14000000e-01
 -4.36000000e-01  1.62000000e-01 -8.04000000e-01  3.83000000e-01
 -2.80000000e-02  3.67000000e-01 -2.05000000e-01 -5.43000000e-01
  1.56000000e-01  9.31000000e-01 -1.31000000e-01  5.95000000e-01
  1.27000000e-01 -5.83000000e-01  6.54000000e-01 -1.48000000e-01
  2.98000000e-01  3.81000000e-01  1.34000000e-01  9.00000000e-02
  1.73000000e-01  2.20000000e-01  1.89000000e-01  4.51000000e-01
 -9.32000000e-01 -4.30000000e-02  3.91000000e-01  2.49000000e-01
  1.90000000e-01 -2.47000000e-01  6.52000000e-01  7.20000000e-02
  1.70000000e-01 -5.82000000e-01  4.41000000e-01 -6.14000000e-01
  6.50000000e-01  2.06000000e-01  3.02000000e-01  1.11000000e-01
  3.31000000e-01 -1.87000000e-01 -1.04000000e-01  1.24000000e-01
 -4.76000000e-01  1.29000000e-01  2.87000000e-01 -3.24000000e-01
 -2.47000000e-01 -3.07000000e-01  4.50000000e-02  1.77000000e-01
 -4.80000000e-02  2.06000000e-01 -2.00000000e-01  3.72000000e-01
 -4.87000000e-01 -1.14000000e-01 -7.35000000e-01  4.00000000e-02
 -1.72000000e-01  4.01000000e-01 -7.11000000e-01  4.90000000e-01
  5.48000000e-01  4.25000000e-01 -4.47000000e-01  6.78000000e-01
  2.74000000e-01 -3.40000000e-02  2.33000000e-01  6.60000000e-02
 -5.23000000e-01  5.98000000e-01 -1.10000000e-02 -5.90000000e-02
 -3.50000000e-02  1.81000000e-01 -9.60000000e-02 -9.00000000e-03
 -4.64000000e-01  1.67000000e-01  1.82000000e-01  4.38000000e-01
  2.89000000e-01  6.26000000e-01  4.08000000e-01 -4.03000000e-01
 -4.44000000e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/09/2021 10:16:39 - INFO - data_loader -   *** Example ***
10/09/2021 10:16:39 - INFO - data_loader -   guid: test-3
10/09/2021 10:16:39 - INFO - data_loader -   tokens: [CLS] yu ##p ign ##ite to ks no ##ob act [SEP]
10/09/2021 10:16:39 - INFO - data_loader -   input_ids: 101 9805 2361 16270 4221 2000 29535 2053 16429 2552 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   intent_label: 2 (id = 2)
10/09/2021 10:16:39 - INFO - data_loader -   slot_labels: 0 4 0 3 0 4 4 7 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   ner_embeds: [ 1.46634594e-01 -5.26245758e-02 -9.95037891e-03 -9.02985111e-02
  1.22633725e-01 -1.46261543e-01  2.04309840e-02 -1.69067800e-01
  3.29489075e-02  1.92000000e-01  3.00000000e-02 -5.41000000e-01
 -2.49000000e-01  8.70000000e-02 -4.46000000e-01  3.70000000e-01
  4.68000000e-01 -1.50000000e-02 -3.35000000e-01  8.40000000e-02
  2.48000000e-01 -1.15000000e-01 -2.24000000e-01 -1.04000000e-01
 -2.34000000e-01 -4.87000000e-01  4.34000000e-01 -1.94000000e-01
 -4.70000000e-02  2.05000000e-01 -2.13000000e-01 -3.94000000e-01
  4.73000000e-01 -4.53000000e-01  3.11000000e-01 -1.39000000e-01
  5.00000000e-01 -2.43000000e-01 -6.13000000e-01  6.62000000e-01
  5.02000000e-01 -1.15000000e-01 -6.00000000e-02 -5.62000000e-01
  4.21000000e-01  1.01000000e-01 -4.60000000e-02  4.17000000e-01
 -1.92000000e-01 -4.67000000e-01  4.93000000e-01  4.42000000e-01
  1.30000000e-02 -2.28000000e-01 -2.03000000e-01 -1.41000000e-01
  2.71000000e-01  1.18000000e-01 -3.38000000e-01  3.52000000e-01
  2.73000000e-01 -4.71000000e-01 -5.99000000e-01 -4.63000000e-01
 -2.47000000e-01  4.56000000e-01  1.35000000e-01  9.40000000e-02
  1.00700000e+00  6.66000000e-01 -8.60000000e-02 -1.05000000e-01
 -1.01000000e+00  4.70000000e-02  7.10000000e-02 -6.45000000e-01
 -5.60000000e-02 -6.30000000e-02  1.00000000e-03  2.21000000e-01
  1.97000000e-01  2.00000000e-03  3.97000000e-01  5.38000000e-01
  9.70000000e-02 -1.10000000e-01  6.21000000e-01  3.56000000e-01
  4.54000000e-01  1.09000000e-01 -1.18000000e-01  8.80000000e-02
  1.50000000e-01 -3.39000000e-01 -1.92000000e-01 -4.03000000e-01
 -3.47000000e-01  2.93000000e-01  5.40000000e-02  1.51000000e-01
  3.64000000e-01  1.86000000e-01 -6.30000000e-01 -1.04000000e-01
 -7.13000000e-01  2.08000000e-01  4.06000000e-01 -3.94000000e-01
  6.60000000e-02  2.45000000e-01  4.08000000e-01  1.03300000e+00
  2.93000000e-01  9.50000000e-02 -8.00000000e-03  4.32000000e-01
 -3.22000000e-01 -5.65000000e-01 -9.60000000e-02 -3.08000000e-01
 -5.73000000e-01 -4.07000000e-01  2.75000000e-01  1.48000000e-01
 -1.80000000e-02  7.28000000e-01 -5.44000000e-01 -1.05200000e+00
  1.10000000e-02  5.48000000e-01 -1.49000000e-01  1.25000000e-01
 -3.13000000e-01 -3.50000000e-02  3.75000000e-01 -7.79000000e-01
 -3.37000000e-01  2.20000000e-02 -4.77000000e-01 -6.28000000e-01
  3.62000000e-01  6.60000000e-02  1.41000000e-01 -2.86000000e-01
  2.92000000e-01 -6.90000000e-02  1.10000000e-02 -5.11000000e-01
 -1.43000000e-01 -1.24000000e-01  5.52000000e-01 -2.40000000e-01
 -1.52000000e-01 -6.00000000e-03 -2.08000000e-01  8.30000000e-02
 -1.27000000e-01 -1.99000000e-01 -1.50000000e-01 -1.87000000e-01
 -3.20000000e-02 -3.70000000e-02 -2.48000000e-01 -3.86000000e-01
 -1.28500000e+00  2.65000000e-01 -4.45000000e-01 -1.56000000e-01
  3.11000000e-01  9.00000000e-03 -7.97000000e-01 -2.30000000e-02
 -1.27000000e-01 -5.87000000e-01  3.30000000e-01  4.85000000e-01
  4.66000000e-01 -6.44000000e-01 -3.77000000e-01 -2.55000000e-01
  7.00000000e-03 -2.55000000e-01  6.24000000e-01  2.89000000e-01
 -1.08000000e-01 -3.26000000e-01  3.38000000e-01 -2.09000000e-01
 -3.15000000e-01  6.17000000e-01 -7.30000000e-02 -1.43000000e-01
  3.33000000e-01 -1.95000000e-01  2.34000000e-01  8.35000000e-01
  1.35000000e-01 -2.53000000e-01  3.40000000e-02  5.29000000e-01
 -2.62000000e-01 -9.70000000e-02  2.98000000e-01  5.29000000e-01
  2.40000000e-02 -3.21000000e-01  4.54000000e-01 -4.95000000e-01
  4.05000000e-01 -3.84000000e-01  4.15000000e-01  5.03000000e-01
 -1.39000000e-01 -2.59000000e-01 -1.34000000e-01 -7.32000000e-01
 -2.70000000e-01 -1.26000000e-01  5.95000000e-01  2.06000000e-01
 -2.77000000e-01  6.90000000e-02 -1.01800000e+00 -2.20000000e-02
  2.90000000e-02 -2.99000000e-01  3.42000000e-01 -3.03000000e-01
 -8.00000000e-02  7.42000000e-01 -3.80000000e-02  1.46000000e-01
 -3.71000000e-01 -7.46000000e-01 -4.52000000e-01 -1.20000000e-02
  2.84000000e-01  3.18000000e-01 -7.60000000e-01 -2.80000000e-01
 -1.16000000e-01  6.60000000e-02  4.19000000e-01 -2.28000000e-01
  1.48000000e-01 -5.00000000e-02  3.20000000e-01 -9.70000000e-02
 -4.52000000e-01  7.45000000e-01 -9.40000000e-02  7.33000000e-01
 -4.37000000e-01  4.89000000e-01 -3.09000000e-01  2.21000000e-01
 -3.14000000e-01 -2.21000000e-01 -7.70000000e-01  7.30000000e-02
  5.16000000e-01  7.95000000e-01 -4.28000000e-01  2.95000000e-01
 -5.41000000e-01  5.65000000e-01 -3.18000000e-01 -3.05000000e-01
 -3.64000000e-01 -5.40000000e-02 -9.98000000e-01 -4.05000000e-01
 -3.30000000e-01 -2.52000000e-01 -4.55000000e-01 -1.16000000e-01
  1.40000000e-02  6.80000000e-02 -6.90000000e-02 -8.40000000e-02
  2.22000000e-01  4.01000000e-01 -6.30000000e-02  1.85000000e-01
  6.60000000e-01  1.52000000e-01 -9.00000000e-03  6.19000000e-01
 -3.46000000e-01  7.70000000e-02 -8.50000000e-02  6.60000000e-02
  5.58000000e-01  2.95000000e-01 -2.67000000e-01 -2.70000000e-01
 -7.50000000e-02  1.60000000e-01 -4.59000000e-01 -4.39000000e-01
 -2.71000000e-01  4.49000000e-01  3.45000000e-01 -2.73000000e-01
 -4.57000000e-01  2.84000000e-01 -3.90000000e-02 -2.00000000e-03
  8.70000000e-02] [ 0.20918053  0.07401545 -0.02397111 -0.05580628  0.11399751 -0.25836805
 -0.02977259 -0.23865256  0.03642225 -0.031       0.503      -0.021
 -0.027      -0.386       0.086       0.245       0.001       0.027
 -0.008      -0.312       0.243      -0.01       -0.062       0.253
 -0.041      -0.091       0.23        0.239       0.004      -0.012
  0.107       0.269       0.025       0.117       0.064      -0.019
  0.173      -0.166       0.209      -0.104      -0.073       0.21
  0.164      -0.081       0.206      -0.022       0.216       0.052
  0.222      -0.104       0.037      -0.239      -0.159      -0.272
  0.165       0.116       0.178       0.014      -0.17       -0.113
  0.07        0.059       0.2         0.02        0.183       0.026
 -0.057      -0.269       0.153       0.303      -0.067       0.021
 -0.187      -0.125      -0.274       0.207      -0.247      -0.014
  0.04       -0.529       0.02       -0.376       0.386       0.069
 -0.029      -0.338      -0.316       0.106      -0.073       0.119
 -0.171       0.227      -0.059      -0.325       0.175      -0.286
  0.002      -0.114      -0.282      -0.18       -0.057      -0.011
  0.035       0.242      -0.024       0.259       0.074       0.102
  0.078       0.399       0.254       0.195       0.013       0.109
 -0.292      -0.303      -0.013      -0.064       0.102      -0.164
  0.007       0.192      -0.246      -0.113       0.032      -0.035
 -0.149       0.066       0.107       0.023       0.018       0.174
 -0.263      -0.038      -0.062      -0.          0.056       0.147
 -0.16        0.057      -0.491      -0.009       0.011       0.046
 -0.215       0.083      -0.059       0.115       0.146      -0.034
  0.095      -0.282       0.076      -0.241      -0.136       0.041
  0.079      -0.384      -0.234      -0.269       0.082      -0.121
 -0.207      -0.301       0.249      -0.193       0.009       0.057
 -0.192       0.406       0.042       0.105       0.038       0.39
  0.085       0.657      -0.167      -0.085      -0.157      -0.024
  0.364      -0.371      -0.046      -0.095       0.053      -0.421
 -0.267       0.105       0.052       0.212       0.04        0.136
 -0.135       0.171       0.087       0.113       0.203       0.269
 -0.06       -0.277       0.         -0.032      -0.16        0.074
 -0.046       0.156      -0.078      -0.33        0.255      -0.213
  0.188       0.292      -0.069      -0.069      -0.334      -0.214
 -0.148       0.403       0.093       0.44       -0.218      -0.157
  0.038       0.551      -0.189      -0.282       0.111       0.268
  0.331       0.106      -0.075      -0.086       0.034      -0.307
 -0.266      -0.327      -0.257       0.274       0.021      -0.102
 -0.146      -0.258      -0.24        0.339      -0.182      -0.32
 -0.031      -0.188       0.032       0.139      -0.065      -0.045
 -0.126       0.287      -0.367      -0.234       0.266      -0.061
  0.089      -0.049       0.311      -0.099      -0.208       0.027
  0.107       0.014      -0.036       0.122       0.044       0.265
  0.163       0.059      -0.076      -0.005       0.02       -0.013
  0.108      -0.247      -0.06       -0.415       0.154       0.175
 -0.03       -0.186      -0.195       0.048      -0.243       0.183
  0.003      -0.096       0.136       0.066       0.199       0.14
  0.27        0.131      -0.172       0.051       0.087      -0.161
  0.289      -0.347      -0.212       0.137      -0.223      -0.341
  0.004      -0.183       0.367     ] [ 0.21902114  0.1636456   0.0723623  -0.04616668  0.06290206 -0.18633354
 -0.05794701 -0.44760141  0.1384591  -0.062       1.006      -0.042
 -0.054      -0.772       0.172       0.49        0.002       0.054
 -0.016      -0.624       0.486      -0.02       -0.124       0.506
 -0.082      -0.182       0.46        0.478       0.008      -0.024
  0.214       0.538       0.05        0.234       0.128      -0.038
  0.346      -0.332       0.418      -0.208      -0.146       0.42
  0.328      -0.162       0.412      -0.044       0.432       0.104
  0.444      -0.208       0.074      -0.478      -0.318      -0.544
  0.33        0.232       0.356       0.028      -0.34       -0.226
  0.14        0.118       0.4         0.04        0.366       0.052
 -0.114      -0.538       0.306       0.606      -0.134       0.042
 -0.374      -0.25       -0.548       0.414      -0.494      -0.028
  0.08       -1.058       0.04       -0.752       0.772       0.138
 -0.058      -0.676      -0.632       0.212      -0.146       0.238
 -0.342       0.454      -0.118      -0.65        0.35       -0.572
  0.004      -0.228      -0.564      -0.36       -0.114      -0.022
  0.07        0.484      -0.048       0.518       0.148       0.204
  0.156       0.798       0.508       0.39        0.026       0.218
 -0.584      -0.606      -0.026      -0.128       0.204      -0.328
  0.014       0.384      -0.492      -0.226       0.064      -0.07
 -0.298       0.132       0.214       0.046       0.036       0.348
 -0.526      -0.076      -0.124      -0.          0.112       0.294
 -0.32        0.114      -0.982      -0.018       0.022       0.092
 -0.43        0.166      -0.118       0.23        0.292      -0.068
  0.19       -0.564       0.152      -0.482      -0.272       0.082
  0.158      -0.768      -0.468      -0.538       0.164      -0.242
 -0.414      -0.602       0.498      -0.386       0.018       0.114
 -0.384       0.812       0.084       0.21        0.076       0.78
  0.17        1.314      -0.334      -0.17       -0.314      -0.048
  0.728      -0.742      -0.092      -0.19        0.106      -0.842
 -0.534       0.21        0.104       0.424       0.08        0.272
 -0.27        0.342       0.174       0.226       0.406       0.538
 -0.12       -0.554       0.         -0.064      -0.32        0.148
 -0.092       0.312      -0.156      -0.66        0.51       -0.426
  0.376       0.584      -0.138      -0.138      -0.668      -0.428
 -0.296       0.806       0.186       0.88       -0.436      -0.314
  0.076       1.102      -0.378      -0.564       0.222       0.536
  0.662       0.212      -0.15       -0.172       0.068      -0.614
 -0.532      -0.654      -0.514       0.548       0.042      -0.204
 -0.292      -0.516      -0.48        0.678      -0.364      -0.64
 -0.062      -0.376       0.064       0.278      -0.13       -0.09
 -0.252       0.574      -0.734      -0.468       0.532      -0.122
  0.178      -0.098       0.622      -0.198      -0.416       0.054
  0.214       0.028      -0.072       0.244       0.088       0.53
  0.326       0.118      -0.152      -0.01        0.04       -0.026
  0.216      -0.494      -0.12       -0.83        0.308       0.35
 -0.06       -0.372      -0.39        0.096      -0.486       0.366
  0.006      -0.192       0.272       0.132       0.398       0.28
  0.54        0.262      -0.344       0.102       0.174      -0.322
  0.578      -0.694      -0.424       0.274      -0.446      -0.682
  0.008      -0.366       0.734     ] [ 0.27047098 -0.00681241  0.04974469 -0.11996648  0.04411501 -0.17894775
 -0.02466077 -0.24340613  0.01943829 -0.036      -0.27       -0.023
  0.22       -0.332       0.129      -0.08       -0.079       0.326
 -0.005      -0.17       -0.322      -0.048       0.149       0.042
  0.046      -0.252      -0.079       0.019       0.061      -0.166
  0.011       0.091      -0.009       0.131      -0.419      -0.478
 -0.221       0.042      -0.036      -0.219       0.215       0.582
 -0.082       0.035      -0.124      -0.016      -0.472       0.181
  0.374       0.116       0.19        0.104      -0.027       0.208
 -0.043      -0.003       0.266      -0.633      -0.353       0.432
  0.512       0.185      -0.149      -0.143       0.305       0.349
 -0.118       0.195       0.018      -0.309       0.241      -0.374
 -0.503      -0.131       0.354       0.161      -0.416      -0.282
  0.086      -0.292      -0.212      -0.26       -0.152       0.107
  0.156       0.327       0.355       0.034      -0.217       0.443
  0.237       0.095       0.224      -0.565      -0.256      -0.445
 -0.224      -0.036       0.233      -0.589       0.097       0.278
 -0.343      -0.078      -0.056       0.104       0.379       0.492
 -0.287       0.21       -0.26        0.381      -0.001      -0.137
 -0.227      -0.234      -0.164      -0.045       0.268      -0.048
 -0.426      -0.184      -0.195       0.107      -0.136       0.115
 -0.124       0.113      -0.136       0.112      -0.04        0.073
 -0.067       0.064       0.098      -0.081       0.292      -0.024
 -0.307      -0.087       0.389       0.13        0.03        0.029
  0.108      -0.223       0.025      -0.327      -0.164      -0.043
  0.324       0.199      -0.181      -0.301       0.173      -0.151
  0.64       -0.317       0.202      -0.028       0.219      -0.162
 -0.236      -0.255      -0.218      -0.002       0.526      -0.069
 -0.512       0.189      -0.083      -0.044      -0.315       0.274
  0.357      -0.05        0.178      -0.565       0.091       0.406
 -0.182       0.025       0.184       0.015       0.227      -0.42
 -0.254      -0.459       0.142       0.231       0.157      -0.131
 -0.206      -0.241      -0.092       0.194       0.259       0.419
 -0.16        0.093      -0.37       -0.341      -0.009       0.066
 -0.316       0.004       0.356       0.018       0.059       0.237
 -0.09       -0.064      -0.101       0.098       0.316      -0.061
 -0.455      -0.244      -0.084       0.051      -0.037      -0.662
 -0.623       0.032      -0.265      -0.084       0.218       0.105
 -0.117       0.042      -0.115      -0.101       0.276       0.269
  0.142      -0.483       0.406       0.302       0.055       0.02
 -0.088      -0.399      -0.277       0.017      -0.265      -0.033
  0.448       0.077       0.228       0.451      -0.02        0.003
  0.33       -0.313      -0.37       -0.456      -0.228      -0.457
 -0.051       0.193       0.295       0.274      -0.178       0.086
 -0.279      -0.063       0.069      -0.221      -0.259      -0.199
 -0.201       0.062      -0.156      -0.25        0.177       0.23
  0.14       -0.003       0.057      -0.216      -0.225       0.151
  0.094      -0.493      -0.198       0.049       0.429       0.207
 -0.214       0.063      -0.136       0.192       0.167       0.034
 -0.359      -0.348      -0.036      -0.121      -0.44        0.187
  0.222       0.09       -0.037       0.012      -0.007      -0.014
 -0.043      -0.201       0.154     ] [ 0.1097418  -0.02649348 -0.02142039 -0.10511508 -0.0092895  -0.19745882
  0.00365825 -0.23398751 -0.14191425 -0.072      -0.54       -0.046
  0.44       -0.664       0.258      -0.16       -0.158       0.652
 -0.01       -0.34       -0.644      -0.096       0.298       0.084
  0.092      -0.504      -0.158       0.038       0.122      -0.332
  0.022       0.182      -0.018       0.262      -0.838      -0.956
 -0.442       0.084      -0.072      -0.438       0.43        1.164
 -0.164       0.07       -0.248      -0.032      -0.944       0.362
  0.748       0.232       0.38        0.208      -0.054       0.416
 -0.086      -0.006       0.532      -1.266      -0.706       0.864
  1.024       0.37       -0.298      -0.286       0.61        0.698
 -0.236       0.39        0.036      -0.618       0.482      -0.748
 -1.006      -0.262       0.708       0.322      -0.832      -0.564
  0.172      -0.584      -0.424      -0.52       -0.304       0.214
  0.312       0.654       0.71        0.068      -0.434       0.886
  0.474       0.19        0.448      -1.13       -0.512      -0.89
 -0.448      -0.072       0.466      -1.178       0.194       0.556
 -0.686      -0.156      -0.112       0.208       0.758       0.984
 -0.574       0.42       -0.52        0.762      -0.002      -0.274
 -0.454      -0.468      -0.328      -0.09        0.536      -0.096
 -0.852      -0.368      -0.39        0.214      -0.272       0.23
 -0.248       0.226      -0.272       0.224      -0.08        0.146
 -0.134       0.128       0.196      -0.162       0.584      -0.048
 -0.614      -0.174       0.778       0.26        0.06        0.058
  0.216      -0.446       0.05       -0.654      -0.328      -0.086
  0.648       0.398      -0.362      -0.602       0.346      -0.302
  1.28       -0.634       0.404      -0.056       0.438      -0.324
 -0.472      -0.51       -0.436      -0.004       1.052      -0.138
 -1.024       0.378      -0.166      -0.088      -0.63        0.548
  0.714      -0.1         0.356      -1.13        0.182       0.812
 -0.364       0.05        0.368       0.03        0.454      -0.84
 -0.508      -0.918       0.284       0.462       0.314      -0.262
 -0.412      -0.482      -0.184       0.388       0.518       0.838
 -0.32        0.186      -0.74       -0.682      -0.018       0.132
 -0.632       0.008       0.712       0.036       0.118       0.474
 -0.18       -0.128      -0.202       0.196       0.632      -0.122
 -0.91       -0.488      -0.168       0.102      -0.074      -1.324
 -1.246       0.064      -0.53       -0.168       0.436       0.21
 -0.234       0.084      -0.23       -0.202       0.552       0.538
  0.284      -0.966       0.812       0.604       0.11        0.04
 -0.176      -0.798      -0.554       0.034      -0.53       -0.066
  0.896       0.154       0.456       0.902      -0.04        0.006
  0.66       -0.626      -0.74       -0.912      -0.456      -0.914
 -0.102       0.386       0.59        0.548      -0.356       0.172
 -0.558      -0.126       0.138      -0.442      -0.518      -0.398
 -0.402       0.124      -0.312      -0.5         0.354       0.46
  0.28       -0.006       0.114      -0.432      -0.45        0.302
  0.188      -0.986      -0.396       0.098       0.858       0.414
 -0.428       0.126      -0.272       0.384       0.334       0.068
 -0.718      -0.696      -0.072      -0.242      -0.88        0.374
  0.444       0.18       -0.074       0.024      -0.014      -0.028
 -0.086      -0.402       0.308     ] [ 0.27799532  0.25141633  0.11683662  0.01045998  0.04270918 -0.07840008
 -0.08661126 -0.30682501  0.16490841  0.214      -0.048      -0.045
 -0.188       0.415      -0.082      -0.318      -0.329      -0.017
  0.204       0.29        0.016       0.338      -0.25       -0.104
 -0.123       0.008       0.228       0.155       0.063      -0.154
 -0.056      -0.098       0.242      -0.069       0.069       0.348
 -0.226      -0.062       0.148       0.055       0.039       0.255
  0.037      -0.349       0.133       0.013      -0.147       0.095
  0.043      -0.123       0.212       0.127       0.191       0.3
 -0.45        0.315       0.327      -0.133      -0.304       0.169
  0.318      -0.501      -0.224       0.026      -0.204      -0.071
  0.123       0.012      -0.048       0.036       0.42       -0.119
 -0.116      -0.298      -0.186       0.14        0.054       0.413
 -0.05       -0.348      -0.309      -0.047      -0.125      -0.08
 -0.306      -0.002       0.152       0.061       0.298       0.186
  0.099       0.258       0.16       -0.272      -0.298       0.024
  0.082      -0.102      -0.168      -0.122       0.077      -0.069
 -0.178      -0.189      -0.173       0.317       0.643      -0.135
 -0.067      -0.084       0.         -0.066      -0.12        0.141
 -0.215      -0.125      -0.189       0.027       0.08        0.108
 -0.184      -0.361       0.047       0.329      -0.208      -0.097
 -0.013      -0.09        0.133       0.148       0.029       0.036
 -0.245      -0.401       0.222      -0.066       0.087      -0.389
  0.064       0.141      -0.281       0.064      -0.326       0.376
 -0.277      -0.302       0.244      -0.096      -0.104      -0.122
 -0.213      -0.085       0.136      -0.184      -0.146      -0.142
 -0.592      -0.319       0.157      -0.056      -0.246      -0.338
 -0.193      -0.07        0.128       0.222      -0.048      -0.286
  0.105       0.566      -0.404       0.093      -0.519       0.171
 -0.293      -0.011       0.205      -0.136       0.184       0.348
 -0.056       0.241       0.442      -0.013      -0.361       0.029
 -0.068      -0.289       0.48        0.182      -0.414      -0.58
 -0.157       0.088      -0.36       -0.494       0.23        0.338
  0.105       0.509      -0.507      -0.209      -0.168       0.157
 -0.284      -0.084       0.132       0.049      -0.309       0.009
 -0.258       0.449      -0.482       0.425       0.054      -0.017
 -0.181      -0.147      -0.244       0.182       0.293       0.34
  0.303       0.271      -0.163      -0.13        0.032       0.056
  0.132       0.174       0.313       0.077      -0.274      -0.165
 -0.285      -0.283       0.207       0.339       0.166       0.084
  0.181       0.015       0.072      -0.028      -0.296      -0.41
  0.24       -0.254       0.123       0.011      -0.021       0.232
 -0.052       0.26       -0.005      -0.384      -0.316       0.13
 -0.175      -0.074       0.26        0.027      -0.177       0.283
 -0.083      -0.382      -0.385       0.035       0.094      -0.137
 -0.136       0.06        0.253      -0.116      -0.201       0.007
 -0.168      -0.091      -0.166      -0.169      -0.216       0.069
 -0.104       0.23        0.234      -0.187       0.377       0.111
 -0.029       0.444       0.365      -0.146      -0.147       0.095
  0.038      -0.042       0.038      -0.02        0.073      -0.328
  0.275       0.113       0.184       0.015       0.56       -0.178
  0.074       0.5         0.013     ] [ 0.11285562  0.07224368  0.05592576  0.09028678  0.03394749 -0.05047422
  0.21182679 -0.12933064 -0.05022215  0.245       0.328       0.026
 -0.433      -0.139      -0.027      -0.323       0.087      -0.167
 -0.274      -0.298       0.153       0.041       0.155      -0.022
  0.055      -0.358      -0.146       0.312       0.241      -0.161
  0.349       0.088       0.222       0.058       0.038       0.096
  0.11        0.194      -0.136      -0.299      -0.022      -0.257
  0.078       0.317      -0.003      -0.17        0.22       -0.593
  0.265      -0.035       0.094      -0.129       0.288      -0.034
  0.101      -0.422       0.039       0.166      -0.385       0.033
  0.037       0.016       0.364      -0.231       0.156      -0.298
 -0.213      -0.128      -0.378      -0.009       0.352      -0.076
  0.141       0.016       0.004      -0.074      -0.659      -0.002
  0.137      -0.486       0.148       0.011      -0.075      -0.006
  0.047       0.174      -0.171       0.18       -0.166       0.69
 -0.212      -0.195       0.651      -0.09       -0.125      -0.278
  0.206       0.327      -0.257       0.057      -0.275      -0.007
 -0.036      -0.056      -0.408       0.428       0.081       0.134
 -0.211      -0.41       -0.293       0.031       0.332       0.403
 -0.001      -0.31       -0.133      -0.225       0.205      -0.077
 -0.148      -0.082       0.02        0.277      -0.057      -0.144
  0.256       0.389       0.023       0.114       0.441      -0.035
 -0.066      -0.058      -0.109       0.195      -0.157      -0.008
 -0.216       0.382      -0.337      -0.198      -0.374       0.09
 -0.329      -0.015       0.061       0.45        0.469       0.173
  0.298      -0.082       0.448      -0.198       0.213       0.016
 -0.085      -0.68       -0.006      -0.25        0.37       -0.264
  0.121      -0.383      -0.096       0.12        0.374      -0.149
  0.308       0.067      -0.649      -0.046       0.643       0.378
 -0.257       0.374      -0.121       0.284      -0.072       0.089
 -0.308      -0.09        0.052       0.107      -0.216       0.261
 -0.19        0.137       0.343      -0.343       0.358       0.032
  0.154      -0.253       0.053       0.546       0.069      -0.338
 -0.398       0.058      -0.215      -0.092       0.027       0.497
 -0.156      -0.004       0.05       -0.297      -0.027      -0.014
  0.545       0.241       0.032       0.02       -0.051      -0.195
  0.163      -0.202      -0.496       0.387       0.091       0.001
  0.127      -0.174       0.072       0.216      -0.29        0.655
  0.451       0.427       0.166       0.192      -0.052      -0.662
  0.021      -0.058      -0.145       0.498       0.457      -0.07
 -0.234       0.064       0.448       0.159      -0.175       0.048
 -0.035       0.008      -0.198       0.061       0.001      -0.314
 -0.003       0.037      -0.204      -0.232       0.586      -0.013
  0.52        0.599      -0.409      -0.285       0.127      -0.262
  0.397      -0.044       0.081       0.19        0.352       0.568
  0.38        0.206      -0.204       0.426      -0.387       0.245
  0.01       -0.041       0.064      -0.212      -0.222       0.255
 -0.377       0.511      -0.404       0.304      -0.004       0.175
  0.469       0.219       0.311       0.358       0.276       0.334
  0.451       0.23        0.298      -0.031      -0.419      -0.001
 -0.13        0.247       0.097      -0.141       0.055      -0.019
 -0.23       -0.403       0.184     ] [ 0.22545928  0.14043103  0.05814986 -0.18108423 -0.04890471 -0.17474529
 -0.07501269 -0.08731554  0.07166796  0.152       0.508       0.125
  0.069      -0.497      -0.161       0.446      -0.629       0.386
  0.075      -0.202      -0.012       0.083      -0.144       0.123
 -0.37        0.122       0.299       0.161      -0.211      -0.036
 -0.12        0.063       0.085      -0.043      -0.239      -0.239
  0.117      -0.         -0.124       0.083      -0.164      -0.164
 -0.192       0.1         0.339      -0.135       0.236      -0.285
  0.48       -0.537       0.087      -0.311      -0.013       0.115
 -0.493      -0.194      -0.237       0.245      -0.26        0.074
  0.318      -0.263       0.003      -0.539       0.566       0.075
  0.125      -0.315       0.455      -0.052       0.38        0.545
 -0.091       0.106       0.424      -0.068      -0.49       -0.111
  0.018      -0.186      -0.433      -0.295       0.108      -0.099
  0.205      -0.26       -0.337       0.146      -0.268       0.198
 -0.357       0.081       0.399      -0.089      -0.544      -0.27
 -0.129       0.065      -0.059      -0.256      -0.37       -0.269
  0.261      -0.415       0.023       0.186       0.717      -0.071
  0.284       0.18       -0.192      -0.01       -0.161      -0.109
 -0.418      -0.13        0.202      -0.316       0.149      -0.149
  0.38       -0.44       -0.096      -0.309      -0.096      -0.148
  0.271      -0.041       0.08       -0.403      -0.063       0.406
 -0.424       0.079      -0.037       0.529       0.01        0.335
 -0.085      -0.382      -0.25       -0.213       0.349      -0.151
  0.048       0.216       0.141       0.09        0.272      -0.025
  0.199      -0.085      -0.276      -0.187       0.207       0.268
 -0.209      -0.332      -0.101      -0.405       0.218      -0.444
 -0.063       0.189       0.055       0.113      -0.011      -0.344
 -0.135       0.515      -0.405      -0.228       0.03       -0.213
 -0.063      -0.062       0.167      -0.089      -0.07       -0.396
  0.35        0.553       0.17        0.23        0.112      -0.431
 -0.357      -0.023       0.159       0.5        -0.281      -0.141
  0.421       0.027      -0.261       0.389      -0.286      -0.119
  0.057       0.023       0.14       -0.128      -0.016      -0.103
  0.002       0.107       0.131      -0.322       0.452      -0.38
  0.247      -0.209      -0.144       0.125      -0.363       0.062
 -0.153      -0.164       0.225      -0.061      -0.418      -0.31
 -0.385       0.499       0.328       0.065      -0.098       0.477
  0.451       0.051       0.18        0.343       0.237      -0.217
 -0.025      -0.564      -0.31        0.119       0.04        0.003
  0.081      -0.676       0.199       0.095      -0.472      -0.271
  0.484      -0.351      -0.119       0.084      -0.188       0.341
 -0.285       0.058      -0.003      -0.279      -0.186       0.279
 -0.337       0.071      -0.156       0.191      -0.424      -0.013
 -0.183       0.447      -0.541      -0.219      -0.386      -0.016
  0.339       0.208      -0.105      -0.441       0.249      -0.321
  0.295      -0.111      -0.343      -0.041      -0.214       0.079
 -0.165       0.228      -0.103       0.16       -0.308       0.354
 -0.587      -0.172      -0.375      -0.01        0.388       0.297
  0.095      -0.169      -0.019      -0.21        0.09       -0.311
  0.19        0.152      -0.29        0.011       0.153       0.045
  0.058      -0.108       0.362     ] [ 0.24989802  0.07626641  0.06149831 -0.11079762 -0.00570099 -0.16137436
  0.01758201 -0.19006416  0.06939683  0.304       1.016       0.25
  0.138      -0.994      -0.322       0.892      -1.258       0.772
  0.15       -0.404      -0.024       0.166      -0.288       0.246
 -0.74        0.244       0.598       0.322      -0.422      -0.072
 -0.24        0.126       0.17       -0.086      -0.478      -0.478
  0.234      -0.         -0.248       0.166      -0.328      -0.328
 -0.384       0.2         0.678      -0.27        0.472      -0.57
  0.96       -1.074       0.174      -0.622      -0.026       0.23
 -0.986      -0.388      -0.474       0.49       -0.52        0.148
  0.636      -0.526       0.006      -1.078       1.132       0.15
  0.25       -0.63        0.91       -0.104       0.76        1.09
 -0.182       0.212       0.848      -0.136      -0.98       -0.222
  0.036      -0.372      -0.866      -0.59        0.216      -0.198
  0.41       -0.52       -0.674       0.292      -0.536       0.396
 -0.714       0.162       0.798      -0.178      -1.088      -0.54
 -0.258       0.13       -0.118      -0.512      -0.74       -0.538
  0.522      -0.83        0.046       0.372       1.434      -0.142
  0.568       0.36       -0.384      -0.02       -0.322      -0.218
 -0.836      -0.26        0.404      -0.632       0.298      -0.298
  0.76       -0.88       -0.192      -0.618      -0.192      -0.296
  0.542      -0.082       0.16       -0.806      -0.126       0.812
 -0.848       0.158      -0.074       1.058       0.02        0.67
 -0.17       -0.764      -0.5        -0.426       0.698      -0.302
  0.096       0.432       0.282       0.18        0.544      -0.05
  0.398      -0.17       -0.552      -0.374       0.414       0.536
 -0.418      -0.664      -0.202      -0.81        0.436      -0.888
 -0.126       0.378       0.11        0.226      -0.022      -0.688
 -0.27        1.03       -0.81       -0.456       0.06       -0.426
 -0.126      -0.124       0.334      -0.178      -0.14       -0.792
  0.7         1.106       0.34        0.46        0.224      -0.862
 -0.714      -0.046       0.318       1.         -0.562      -0.282
  0.842       0.054      -0.522       0.778      -0.572      -0.238
  0.114       0.046       0.28       -0.256      -0.032      -0.206
  0.004       0.214       0.262      -0.644       0.904      -0.76
  0.494      -0.418      -0.288       0.25       -0.726       0.124
 -0.306      -0.328       0.45       -0.122      -0.836      -0.62
 -0.77        0.998       0.656       0.13       -0.196       0.954
  0.902       0.102       0.36        0.686       0.474      -0.434
 -0.05       -1.128      -0.62        0.238       0.08        0.006
  0.162      -1.352       0.398       0.19       -0.944      -0.542
  0.968      -0.702      -0.238       0.168      -0.376       0.682
 -0.57        0.116      -0.006      -0.558      -0.372       0.558
 -0.674       0.142      -0.312       0.382      -0.848      -0.026
 -0.366       0.894      -1.082      -0.438      -0.772      -0.032
  0.678       0.416      -0.21       -0.882       0.498      -0.642
  0.59       -0.222      -0.686      -0.082      -0.428       0.158
 -0.33        0.456      -0.206       0.32       -0.616       0.708
 -1.174      -0.344      -0.75       -0.02        0.776       0.594
  0.19       -0.338      -0.038      -0.42        0.18       -0.622
  0.38        0.304      -0.58        0.022       0.306       0.09
  0.116      -0.216       0.724     ] [ 3.77994835e-01  5.87039590e-02  1.49804547e-01 -1.17118746e-01
 -1.51070915e-02 -2.61278927e-01 -4.70321998e-02 -1.60322666e-01
 -1.44194663e-01  2.21000000e-01 -2.83000000e-01  3.18000000e-01
  1.00000000e-02 -6.47000000e-01  1.18000000e-01 -1.07700000e+00
  2.19000000e-01 -6.01000000e-01  1.18000000e-01  4.79000000e-01
 -6.49000000e-01 -4.58000000e-01  7.20000000e-01 -2.07000000e-01
 -5.16000000e-01 -2.38000000e-01 -3.66000000e-01  2.44000000e-01
 -2.64000000e-01  2.32000000e-01 -2.01000000e-01  5.51000000e-01
  8.86000000e-01  9.98000000e-01 -5.78000000e-01  4.53000000e-01
 -8.04000000e-01 -2.65000000e-01 -9.90000000e-02  8.63000000e-01
 -2.91000000e-01  4.15000000e-01 -3.77000000e-01  1.33000000e-01
  8.13000000e-01  8.70000000e-02  7.50000000e-01  4.47000000e-01
 -1.81000000e-01 -6.61000000e-01  7.46000000e-01  6.90000000e-02
  1.07800000e+00 -6.60000000e-01 -9.30000000e-01 -9.50000000e-02
 -1.43000000e-01 -7.31000000e-01  7.40000000e-02  3.39000000e-01
 -6.50000000e-01  2.96000000e-01  2.45000000e-01 -8.20000000e-02
  3.35000000e-01  7.90000000e-02 -1.29000000e-01  7.72000000e-01
 -3.42000000e-01  6.44000000e-01 -1.39000000e-01  3.75000000e-01
  2.16000000e-01 -2.49000000e-01  5.31000000e-01  4.98000000e-01
  2.20000000e-02 -2.61000000e-01 -5.99000000e-01 -5.47000000e-01
 -7.20000000e-02 -1.08000000e-01  6.13000000e-01  3.88000000e-01
  3.90000000e-01  6.38000000e-01  1.63000000e-01 -3.83000000e-01
  3.00000000e-03  1.13000000e-01 -2.97000000e-01  6.28000000e-01
 -3.56000000e-01 -3.09000000e-01  7.18000000e-01 -2.02000000e-01
 -6.30000000e-02 -8.00000000e-02 -4.60000000e-02  2.41000000e-01
  7.27000000e-01 -2.74000000e-01 -6.45000000e-01  8.55000000e-01
  1.99000000e-01  3.35000000e-01  1.03100000e+00 -1.00000000e-01
 -4.60000000e-02 -3.40000000e-01  1.46000000e-01  2.04000000e-01
  4.11000000e-01 -6.51000000e-01 -3.44000000e-01  3.35000000e-01
 -5.24000000e-01 -5.13000000e-01  5.96000000e-01 -4.05000000e-01
  4.48000000e-01 -1.32100000e+00  1.69000000e-01  2.42000000e-01
  1.68000000e-01  6.96000000e-01  4.34000000e-01  5.14000000e-01
 -4.95000000e-01  1.23500000e+00 -7.77000000e-01  1.01200000e+00
 -4.20000000e-01  2.58000000e-01  3.81000000e-01 -1.04600000e+00
  7.32000000e-01  2.24000000e-01  3.90000000e-02  6.79000000e-01
 -1.40000000e-02 -5.35000000e-01  6.80000000e-02  2.62000000e-01
 -3.22000000e-01 -1.18000000e-01 -6.42000000e-01 -3.56000000e-01
  1.50000000e-01  5.64000000e-01 -1.94000000e-01  3.33000000e-01
 -4.38000000e-01 -3.04000000e-01  2.76000000e-01  4.09000000e-01
 -4.76000000e-01 -1.00000000e-01 -1.35000000e-01 -2.87000000e-01
 -4.49000000e-01 -4.28000000e-01 -2.74000000e-01 -4.90000000e-02
  1.04000000e-01  1.03000000e-01 -5.60000000e-02 -9.99000000e-01
  2.95000000e-01  6.10000000e-02  4.50000000e-02 -5.30000000e-02
 -6.44000000e-01  1.81000000e-01  5.09000000e-01 -2.99000000e-01
  1.98000000e-01  3.08000000e-01 -1.96000000e-01 -5.52000000e-01
 -9.00000000e-02 -2.99000000e-01  3.19000000e-01  3.17000000e-01
  7.59000000e-01 -5.34000000e-01  3.98000000e-01 -7.46000000e-01
  2.05000000e-01  4.47000000e-01  2.72000000e-01  5.90000000e-02
 -1.35000000e-01 -1.82000000e-01 -2.92000000e-01  1.36000000e-01
 -9.50000000e-02  3.05000000e-01  1.00000000e-03  4.58000000e-01
  2.50000000e-02 -1.85000000e-01  3.89000000e-01 -3.00000000e-03
  7.30000000e-01  4.95000000e-01  4.35000000e-01  5.64000000e-01
 -3.34000000e-01 -4.92000000e-01  1.44000000e-01  7.41000000e-01
 -3.02000000e-01  4.00000000e-01 -4.67000000e-01  0.00000000e+00
 -7.00000000e-02  1.95000000e-01  1.40000000e-02  3.47000000e-01
  3.82000000e-01  3.84000000e-01 -7.81000000e-01  2.93000000e-01
 -5.83000000e-01 -5.44000000e-01 -1.90000000e-01 -5.35000000e-01
  3.68000000e-01  2.72000000e-01 -4.08000000e-01  2.67000000e-01
 -8.50000000e-02  4.40000000e-02  1.96000000e-01  7.80000000e-02
  1.25100000e+00 -1.26000000e-01 -6.80000000e-02  3.03000000e-01
  1.77000000e-01  2.42000000e-01  4.18000000e-01  2.14000000e-01
  2.51000000e-01  3.31000000e-01  1.90000000e-02 -1.81000000e-01
 -8.29000000e-01  2.36000000e-01  1.60000000e-02  2.54000000e-01
 -7.90000000e-02  6.00000000e-03  3.97000000e-01  9.51000000e-01
  6.05000000e-01 -6.07000000e-01  3.80000000e-02  4.92000000e-01
 -6.29000000e-01 -2.23000000e-01 -3.69000000e-01 -5.10000000e-02
  6.25000000e-01 -4.00000000e-03 -5.52000000e-01 -7.71000000e-01
  5.10000000e-02  1.19100000e+00 -1.06200000e+00  1.86000000e-01
  3.50000000e-02 -5.32000000e-01 -1.01000000e-01 -6.56000000e-01
  4.83000000e-01 -4.05000000e-01  2.02000000e-01  4.71000000e-01
  3.73000000e-01  9.30000000e-02 -6.10000000e-01 -2.72000000e-01
  3.15000000e-01 -5.05000000e-01 -1.06000000e-01  4.50000000e-02
  6.90000000e-02 -6.80000000e-02  3.48000000e-01  4.90000000e-02
 -5.50000000e-02  5.88000000e-01 -5.37000000e-01 -3.01000000e-01
  1.31000000e-01  3.80000000e-02  3.59000000e-01 -3.99000000e-01
  2.70000000e-01  4.91000000e-01  1.68000000e-01  2.00000000e-01
  2.78000000e-01 -8.20000000e-02 -1.45000000e-01  2.59000000e-01
  4.72000000e-01] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  1.19000000e-01 -3.60000000e-02  4.26000000e-01
  1.67000000e-01 -4.56000000e-01  9.12000000e-01  4.56000000e-01
  4.57000000e-01 -2.29000000e-01 -6.00000000e-01  5.30000000e-02
  2.45000000e-01  5.47000000e-01  3.00000000e-03 -2.83000000e-01
  3.60000000e-01 -3.53000000e-01  2.21000000e-01  1.00000000e-03
  7.59000000e-01  2.92000000e-01 -8.70000000e-02  3.59000000e-01
  3.78000000e-01 -4.06000000e-01 -8.24000000e-01  4.61000000e-01
  3.93000000e-01 -1.96000000e-01  2.71000000e-01  4.74000000e-01
 -1.39000000e-01  1.02400000e+00 -3.14000000e-01 -3.85000000e-01
  9.20000000e-02 -5.61000000e-01 -3.53000000e-01  1.66000000e-01
 -1.25000000e-01 -1.94000000e-01  1.26900000e+00 -4.43000000e-01
 -9.90000000e-02 -6.96000000e-01  2.33000000e-01  3.49000000e-01
 -4.22000000e-01  3.11000000e-01  7.08000000e-01  1.73000000e-01
  9.06000000e-01  1.50000000e-02 -3.18000000e-01 -5.15000000e-01
 -2.72000000e-01 -6.40000000e-01  1.00000000e-03  3.72000000e-01
  3.37000000e-01 -1.40000000e+00  1.00000000e-02 -3.71000000e-01
 -8.50000000e-02 -6.14000000e-01 -5.00000000e-01  3.39000000e-01
  4.00000000e-02 -3.65000000e-01  7.77000000e-01  4.12000000e-01
  2.02000000e-01  1.77000000e-01 -2.84000000e-01 -3.29000000e-01
  1.36800000e+00 -1.36000000e-01  1.69000000e-01  1.17000000e-01
 -3.10000000e-02  7.47000000e-01 -2.58000000e-01 -1.57000000e-01
  4.62000000e-01 -2.90000000e-02 -5.01000000e-01 -3.54000000e-01
 -3.67000000e-01 -1.99000000e-01 -1.12000000e-01  4.60000000e-02
  5.07000000e-01 -2.26000000e-01  5.60000000e-02  4.70000000e-02
 -4.15000000e-01  4.01000000e-01  1.17000000e-01  4.55000000e-01
  2.12000000e-01  3.45000000e-01  2.29000000e-01  9.82000000e-01
 -5.95000000e-01 -1.83000000e-01 -3.51000000e-01  6.36000000e-01
  5.42000000e-01 -6.65000000e-01  6.40000000e-01 -8.11000000e-01
 -4.48000000e-01  1.69000000e-01  6.06000000e-01  4.08000000e-01
  6.26000000e-01  2.66000000e-01 -3.94000000e-01 -1.25000000e-01
  3.20000000e-01  3.66000000e-01 -3.64000000e-01  2.28000000e-01
 -1.55000000e-01 -4.55000000e-01  4.68000000e-01 -4.77000000e-01
 -3.50000000e-02  4.46000000e-01 -3.73000000e-01 -5.65000000e-01
 -6.03000000e-01  8.38000000e-01  4.60000000e-01 -7.20000000e-02
 -3.48000000e-01 -2.26000000e-01  4.00000000e-03 -2.87000000e-01
  3.30000000e-02 -2.89000000e-01 -3.73000000e-01  6.56000000e-01
  1.60000000e-02 -6.68000000e-01  1.20000000e-01 -8.90000000e-02
 -1.31000000e-01 -2.73000000e-01 -6.60000000e-02 -4.16000000e-01
  6.76000000e-01 -4.67000000e-01  3.31000000e-01  3.06000000e-01
  3.21000000e-01  2.10000000e-01 -2.38000000e-01 -7.86000000e-01
  4.94000000e-01 -2.17000000e-01  3.67000000e-01 -1.26000000e-01
 -2.04000000e-01  1.40000000e-02 -4.50000000e-02  8.61000000e-01
  6.05000000e-01 -5.10000000e-02  2.21000000e-01  1.12000000e-01
 -2.93000000e-01 -6.30000000e-01 -2.69000000e-01  3.92000000e-01
  2.53000000e-01 -2.31000000e-01 -2.82000000e-01  3.83000000e-01
  2.02000000e-01  2.22000000e-01  1.01000000e+00  1.90000000e-01
  8.40000000e-02  7.46000000e-01 -6.37000000e-01 -3.62000000e-01
 -6.04000000e-01 -7.68000000e-01  4.40000000e-02  3.82000000e-01
 -1.57000000e-01  4.67000000e-01 -3.59000000e-01  6.10000000e-01
 -6.84000000e-01 -1.51000000e-01  3.25000000e-01  1.97000000e-01
 -2.41000000e-01  5.70000000e-02  1.84000000e-01 -2.52000000e-01
  2.53000000e-01  5.80000000e-01 -6.70000000e-02 -9.24000000e-01
 -2.92000000e-01 -8.90000000e-02 -6.63000000e-01 -4.14000000e-01
 -4.36000000e-01  1.62000000e-01 -8.04000000e-01  3.83000000e-01
 -2.80000000e-02  3.67000000e-01 -2.05000000e-01 -5.43000000e-01
  1.56000000e-01  9.31000000e-01 -1.31000000e-01  5.95000000e-01
  1.27000000e-01 -5.83000000e-01  6.54000000e-01 -1.48000000e-01
  2.98000000e-01  3.81000000e-01  1.34000000e-01  9.00000000e-02
  1.73000000e-01  2.20000000e-01  1.89000000e-01  4.51000000e-01
 -9.32000000e-01 -4.30000000e-02  3.91000000e-01  2.49000000e-01
  1.90000000e-01 -2.47000000e-01  6.52000000e-01  7.20000000e-02
  1.70000000e-01 -5.82000000e-01  4.41000000e-01 -6.14000000e-01
  6.50000000e-01  2.06000000e-01  3.02000000e-01  1.11000000e-01
  3.31000000e-01 -1.87000000e-01 -1.04000000e-01  1.24000000e-01
 -4.76000000e-01  1.29000000e-01  2.87000000e-01 -3.24000000e-01
 -2.47000000e-01 -3.07000000e-01  4.50000000e-02  1.77000000e-01
 -4.80000000e-02  2.06000000e-01 -2.00000000e-01  3.72000000e-01
 -4.87000000e-01 -1.14000000e-01 -7.35000000e-01  4.00000000e-02
 -1.72000000e-01  4.01000000e-01 -7.11000000e-01  4.90000000e-01
  5.48000000e-01  4.25000000e-01 -4.47000000e-01  6.78000000e-01
  2.74000000e-01 -3.40000000e-02  2.33000000e-01  6.60000000e-02
 -5.23000000e-01  5.98000000e-01 -1.10000000e-02 -5.90000000e-02
 -3.50000000e-02  1.81000000e-01 -9.60000000e-02 -9.00000000e-03
 -4.64000000e-01  1.67000000e-01  1.82000000e-01  4.38000000e-01
  2.89000000e-01  6.26000000e-01  4.08000000e-01 -4.03000000e-01
 -4.44000000e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/09/2021 10:16:39 - INFO - data_loader -   *** Example ***
10/09/2021 10:16:39 - INFO - data_loader -   guid: test-4
10/09/2021 10:16:39 - INFO - data_loader -   tokens: [CLS] no flash th ##resh [SEP]
10/09/2021 10:16:39 - INFO - data_loader -   input_ids: 101 2053 5956 16215 21898 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   attention_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   intent_label: 4 (id = 4)
10/09/2021 10:16:39 - INFO - data_loader -   slot_labels: 0 4 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/09/2021 10:16:39 - INFO - data_loader -   ner_embeds: [ 1.46634594e-01 -5.26245758e-02 -9.95037891e-03 -9.02985111e-02
  1.22633725e-01 -1.46261543e-01  2.04309840e-02 -1.69067800e-01
  3.29489075e-02  1.92000000e-01  3.00000000e-02 -5.41000000e-01
 -2.49000000e-01  8.70000000e-02 -4.46000000e-01  3.70000000e-01
  4.68000000e-01 -1.50000000e-02 -3.35000000e-01  8.40000000e-02
  2.48000000e-01 -1.15000000e-01 -2.24000000e-01 -1.04000000e-01
 -2.34000000e-01 -4.87000000e-01  4.34000000e-01 -1.94000000e-01
 -4.70000000e-02  2.05000000e-01 -2.13000000e-01 -3.94000000e-01
  4.73000000e-01 -4.53000000e-01  3.11000000e-01 -1.39000000e-01
  5.00000000e-01 -2.43000000e-01 -6.13000000e-01  6.62000000e-01
  5.02000000e-01 -1.15000000e-01 -6.00000000e-02 -5.62000000e-01
  4.21000000e-01  1.01000000e-01 -4.60000000e-02  4.17000000e-01
 -1.92000000e-01 -4.67000000e-01  4.93000000e-01  4.42000000e-01
  1.30000000e-02 -2.28000000e-01 -2.03000000e-01 -1.41000000e-01
  2.71000000e-01  1.18000000e-01 -3.38000000e-01  3.52000000e-01
  2.73000000e-01 -4.71000000e-01 -5.99000000e-01 -4.63000000e-01
 -2.47000000e-01  4.56000000e-01  1.35000000e-01  9.40000000e-02
  1.00700000e+00  6.66000000e-01 -8.60000000e-02 -1.05000000e-01
 -1.01000000e+00  4.70000000e-02  7.10000000e-02 -6.45000000e-01
 -5.60000000e-02 -6.30000000e-02  1.00000000e-03  2.21000000e-01
  1.97000000e-01  2.00000000e-03  3.97000000e-01  5.38000000e-01
  9.70000000e-02 -1.10000000e-01  6.21000000e-01  3.56000000e-01
  4.54000000e-01  1.09000000e-01 -1.18000000e-01  8.80000000e-02
  1.50000000e-01 -3.39000000e-01 -1.92000000e-01 -4.03000000e-01
 -3.47000000e-01  2.93000000e-01  5.40000000e-02  1.51000000e-01
  3.64000000e-01  1.86000000e-01 -6.30000000e-01 -1.04000000e-01
 -7.13000000e-01  2.08000000e-01  4.06000000e-01 -3.94000000e-01
  6.60000000e-02  2.45000000e-01  4.08000000e-01  1.03300000e+00
  2.93000000e-01  9.50000000e-02 -8.00000000e-03  4.32000000e-01
 -3.22000000e-01 -5.65000000e-01 -9.60000000e-02 -3.08000000e-01
 -5.73000000e-01 -4.07000000e-01  2.75000000e-01  1.48000000e-01
 -1.80000000e-02  7.28000000e-01 -5.44000000e-01 -1.05200000e+00
  1.10000000e-02  5.48000000e-01 -1.49000000e-01  1.25000000e-01
 -3.13000000e-01 -3.50000000e-02  3.75000000e-01 -7.79000000e-01
 -3.37000000e-01  2.20000000e-02 -4.77000000e-01 -6.28000000e-01
  3.62000000e-01  6.60000000e-02  1.41000000e-01 -2.86000000e-01
  2.92000000e-01 -6.90000000e-02  1.10000000e-02 -5.11000000e-01
 -1.43000000e-01 -1.24000000e-01  5.52000000e-01 -2.40000000e-01
 -1.52000000e-01 -6.00000000e-03 -2.08000000e-01  8.30000000e-02
 -1.27000000e-01 -1.99000000e-01 -1.50000000e-01 -1.87000000e-01
 -3.20000000e-02 -3.70000000e-02 -2.48000000e-01 -3.86000000e-01
 -1.28500000e+00  2.65000000e-01 -4.45000000e-01 -1.56000000e-01
  3.11000000e-01  9.00000000e-03 -7.97000000e-01 -2.30000000e-02
 -1.27000000e-01 -5.87000000e-01  3.30000000e-01  4.85000000e-01
  4.66000000e-01 -6.44000000e-01 -3.77000000e-01 -2.55000000e-01
  7.00000000e-03 -2.55000000e-01  6.24000000e-01  2.89000000e-01
 -1.08000000e-01 -3.26000000e-01  3.38000000e-01 -2.09000000e-01
 -3.15000000e-01  6.17000000e-01 -7.30000000e-02 -1.43000000e-01
  3.33000000e-01 -1.95000000e-01  2.34000000e-01  8.35000000e-01
  1.35000000e-01 -2.53000000e-01  3.40000000e-02  5.29000000e-01
 -2.62000000e-01 -9.70000000e-02  2.98000000e-01  5.29000000e-01
  2.40000000e-02 -3.21000000e-01  4.54000000e-01 -4.95000000e-01
  4.05000000e-01 -3.84000000e-01  4.15000000e-01  5.03000000e-01
 -1.39000000e-01 -2.59000000e-01 -1.34000000e-01 -7.32000000e-01
 -2.70000000e-01 -1.26000000e-01  5.95000000e-01  2.06000000e-01
 -2.77000000e-01  6.90000000e-02 -1.01800000e+00 -2.20000000e-02
  2.90000000e-02 -2.99000000e-01  3.42000000e-01 -3.03000000e-01
 -8.00000000e-02  7.42000000e-01 -3.80000000e-02  1.46000000e-01
 -3.71000000e-01 -7.46000000e-01 -4.52000000e-01 -1.20000000e-02
  2.84000000e-01  3.18000000e-01 -7.60000000e-01 -2.80000000e-01
 -1.16000000e-01  6.60000000e-02  4.19000000e-01 -2.28000000e-01
  1.48000000e-01 -5.00000000e-02  3.20000000e-01 -9.70000000e-02
 -4.52000000e-01  7.45000000e-01 -9.40000000e-02  7.33000000e-01
 -4.37000000e-01  4.89000000e-01 -3.09000000e-01  2.21000000e-01
 -3.14000000e-01 -2.21000000e-01 -7.70000000e-01  7.30000000e-02
  5.16000000e-01  7.95000000e-01 -4.28000000e-01  2.95000000e-01
 -5.41000000e-01  5.65000000e-01 -3.18000000e-01 -3.05000000e-01
 -3.64000000e-01 -5.40000000e-02 -9.98000000e-01 -4.05000000e-01
 -3.30000000e-01 -2.52000000e-01 -4.55000000e-01 -1.16000000e-01
  1.40000000e-02  6.80000000e-02 -6.90000000e-02 -8.40000000e-02
  2.22000000e-01  4.01000000e-01 -6.30000000e-02  1.85000000e-01
  6.60000000e-01  1.52000000e-01 -9.00000000e-03  6.19000000e-01
 -3.46000000e-01  7.70000000e-02 -8.50000000e-02  6.60000000e-02
  5.58000000e-01  2.95000000e-01 -2.67000000e-01 -2.70000000e-01
 -7.50000000e-02  1.60000000e-01 -4.59000000e-01 -4.39000000e-01
 -2.71000000e-01  4.49000000e-01  3.45000000e-01 -2.73000000e-01
 -4.57000000e-01  2.84000000e-01 -3.90000000e-02 -2.00000000e-03
  8.70000000e-02] [ 0.22545928  0.14043103  0.05814986 -0.18108423 -0.04890471 -0.17474529
 -0.07501269 -0.08731554  0.07166796  0.006       0.114      -0.266
  0.01       -0.121       0.148       0.175      -0.024      -0.091
  0.202      -0.056      -0.105       0.152      -0.097      -0.027
  0.245      -0.214      -0.514      -0.061       0.158      -0.163
 -0.144      -0.064       0.01        0.275       0.394       0.46
 -0.157      -0.1        -0.04       -0.102      -0.107       0.162
  0.119      -0.015      -0.026      -0.039       0.329       0.407
  0.034      -0.197       0.123      -0.252       0.048       0.095
  0.145       0.133      -0.043      -0.002      -0.136       0.315
  0.212       0.09        0.072       0.102       0.102       0.002
  0.345       0.041      -0.005      -0.106       0.057      -0.045
  0.315      -0.225       0.328       0.298       0.236       0.207
  0.036      -0.19        0.064      -0.049       0.4        -0.037
  0.173       0.115      -0.038       0.111      -0.434      -0.444
 -0.208      -0.132       0.022      -0.401      -0.204      -0.121
  0.212      -0.29        0.248      -0.23       -0.003       0.07
 -0.13       -0.044      -0.004       0.069       0.134      -0.457
 -0.07       -0.145       0.089       0.045       0.128       0.001
  0.159      -0.04       -0.08       -0.252       0.114      -0.097
  0.396      -0.312      -0.052      -0.074      -0.045       0.143
  0.096       0.1         0.008       0.388       0.18        0.303
 -0.289       0.159       0.395       0.257      -0.025      -0.039
 -0.317      -0.16       -0.183       0.098      -0.303       0.081
 -0.391      -0.168      -0.186       0.354       0.084      -0.242
  0.136       0.4        -0.123       0.088       0.134       0.069
  0.049       0.133       0.313       0.27        0.146      -0.013
  0.049      -0.209      -0.387       0.263      -0.007      -0.399
  0.008       0.132       0.031       0.1         0.107       0.323
  0.082      -0.046      -0.478       0.111      -0.166       0.173
  0.042      -0.298      -0.003      -0.048      -0.112      -0.323
 -0.107       0.071       0.188      -0.234       0.312      -0.16
  0.247      -0.03        0.036      -0.157       0.026      -0.126
 -0.184       0.261      -0.329       0.272       0.356       0.093
 -0.16       -0.013      -0.178      -0.289       0.335       0.276
  0.084       0.27       -0.419      -0.052      -0.189      -0.253
  0.192       0.079      -0.108       0.028       0.188       0.036
  0.354       0.368      -0.005      -0.181       0.06       -0.105
  0.124       0.487       0.005       0.099       0.052      -0.096
 -0.537       0.073      -0.201       0.443       0.094      -0.413
  0.117      -0.267       0.108       0.204      -0.235      -0.366
  0.198      -0.157      -0.629      -0.202       0.067       0.042
 -0.183      -0.069       0.129       0.008      -0.026       0.011
 -0.099       0.441       0.284      -0.051       0.011      -0.027
 -0.099      -0.213      -0.061      -0.029      -0.099       0.197
 -0.095       0.224       0.086      -0.283       0.139      -0.227
 -0.084      -0.337      -0.246       0.055       0.034      -0.141
 -0.45       -0.253       0.239      -0.178      -0.073      -0.031
 -0.15        0.236       0.238       0.039       0.111       0.171
  0.14       -0.077       0.042      -0.406      -0.064       0.02
  0.246       0.087      -0.245      -0.15        0.29        0.228
 -0.056       0.007       0.037     ] [ 0.11639418  0.08558187  0.09059657 -0.02518909 -0.00108023 -0.16276515
  0.06848613 -0.19651432  0.05127958  0.251       0.132       0.483
 -0.087      -0.362       0.364       0.066      -0.111       0.228
 -0.012      -0.112      -0.235       0.107      -0.296       0.219
  0.032       0.145      -0.263       0.007      -0.179       0.146
  0.202      -0.411       0.308      -0.181      -0.608      -0.303
  0.01       -0.508      -0.367      -0.141      -0.022       0.154
  0.432      -0.294       0.16       -0.234       0.201       0.168
  0.518      -0.199       0.058      -0.249       0.169      -0.296
 -0.596       0.34       -0.29       -0.412       0.027      -0.055
  0.059       0.22        0.09        0.314       0.191       0.261
 -0.065       0.298      -0.018      -0.224       0.403      -0.015
  0.016      -0.297       0.322       0.223      -0.545       0.103
 -0.488       0.258       0.089      -0.187       0.064       0.365
 -0.005       0.422       0.086       0.079      -0.276       0.054
 -0.242      -0.102      -0.225      -0.336      -0.01       -0.527
 -0.349      -0.347       0.24       -0.253       0.377       0.379
 -0.105      -0.551       0.319       0.171       0.178       0.146
  0.298      -0.381       0.218      -0.042      -0.481      -0.444
 -0.283       0.109      -0.441      -0.088      -0.067       0.205
 -0.189      -0.208      -0.183       0.595      -0.146       0.72
 -0.003       0.113      -0.215      -0.541       0.214       0.034
 -0.086      -0.083      -0.201      -0.026      -0.041      -0.005
 -0.613      -0.105       0.443       0.109      -0.068       0.058
 -0.429      -0.502      -0.041       0.019       0.288      -0.356
 -0.027      -0.051       0.03       -0.391       0.253      -0.124
  0.113      -0.441      -0.118       0.153       0.033      -0.106
  0.232      -0.057      -0.468      -0.088       0.002      -0.251
 -0.213       0.256       0.059      -0.375       0.132      -0.052
  0.055      -0.404       0.172      -0.275       0.29        0.204
  0.128       0.246       0.353       0.433      -0.007      -0.427
  0.395      -0.288       0.817       0.321       0.474      -0.244
 -0.033      -0.22       -0.282      -0.346      -0.355       0.412
 -0.595      -0.078       0.121      -0.323       0.201       0.014
 -0.202      -0.244       0.208      -0.159       0.197      -0.355
  0.101      -0.108      -0.141       0.255       0.345       0.202
 -0.313      -0.396       0.01        0.223      -0.2        -0.222
  0.093       0.498      -0.433       0.024       0.171       0.017
  0.071       0.059       0.35       -0.218       0.46        0.12
 -0.098      -0.01        0.231       0.304       0.14        0.035
  0.065       0.027      -0.322       0.069      -0.623      -0.149
  0.498      -0.484       0.201       0.772       0.013       0.522
 -0.085      -0.392       0.13       -0.26        0.068      -0.235
 -0.561       0.247       0.196       0.021      -0.076       0.486
 -0.001       0.425      -0.25        0.49       -0.022      -0.02
  0.336      -0.003      -0.27       -0.511       0.157      -0.011
 -0.224      -0.145      -0.138       0.001      -0.094       0.031
  0.215      -0.182      -0.174      -0.065       0.272       0.035
 -0.206      -0.418       0.62        0.053       0.481       0.07
 -0.336       0.16        0.072      -0.18       -0.286      -0.058
  0.241      -0.077      -0.27        0.428       0.291       0.154
 -0.212       0.27       -0.039     ] [ 0.17355511  0.13193156  0.15399472 -0.06267814  0.02265351 -0.17596389
  0.06274267 -0.19548027  0.01290474 -0.153       0.06       -0.283
 -0.047      -0.245       0.433       0.068      -0.181      -0.205
  0.086       0.045       0.005      -0.156      -0.101       0.334
  0.24        0.146       0.12       -0.11        0.165       0.065
  0.325       0.06       -0.197       0.311      -0.29        0.278
  0.311      -0.251      -0.089      -0.272       0.003       0.15
  0.054      -0.453       0.105      -0.263       0.349      -0.148
  0.113      -0.381       0.37       -0.243      -0.519      -0.487
 -0.456       0.502       0.25        0.069       0.202      -0.186
 -0.24        0.227      -0.581      -0.5         0.251       0.068
 -0.096       0.094      -0.07       -0.294      -0.132      -0.365
 -0.359       0.062       0.082       0.215      -0.036      -0.593
 -0.059      -0.048       0.032      -0.744      -0.036       0.311
 -0.001      -0.359       0.197       0.141      -0.049       0.627
 -0.435       0.517       0.297      -0.415      -0.164      -0.171
  0.35       -0.38       -0.116      -0.179      -0.211      -0.77
 -0.214      -0.006      -0.637       0.397       0.536      -0.261
  0.194       0.122      -0.055       0.49       -0.432       0.202
 -0.075      -0.443      -0.238       0.692       0.203       0.357
 -0.321      -0.355       0.016      -0.154      -0.409       0.128
 -0.341      -0.524       0.118       0.251       0.08        0.274
 -0.117       0.175      -0.064       0.1         0.219       0.047
 -0.082      -0.102       0.196       0.307       0.003       0.223
  0.147      -0.141      -0.505       0.167       0.018      -0.062
  0.065      -0.129       0.221       0.027      -0.159       0.225
 -0.14       -0.371      -0.218       0.069      -0.014       0.224
  0.065       0.195      -0.17        0.187       0.11       -0.53
 -0.326       0.033      -0.012       0.227       0.238       0.532
  0.231      -0.001       0.663      -0.139       0.038       0.26
  0.473      -0.27        0.059       0.013      -0.065      -0.26
 -0.14       -0.074       0.153      -0.403      -0.366      -0.623
  0.123      -0.047       0.326       0.111       0.051       0.144
 -0.032       0.053      -0.145      -0.143       0.554      -0.08
  0.217       0.635      -0.074       0.071       0.381       0.323
  0.227      -0.246      -0.022       0.188       0.539       0.219
  0.159       0.197       0.011       0.239      -0.098       0.109
 -0.294       0.279      -0.125      -0.4         0.242       0.341
  0.226       0.225      -0.219       0.05        0.239      -0.499
  0.249       0.016      -0.042       0.076       0.182      -0.179
  0.063      -0.309      -0.373       0.176       0.352       0.186
  0.293       0.113      -0.173       0.222      -0.05       -0.053
 -0.204      -0.47        0.019       0.062      -0.2         0.232
 -0.387      -0.283       0.305       0.044      -0.065       0.086
  0.121       0.31       -0.352      -0.472       0.034      -0.057
 -0.21       -0.039       0.471      -0.233      -0.164      -0.372
  0.261       0.166       0.028      -0.454      -0.433      -0.007
  0.101      -0.005       0.128       0.406       0.179       0.041
 -0.229      -0.237       0.208       0.541       0.339       0.422
 -0.309      -0.074       0.165      -0.58       -0.015      -0.4
  0.239      -0.605       0.015       0.554       0.23       -0.205
 -0.255      -0.165      -0.085     ] [ 0.21875346  0.1611518   0.16730075  0.08056363  0.00323358 -0.24987771
  0.05385944 -0.22917029  0.06343329 -0.306       0.12       -0.566
 -0.094      -0.49        0.866       0.136      -0.362      -0.41
  0.172       0.09        0.01       -0.312      -0.202       0.668
  0.48        0.292       0.24       -0.22        0.33        0.13
  0.65        0.12       -0.394       0.622      -0.58        0.556
  0.622      -0.502      -0.178      -0.544       0.006       0.3
  0.108      -0.906       0.21       -0.526       0.698      -0.296
  0.226      -0.762       0.74       -0.486      -1.038      -0.974
 -0.912       1.004       0.5         0.138       0.404      -0.372
 -0.48        0.454      -1.162      -1.          0.502       0.136
 -0.192       0.188      -0.14       -0.588      -0.264      -0.73
 -0.718       0.124       0.164       0.43       -0.072      -1.186
 -0.118      -0.096       0.064      -1.488      -0.072       0.622
 -0.002      -0.718       0.394       0.282      -0.098       1.254
 -0.87        1.034       0.594      -0.83       -0.328      -0.342
  0.7        -0.76       -0.232      -0.358      -0.422      -1.54
 -0.428      -0.012      -1.274       0.794       1.072      -0.522
  0.388       0.244      -0.11        0.98       -0.864       0.404
 -0.15       -0.886      -0.476       1.384       0.406       0.714
 -0.642      -0.71        0.032      -0.308      -0.818       0.256
 -0.682      -1.048       0.236       0.502       0.16        0.548
 -0.234       0.35       -0.128       0.2         0.438       0.094
 -0.164      -0.204       0.392       0.614       0.006       0.446
  0.294      -0.282      -1.01        0.334       0.036      -0.124
  0.13       -0.258       0.442       0.054      -0.318       0.45
 -0.28       -0.742      -0.436       0.138      -0.028       0.448
  0.13        0.39       -0.34        0.374       0.22       -1.06
 -0.652       0.066      -0.024       0.454       0.476       1.064
  0.462      -0.002       1.326      -0.278       0.076       0.52
  0.946      -0.54        0.118       0.026      -0.13       -0.52
 -0.28       -0.148       0.306      -0.806      -0.732      -1.246
  0.246      -0.094       0.652       0.222       0.102       0.288
 -0.064       0.106      -0.29       -0.286       1.108      -0.16
  0.434       1.27       -0.148       0.142       0.762       0.646
  0.454      -0.492      -0.044       0.376       1.078       0.438
  0.318       0.394       0.022       0.478      -0.196       0.218
 -0.588       0.558      -0.25       -0.8         0.484       0.682
  0.452       0.45       -0.438       0.1         0.478      -0.998
  0.498       0.032      -0.084       0.152       0.364      -0.358
  0.126      -0.618      -0.746       0.352       0.704       0.372
  0.586       0.226      -0.346       0.444      -0.1        -0.106
 -0.408      -0.94        0.038       0.124      -0.4         0.464
 -0.774      -0.566       0.61        0.088      -0.13        0.172
  0.242       0.62       -0.704      -0.944       0.068      -0.114
 -0.42       -0.078       0.942      -0.466      -0.328      -0.744
  0.522       0.332       0.056      -0.908      -0.866      -0.014
  0.202      -0.01        0.256       0.812       0.358       0.082
 -0.458      -0.474       0.416       1.082       0.678       0.844
 -0.618      -0.148       0.33       -1.16       -0.03       -0.8
  0.478      -1.21        0.03        1.108       0.46       -0.41
 -0.51       -0.33       -0.17      ] [ 1.56437024e-01  2.44080275e-01  1.65596768e-01  7.75206313e-02
  3.03578172e-02 -1.92708120e-01  8.33905041e-02 -1.59223258e-01
 -6.26833811e-02  1.19000000e-01 -3.60000000e-02  4.26000000e-01
  1.67000000e-01 -4.56000000e-01  9.12000000e-01  4.56000000e-01
  4.57000000e-01 -2.29000000e-01 -6.00000000e-01  5.30000000e-02
  2.45000000e-01  5.47000000e-01  3.00000000e-03 -2.83000000e-01
  3.60000000e-01 -3.53000000e-01  2.21000000e-01  1.00000000e-03
  7.59000000e-01  2.92000000e-01 -8.70000000e-02  3.59000000e-01
  3.78000000e-01 -4.06000000e-01 -8.24000000e-01  4.61000000e-01
  3.93000000e-01 -1.96000000e-01  2.71000000e-01  4.74000000e-01
 -1.39000000e-01  1.02400000e+00 -3.14000000e-01 -3.85000000e-01
  9.20000000e-02 -5.61000000e-01 -3.53000000e-01  1.66000000e-01
 -1.25000000e-01 -1.94000000e-01  1.26900000e+00 -4.43000000e-01
 -9.90000000e-02 -6.96000000e-01  2.33000000e-01  3.49000000e-01
 -4.22000000e-01  3.11000000e-01  7.08000000e-01  1.73000000e-01
  9.06000000e-01  1.50000000e-02 -3.18000000e-01 -5.15000000e-01
 -2.72000000e-01 -6.40000000e-01  1.00000000e-03  3.72000000e-01
  3.37000000e-01 -1.40000000e+00  1.00000000e-02 -3.71000000e-01
 -8.50000000e-02 -6.14000000e-01 -5.00000000e-01  3.39000000e-01
  4.00000000e-02 -3.65000000e-01  7.77000000e-01  4.12000000e-01
  2.02000000e-01  1.77000000e-01 -2.84000000e-01 -3.29000000e-01
  1.36800000e+00 -1.36000000e-01  1.69000000e-01  1.17000000e-01
 -3.10000000e-02  7.47000000e-01 -2.58000000e-01 -1.57000000e-01
  4.62000000e-01 -2.90000000e-02 -5.01000000e-01 -3.54000000e-01
 -3.67000000e-01 -1.99000000e-01 -1.12000000e-01  4.60000000e-02
  5.07000000e-01 -2.26000000e-01  5.60000000e-02  4.70000000e-02
 -4.15000000e-01  4.01000000e-01  1.17000000e-01  4.55000000e-01
  2.12000000e-01  3.45000000e-01  2.29000000e-01  9.82000000e-01
 -5.95000000e-01 -1.83000000e-01 -3.51000000e-01  6.36000000e-01
  5.42000000e-01 -6.65000000e-01  6.40000000e-01 -8.11000000e-01
 -4.48000000e-01  1.69000000e-01  6.06000000e-01  4.08000000e-01
  6.26000000e-01  2.66000000e-01 -3.94000000e-01 -1.25000000e-01
  3.20000000e-01  3.66000000e-01 -3.64000000e-01  2.28000000e-01
 -1.55000000e-01 -4.55000000e-01  4.68000000e-01 -4.77000000e-01
 -3.50000000e-02  4.46000000e-01 -3.73000000e-01 -5.65000000e-01
 -6.03000000e-01  8.38000000e-01  4.60000000e-01 -7.20000000e-02
 -3.48000000e-01 -2.26000000e-01  4.00000000e-03 -2.87000000e-01
  3.30000000e-02 -2.89000000e-01 -3.73000000e-01  6.56000000e-01
  1.60000000e-02 -6.68000000e-01  1.20000000e-01 -8.90000000e-02
 -1.31000000e-01 -2.73000000e-01 -6.60000000e-02 -4.16000000e-01
  6.76000000e-01 -4.67000000e-01  3.31000000e-01  3.06000000e-01
  3.21000000e-01  2.10000000e-01 -2.38000000e-01 -7.86000000e-01
  4.94000000e-01 -2.17000000e-01  3.67000000e-01 -1.26000000e-01
 -2.04000000e-01  1.40000000e-02 -4.50000000e-02  8.61000000e-01
  6.05000000e-01 -5.10000000e-02  2.21000000e-01  1.12000000e-01
 -2.93000000e-01 -6.30000000e-01 -2.69000000e-01  3.92000000e-01
  2.53000000e-01 -2.31000000e-01 -2.82000000e-01  3.83000000e-01
  2.02000000e-01  2.22000000e-01  1.01000000e+00  1.90000000e-01
  8.40000000e-02  7.46000000e-01 -6.37000000e-01 -3.62000000e-01
 -6.04000000e-01 -7.68000000e-01  4.40000000e-02  3.82000000e-01
 -1.57000000e-01  4.67000000e-01 -3.59000000e-01  6.10000000e-01
 -6.84000000e-01 -1.51000000e-01  3.25000000e-01  1.97000000e-01
 -2.41000000e-01  5.70000000e-02  1.84000000e-01 -2.52000000e-01
  2.53000000e-01  5.80000000e-01 -6.70000000e-02 -9.24000000e-01
 -2.92000000e-01 -8.90000000e-02 -6.63000000e-01 -4.14000000e-01
 -4.36000000e-01  1.62000000e-01 -8.04000000e-01  3.83000000e-01
 -2.80000000e-02  3.67000000e-01 -2.05000000e-01 -5.43000000e-01
  1.56000000e-01  9.31000000e-01 -1.31000000e-01  5.95000000e-01
  1.27000000e-01 -5.83000000e-01  6.54000000e-01 -1.48000000e-01
  2.98000000e-01  3.81000000e-01  1.34000000e-01  9.00000000e-02
  1.73000000e-01  2.20000000e-01  1.89000000e-01  4.51000000e-01
 -9.32000000e-01 -4.30000000e-02  3.91000000e-01  2.49000000e-01
  1.90000000e-01 -2.47000000e-01  6.52000000e-01  7.20000000e-02
  1.70000000e-01 -5.82000000e-01  4.41000000e-01 -6.14000000e-01
  6.50000000e-01  2.06000000e-01  3.02000000e-01  1.11000000e-01
  3.31000000e-01 -1.87000000e-01 -1.04000000e-01  1.24000000e-01
 -4.76000000e-01  1.29000000e-01  2.87000000e-01 -3.24000000e-01
 -2.47000000e-01 -3.07000000e-01  4.50000000e-02  1.77000000e-01
 -4.80000000e-02  2.06000000e-01 -2.00000000e-01  3.72000000e-01
 -4.87000000e-01 -1.14000000e-01 -7.35000000e-01  4.00000000e-02
 -1.72000000e-01  4.01000000e-01 -7.11000000e-01  4.90000000e-01
  5.48000000e-01  4.25000000e-01 -4.47000000e-01  6.78000000e-01
  2.74000000e-01 -3.40000000e-02  2.33000000e-01  6.60000000e-02
 -5.23000000e-01  5.98000000e-01 -1.10000000e-02 -5.90000000e-02
 -3.50000000e-02  1.81000000e-01 -9.60000000e-02 -9.00000000e-03
 -4.64000000e-01  1.67000000e-01  1.82000000e-01  4.38000000e-01
  2.89000000e-01  6.26000000e-01  4.08000000e-01 -4.03000000e-01
 -4.44000000e-01] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
10/09/2021 10:16:41 - INFO - data_loader -   Saving features into cached file ./data\cached_test_low_distilbert-base-uncased_50
10/09/2021 10:16:50 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\Users\k3lan/.cache\torch\transformers\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c
10/09/2021 10:16:50 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "DistilBertForMaskedLM"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "low",
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 30522
}

10/09/2021 10:16:50 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at C:\Users\k3lan/.cache\torch\transformers\ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5
10/09/2021 10:16:51 - WARNING - transformers.modeling_utils -   Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing JointDistilBERT: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing JointDistilBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing JointDistilBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
10/09/2021 10:16:51 - WARNING - transformers.modeling_utils -   Some weights of JointDistilBERT were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['intent_classifier.linear.weight', 'intent_classifier.linear.bias', 'slot_classifier.linear.weight', 'slot_classifier.linear.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
True
NVIDIA GeForce RTX 2070 Super with Max-Q Design
Using device: cuda
10/09/2021 10:16:54 - INFO - trainer -   ***** Running training *****
10/09/2021 10:16:54 - INFO - trainer -     Num examples = 29358
10/09/2021 10:16:54 - INFO - trainer -     Num Epochs = 20
10/09/2021 10:16:54 - INFO - trainer -     Total train batch size = 32
10/09/2021 10:16:54 - INFO - trainer -     Gradient Accumulation steps = 1
10/09/2021 10:16:54 - INFO - trainer -     Total optimization steps = 18360
10/09/2021 10:16:54 - INFO - trainer -     Logging steps = 200
10/09/2021 10:16:54 - INFO - trainer -     Save steps = 200
Epoch:   0%|                                                                                    | 0/20 [00:00<?, ?it/s]10/09/2021 10:17:18 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 199/918 [00:23<01:23,  8.63it/s]
10/09/2021 10:17:18 - INFO - trainer -     Num examples = 3258
10/09/2021 10:17:18 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.91it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/09/2021 10:17:22 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:17:22 - INFO - trainer -     T-F1 = 0.950258222343028
10/09/2021 10:17:22 - INFO - trainer -     T-F1(C) = 0.8867132867132866
10/09/2021 10:17:22 - INFO - trainer -     T-F1(L) = 0.8590163934426229
10/09/2021 10:17:22 - INFO - trainer -     T-F1(O) = 0.9815397980236725
10/09/2021 10:17:22 - INFO - trainer -     T-F1(P) = 0.9880152002338499
10/09/2021 10:17:22 - INFO - trainer -     T-F1(S) = 0.9473206176203451
10/09/2021 10:17:22 - INFO - trainer -     T-F1(T) = 0.881118881118881
10/09/2021 10:17:22 - INFO - trainer -     U-F1(A) = 0.1020408163265306
10/09/2021 10:17:22 - INFO - trainer -     U-F1(E) = 0.7630662020905924
10/09/2021 10:17:22 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:17:22 - INFO - trainer -     U-F1(O) = 0.9568506102802132
10/09/2021 10:17:22 - INFO - trainer -     intent_acc = 0.9229588704726827
10/09/2021 10:17:22 - INFO - trainer -     loss = 0.3236449615043752
10/09/2021 10:17:22 - INFO - trainer -     semantic_frame_acc = 0.8403928790669122
10/09/2021 10:17:22 - INFO - trainer -     slot_f1 = 0.9474123539232053
10/09/2021 10:17:22 - INFO - trainer -     slot_precision = 0.9439977820903798
10/09/2021 10:17:22 - INFO - trainer -     slot_recall = 0.9508517173973751

10/09/2021 10:17:22 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:17:22 - INFO - trainer -     T-F1 = 0.950258222343028
10/09/2021 10:17:22 - INFO - trainer -     T-F1(C) = 0.8867132867132866
10/09/2021 10:17:22 - INFO - trainer -     T-F1(L) = 0.8590163934426229
10/09/2021 10:17:22 - INFO - trainer -     T-F1(O) = 0.9815397980236725
10/09/2021 10:17:22 - INFO - trainer -     T-F1(P) = 0.9880152002338499
10/09/2021 10:17:22 - INFO - trainer -     T-F1(S) = 0.9473206176203451
10/09/2021 10:17:22 - INFO - trainer -     T-F1(T) = 0.881118881118881
10/09/2021 10:17:22 - INFO - trainer -     U-F1(A) = 0.1020408163265306
10/09/2021 10:17:22 - INFO - trainer -     U-F1(E) = 0.7630662020905924
10/09/2021 10:17:22 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:17:22 - INFO - trainer -     U-F1(O) = 0.9568506102802132
10/09/2021 10:17:22 - INFO - trainer -     intent_acc = 0.9229588704726827
10/09/2021 10:17:22 - INFO - trainer -     semantic_frame_acc = 0.8403928790669122
10/09/2021 10:17:22 - INFO - trainer -     slot_f1 = 0.9474123539232053
10/09/2021 10:17:22 - INFO - trainer -     slot_precision = 0.9439977820903798
10/09/2021 10:17:22 - INFO - trainer -     slot_recall = 0.9508517173973751
10/09/2021 10:17:22 - INFO - transformers.configuration_utils -   Configuration saved in final_low_distilbert_de_model\config.json
10/09/2021 10:17:23 - INFO - transformers.modeling_utils -   Model weights saved in final_low_distilbert_de_model\pytorch_model.bin
10/09/2021 10:17:23 - INFO - trainer -   Saving model checkpoint to final_low_distilbert_de_model
Best model saved
                                                                                                                       10/09/2021 10:17:46 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 399/918 [00:51<01:03,  8.20it/s]
10/09/2021 10:17:46 - INFO - trainer -     Num examples = 3258
10/09/2021 10:17:46 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.17it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/09/2021 10:17:50 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:17:50 - INFO - trainer -     T-F1 = 0.970337261275904
10/09/2021 10:17:50 - INFO - trainer -     T-F1(C) = 0.9031377899045021
10/09/2021 10:17:50 - INFO - trainer -     T-F1(L) = 0.9158878504672897
10/09/2021 10:17:50 - INFO - trainer -     T-F1(O) = 0.9883107704017834
10/09/2021 10:17:50 - INFO - trainer -     T-F1(P) = 0.9932885906040267
10/09/2021 10:17:50 - INFO - trainer -     T-F1(S) = 0.9780420860018298
10/09/2021 10:17:50 - INFO - trainer -     T-F1(T) = 0.9301675977653632
10/09/2021 10:17:50 - INFO - trainer -     U-F1(A) = 0.7729468599033816
10/09/2021 10:17:50 - INFO - trainer -     U-F1(E) = 0.7272727272727272
10/09/2021 10:17:50 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:17:50 - INFO - trainer -     U-F1(O) = 0.9650008706251089
10/09/2021 10:17:50 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:17:50 - INFO - trainer -     loss = 0.2546832665947138
10/09/2021 10:17:50 - INFO - trainer -     semantic_frame_acc = 0.8852056476365868
10/09/2021 10:17:50 - INFO - trainer -     slot_f1 = 0.9685115827437926
10/09/2021 10:17:50 - INFO - trainer -     slot_precision = 0.9622381477398015
10/09/2021 10:17:50 - INFO - trainer -     slot_recall = 0.9748673554872941

10/09/2021 10:17:50 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:17:50 - INFO - trainer -     T-F1 = 0.950258222343028
10/09/2021 10:17:50 - INFO - trainer -     T-F1(C) = 0.8867132867132866
10/09/2021 10:17:50 - INFO - trainer -     T-F1(L) = 0.8590163934426229
10/09/2021 10:17:50 - INFO - trainer -     T-F1(O) = 0.9815397980236725
10/09/2021 10:17:50 - INFO - trainer -     T-F1(P) = 0.9880152002338499
10/09/2021 10:17:50 - INFO - trainer -     T-F1(S) = 0.9473206176203451
10/09/2021 10:17:50 - INFO - trainer -     T-F1(T) = 0.881118881118881
10/09/2021 10:17:50 - INFO - trainer -     U-F1(A) = 0.1020408163265306
10/09/2021 10:17:50 - INFO - trainer -     U-F1(E) = 0.7630662020905924
10/09/2021 10:17:50 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:17:50 - INFO - trainer -     U-F1(O) = 0.9568506102802132
10/09/2021 10:17:50 - INFO - trainer -     intent_acc = 0.9229588704726827
10/09/2021 10:17:50 - INFO - trainer -     semantic_frame_acc = 0.8403928790669122
10/09/2021 10:17:50 - INFO - trainer -     slot_f1 = 0.9474123539232053
10/09/2021 10:17:50 - INFO - trainer -     slot_precision = 0.9439977820903798
10/09/2021 10:17:50 - INFO - trainer -     slot_recall = 0.9508517173973751
                                                                                                                       10/09/2021 10:18:14 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 599/918 [01:19<00:37,  8.45it/s]
10/09/2021 10:18:14 - INFO - trainer -     Num examples = 3258
10/09/2021 10:18:14 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.62it/s]
10/09/2021 10:18:18 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:18:18 - INFO - trainer -     T-F1 = 0.9778018408229562 | 50/51 [00:03<00:00, 14.50it/s]
10/09/2021 10:18:18 - INFO - trainer -     T-F1(C) = 0.9324137931034483
10/09/2021 10:18:18 - INFO - trainer -     T-F1(L) = 0.9642857142857143
10/09/2021 10:18:18 - INFO - trainer -     T-F1(O) = 0.9910811398738307
10/09/2021 10:18:18 - INFO - trainer -     T-F1(P) = 0.9929824561403507
10/09/2021 10:18:18 - INFO - trainer -     T-F1(S) = 0.9799086757990867
10/09/2021 10:18:18 - INFO - trainer -     T-F1(T) = 0.9511854951185496
10/09/2021 10:18:18 - INFO - trainer -     U-F1(A) = 0.7884615384615384
10/09/2021 10:18:18 - INFO - trainer -     U-F1(E) = 0.7103174603174603
10/09/2021 10:18:18 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/09/2021 10:18:18 - INFO - trainer -     U-F1(O) = 0.9645956265185699
10/09/2021 10:18:18 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:18:18 - INFO - trainer -     loss = 0.20934241571847131
10/09/2021 10:18:18 - INFO - trainer -     semantic_frame_acc = 0.894413750767342
10/09/2021 10:18:18 - INFO - trainer -     slot_f1 = 0.9764542936288089
10/09/2021 10:18:18 - INFO - trainer -     slot_precision = 0.9686727122835944
10/09/2021 10:18:18 - INFO - trainer -     slot_recall = 0.984361910080983

10/09/2021 10:18:18 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:18:18 - INFO - trainer -     T-F1 = 0.9778018408229562
10/09/2021 10:18:18 - INFO - trainer -     T-F1(C) = 0.9324137931034483
10/09/2021 10:18:18 - INFO - trainer -     T-F1(L) = 0.9642857142857143
10/09/2021 10:18:18 - INFO - trainer -     T-F1(O) = 0.9910811398738307
10/09/2021 10:18:18 - INFO - trainer -     T-F1(P) = 0.9929824561403507
10/09/2021 10:18:18 - INFO - trainer -     T-F1(S) = 0.9799086757990867
10/09/2021 10:18:18 - INFO - trainer -     T-F1(T) = 0.9511854951185496
10/09/2021 10:18:18 - INFO - trainer -     U-F1(A) = 0.7884615384615384
10/09/2021 10:18:18 - INFO - trainer -     U-F1(E) = 0.7103174603174603
10/09/2021 10:18:18 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/09/2021 10:18:18 - INFO - trainer -     U-F1(O) = 0.9645956265185699
10/09/2021 10:18:18 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:18:18 - INFO - trainer -     semantic_frame_acc = 0.894413750767342
10/09/2021 10:18:18 - INFO - trainer -     slot_f1 = 0.9764542936288089
10/09/2021 10:18:18 - INFO - trainer -     slot_precision = 0.9686727122835944
10/09/2021 10:18:18 - INFO - trainer -     slot_recall = 0.984361910080983
10/09/2021 10:18:18 - INFO - transformers.configuration_utils -   Configuration saved in final_low_distilbert_de_model\config.json
10/09/2021 10:18:18 - INFO - transformers.modeling_utils -   Model weights saved in final_low_distilbert_de_model\pytorch_model.bin
10/09/2021 10:18:18 - INFO - trainer -   Saving model checkpoint to final_low_distilbert_de_model
Best model saved
                                                                                                                       10/09/2021 10:18:42 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 799/918 [01:47<00:14,  8.24it/s]
10/09/2021 10:18:42 - INFO - trainer -     Num examples = 3258
10/09/2021 10:18:42 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.23it/s]
10/09/2021 10:18:46 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:18:46 - INFO - trainer -     T-F1 = 0.9810600899305082 | 50/51 [00:03<00:00, 14.26it/s]
10/09/2021 10:18:46 - INFO - trainer -     T-F1(C) = 0.9570200573065902
10/09/2021 10:18:46 - INFO - trainer -     T-F1(L) = 0.9788519637462235
10/09/2021 10:18:46 - INFO - trainer -     T-F1(O) = 0.9924608124966101
10/09/2021 10:18:46 - INFO - trainer -     T-F1(P) = 0.9956024626209323
10/09/2021 10:18:46 - INFO - trainer -     T-F1(S) = 0.9781420765027322
10/09/2021 10:18:46 - INFO - trainer -     T-F1(T) = 0.9445234708392602
10/09/2021 10:18:46 - INFO - trainer -     U-F1(A) = 0.7632850241545893
10/09/2021 10:18:46 - INFO - trainer -     U-F1(E) = 0.7661016949152543
10/09/2021 10:18:46 - INFO - trainer -     U-F1(I) = 0.06451612903225806
10/09/2021 10:18:46 - INFO - trainer -     U-F1(O) = 0.9648382559774965
10/09/2021 10:18:46 - INFO - trainer -     intent_acc = 0.9361571516267649
10/09/2021 10:18:46 - INFO - trainer -     loss = 0.21970467013763448
10/09/2021 10:18:46 - INFO - trainer -     semantic_frame_acc = 0.9030079803560467
10/09/2021 10:18:46 - INFO - trainer -     slot_f1 = 0.9803484320557491
10/09/2021 10:18:46 - INFO - trainer -     slot_precision = 0.9785754034501948
10/09/2021 10:18:46 - INFO - trainer -     slot_recall = 0.982127897235409

10/09/2021 10:18:46 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:18:46 - INFO - trainer -     T-F1 = 0.9778018408229562
10/09/2021 10:18:46 - INFO - trainer -     T-F1(C) = 0.9324137931034483
10/09/2021 10:18:46 - INFO - trainer -     T-F1(L) = 0.9642857142857143
10/09/2021 10:18:46 - INFO - trainer -     T-F1(O) = 0.9910811398738307
10/09/2021 10:18:46 - INFO - trainer -     T-F1(P) = 0.9929824561403507
10/09/2021 10:18:46 - INFO - trainer -     T-F1(S) = 0.9799086757990867
10/09/2021 10:18:46 - INFO - trainer -     T-F1(T) = 0.9511854951185496
10/09/2021 10:18:46 - INFO - trainer -     U-F1(A) = 0.7884615384615384
10/09/2021 10:18:46 - INFO - trainer -     U-F1(E) = 0.7103174603174603
10/09/2021 10:18:46 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/09/2021 10:18:46 - INFO - trainer -     U-F1(O) = 0.9645956265185699
10/09/2021 10:18:46 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:18:46 - INFO - trainer -     semantic_frame_acc = 0.894413750767342
10/09/2021 10:18:46 - INFO - trainer -     slot_f1 = 0.9764542936288089
10/09/2021 10:18:46 - INFO - trainer -     slot_precision = 0.9686727122835944
10/09/2021 10:18:46 - INFO - trainer -     slot_recall = 0.984361910080983
Iteration: 100%|| 918/918 [02:05<00:00,  7.31it/s]
Epoch:   5%|                                                                       | 1/20 [02:05<39:46, 125.62s/it]10/09/2021 10:19:10 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 81/918 [00:09<01:39,  8.43it/s]
10/09/2021 10:19:10 - INFO - trainer -     Num examples = 3258
10/09/2021 10:19:10 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.41it/s]
10/09/2021 10:19:14 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:19:14 - INFO - trainer -     T-F1 = 0.9832630289835352 | 50/51 [00:03<00:00, 14.35it/s]
10/09/2021 10:19:14 - INFO - trainer -     T-F1(C) = 0.9483960948396094
10/09/2021 10:19:14 - INFO - trainer -     T-F1(L) = 0.9792284866468843
10/09/2021 10:19:14 - INFO - trainer -     T-F1(O) = 0.9933250122103436
10/09/2021 10:19:14 - INFO - trainer -     T-F1(P) = 0.9938757655293088
10/09/2021 10:19:14 - INFO - trainer -     T-F1(S) = 0.9897769516728624
10/09/2021 10:19:14 - INFO - trainer -     T-F1(T) = 0.9495798319327731
10/09/2021 10:19:14 - INFO - trainer -     U-F1(A) = 0.13861386138613863
10/09/2021 10:19:14 - INFO - trainer -     U-F1(E) = 0.7607573149741825
10/09/2021 10:19:14 - INFO - trainer -     U-F1(I) = 0.06896551724137931
10/09/2021 10:19:14 - INFO - trainer -     U-F1(O) = 0.9564168819982775
10/09/2021 10:19:14 - INFO - trainer -     intent_acc = 0.9223449969306323
10/09/2021 10:19:14 - INFO - trainer -     loss = 0.20664124190807343
10/09/2021 10:19:14 - INFO - trainer -     semantic_frame_acc = 0.8922651933701657
10/09/2021 10:19:14 - INFO - trainer -     slot_f1 = 0.9821677347450544
10/09/2021 10:19:14 - INFO - trainer -     slot_precision = 0.9799833194328608
10/09/2021 10:19:14 - INFO - trainer -     slot_recall = 0.984361910080983

10/09/2021 10:19:14 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:19:14 - INFO - trainer -     T-F1 = 0.9778018408229562
10/09/2021 10:19:14 - INFO - trainer -     T-F1(C) = 0.9324137931034483
10/09/2021 10:19:14 - INFO - trainer -     T-F1(L) = 0.9642857142857143
10/09/2021 10:19:14 - INFO - trainer -     T-F1(O) = 0.9910811398738307
10/09/2021 10:19:14 - INFO - trainer -     T-F1(P) = 0.9929824561403507
10/09/2021 10:19:14 - INFO - trainer -     T-F1(S) = 0.9799086757990867
10/09/2021 10:19:14 - INFO - trainer -     T-F1(T) = 0.9511854951185496
10/09/2021 10:19:14 - INFO - trainer -     U-F1(A) = 0.7884615384615384
10/09/2021 10:19:14 - INFO - trainer -     U-F1(E) = 0.7103174603174603
10/09/2021 10:19:14 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/09/2021 10:19:14 - INFO - trainer -     U-F1(O) = 0.9645956265185699
10/09/2021 10:19:14 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:19:14 - INFO - trainer -     semantic_frame_acc = 0.894413750767342
10/09/2021 10:19:14 - INFO - trainer -     slot_f1 = 0.9764542936288089
10/09/2021 10:19:14 - INFO - trainer -     slot_precision = 0.9686727122835944
10/09/2021 10:19:14 - INFO - trainer -     slot_recall = 0.984361910080983
                                                                                                                       10/09/2021 10:19:38 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 281/918 [00:37<01:17,  8.24it/s]
10/09/2021 10:19:38 - INFO - trainer -     Num examples = 3258
10/09/2021 10:19:38 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.34it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/09/2021 10:19:42 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:19:42 - INFO - trainer -     T-F1 = 0.9846571622539037
10/09/2021 10:19:42 - INFO - trainer -     T-F1(C) = 0.96045197740113
10/09/2021 10:19:42 - INFO - trainer -     T-F1(L) = 0.9820359281437125
10/09/2021 10:19:42 - INFO - trainer -     T-F1(O) = 0.9938623648905547
10/09/2021 10:19:42 - INFO - trainer -     T-F1(P) = 0.9958992384299942
10/09/2021 10:19:42 - INFO - trainer -     T-F1(S) = 0.9880184331797235
10/09/2021 10:19:42 - INFO - trainer -     T-F1(T) = 0.9472259810554803
10/09/2021 10:19:42 - INFO - trainer -     U-F1(A) = 0.7715736040609137
10/09/2021 10:19:42 - INFO - trainer -     U-F1(E) = 0.7668393782383419
10/09/2021 10:19:42 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:19:42 - INFO - trainer -     U-F1(O) = 0.9658673201470331
10/09/2021 10:19:42 - INFO - trainer -     intent_acc = 0.9383057090239411
10/09/2021 10:19:42 - INFO - trainer -     loss = 0.20200444575326115
10/09/2021 10:19:42 - INFO - trainer -     semantic_frame_acc = 0.9097605893186004
10/09/2021 10:19:42 - INFO - trainer -     slot_f1 = 0.9837477427420477
10/09/2021 10:19:42 - INFO - trainer -     slot_precision = 0.9787175234936429
10/09/2021 10:19:42 - INFO - trainer -     slot_recall = 0.9888299357721307

10/09/2021 10:19:42 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:19:42 - INFO - trainer -     T-F1 = 0.9778018408229562
10/09/2021 10:19:42 - INFO - trainer -     T-F1(C) = 0.9324137931034483
10/09/2021 10:19:42 - INFO - trainer -     T-F1(L) = 0.9642857142857143
10/09/2021 10:19:42 - INFO - trainer -     T-F1(O) = 0.9910811398738307
10/09/2021 10:19:42 - INFO - trainer -     T-F1(P) = 0.9929824561403507
10/09/2021 10:19:42 - INFO - trainer -     T-F1(S) = 0.9799086757990867
10/09/2021 10:19:42 - INFO - trainer -     T-F1(T) = 0.9511854951185496
10/09/2021 10:19:42 - INFO - trainer -     U-F1(A) = 0.7884615384615384
10/09/2021 10:19:42 - INFO - trainer -     U-F1(E) = 0.7103174603174603
10/09/2021 10:19:42 - INFO - trainer -     U-F1(I) = 0.23809523809523808
10/09/2021 10:19:42 - INFO - trainer -     U-F1(O) = 0.9645956265185699
10/09/2021 10:19:42 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:19:42 - INFO - trainer -     semantic_frame_acc = 0.894413750767342
10/09/2021 10:19:42 - INFO - trainer -     slot_f1 = 0.9764542936288089
10/09/2021 10:19:42 - INFO - trainer -     slot_precision = 0.9686727122835944
10/09/2021 10:19:42 - INFO - trainer -     slot_recall = 0.984361910080983
                                                                                                                       10/09/2021 10:20:06 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 481/918 [01:05<00:52,  8.35it/s]
10/09/2021 10:20:06 - INFO - trainer -     Num examples = 3258
10/09/2021 10:20:06 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.24it/s]
10/09/2021 10:20:10 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:20:10 - INFO - trainer -     T-F1 = 0.9866339334424441 | 50/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:20:10 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:20:10 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:20:10 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:20:10 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:20:10 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:20:10 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:20:10 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:20:10 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:20:10 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:20:10 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:20:10 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:20:10 - INFO - trainer -     loss = 0.2016651245454947
10/09/2021 10:20:10 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:20:10 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:20:10 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:20:10 - INFO - trainer -     slot_recall = 0.9860374197151633

10/09/2021 10:20:10 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:20:10 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:20:10 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:20:10 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:20:10 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:20:10 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:20:10 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:20:10 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:20:10 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:20:10 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:20:10 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:20:10 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:20:10 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:20:10 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:20:10 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:20:10 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:20:10 - INFO - trainer -     slot_recall = 0.9860374197151633
10/09/2021 10:20:10 - INFO - transformers.configuration_utils -   Configuration saved in final_low_distilbert_de_model\config.json
10/09/2021 10:20:10 - INFO - transformers.modeling_utils -   Model weights saved in final_low_distilbert_de_model\pytorch_model.bin
10/09/2021 10:20:10 - INFO - trainer -   Saving model checkpoint to final_low_distilbert_de_model
Best model saved
                                                                                                                       10/09/2021 10:20:34 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 681/918 [01:33<00:28,  8.39it/s]
10/09/2021 10:20:34 - INFO - trainer -     Num examples = 3258
10/09/2021 10:20:34 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.28it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/09/2021 10:20:38 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:20:38 - INFO - trainer -     T-F1 = 0.9856889736949707
10/09/2021 10:20:38 - INFO - trainer -     T-F1(C) = 0.9643366619115549
10/09/2021 10:20:38 - INFO - trainer -     T-F1(L) = 0.9819277108433735
10/09/2021 10:20:38 - INFO - trainer -     T-F1(O) = 0.9943055480232118
10/09/2021 10:20:38 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:20:38 - INFO - trainer -     T-F1(S) = 0.9893567792688569
10/09/2021 10:20:38 - INFO - trainer -     T-F1(T) = 0.9602272727272726
10/09/2021 10:20:38 - INFO - trainer -     U-F1(A) = 0.7649769585253456
10/09/2021 10:20:38 - INFO - trainer -     U-F1(E) = 0.6915113871635611
10/09/2021 10:20:38 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:20:38 - INFO - trainer -     U-F1(O) = 0.9649334945586459
10/09/2021 10:20:38 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:20:38 - INFO - trainer -     loss = 0.21242439155192935
10/09/2021 10:20:38 - INFO - trainer -     semantic_frame_acc = 0.9076120319214241
10/09/2021 10:20:38 - INFO - trainer -     slot_f1 = 0.984375
10/09/2021 10:20:38 - INFO - trainer -     slot_precision = 0.9835517145246724
10/09/2021 10:20:38 - INFO - trainer -     slot_recall = 0.9851996648980732

10/09/2021 10:20:38 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:20:38 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:20:38 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:20:38 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:20:38 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:20:38 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:20:38 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:20:38 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:20:38 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:20:38 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:20:38 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:20:38 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:20:38 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:20:38 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:20:38 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:20:38 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:20:38 - INFO - trainer -     slot_recall = 0.9860374197151633
                                                                                                                       10/09/2021 10:21:02 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 881/918 [02:01<00:04,  8.36it/s]
10/09/2021 10:21:02 - INFO - trainer -     Num examples = 3258
10/09/2021 10:21:02 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.22it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/09/2021 10:21:06 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:21:06 - INFO - trainer -     T-F1 = 0.9877384196185286
10/09/2021 10:21:06 - INFO - trainer -     T-F1(C) = 0.9714285714285715
10/09/2021 10:21:06 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:21:06 - INFO - trainer -     T-F1(O) = 0.995118246908223
10/09/2021 10:21:06 - INFO - trainer -     T-F1(P) = 0.9956101843722563
10/09/2021 10:21:06 - INFO - trainer -     T-F1(S) = 0.9880624426078971
10/09/2021 10:21:06 - INFO - trainer -     T-F1(T) = 0.9661971830985916
10/09/2021 10:21:06 - INFO - trainer -     U-F1(A) = 0.783410138248848
10/09/2021 10:21:06 - INFO - trainer -     U-F1(E) = 0.7814113597246128
10/09/2021 10:21:06 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:21:06 - INFO - trainer -     U-F1(O) = 0.9678439641539273
10/09/2021 10:21:06 - INFO - trainer -     intent_acc = 0.9410681399631676
10/09/2021 10:21:06 - INFO - trainer -     loss = 0.1942318288572863
10/09/2021 10:21:06 - INFO - trainer -     semantic_frame_acc = 0.9183548189073051
10/09/2021 10:21:06 - INFO - trainer -     slot_f1 = 0.9870419395290511
10/09/2021 10:21:06 - INFO - trainer -     slot_precision = 0.9849833147942157
10/09/2021 10:21:06 - INFO - trainer -     slot_recall = 0.9891091873778274

10/09/2021 10:21:06 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:21:06 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:21:06 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:21:06 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:21:06 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:21:06 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:21:06 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:21:06 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:21:06 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:21:06 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:21:06 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:21:06 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:21:06 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:21:06 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:21:06 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:21:06 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:21:06 - INFO - trainer -     slot_recall = 0.9860374197151633
Iteration: 100%|| 918/918 [02:10<00:00,  7.04it/s]
Epoch:  10%|                                                                   | 2/20 [04:15<38:31, 128.39s/it]10/09/2021 10:21:30 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 163/918 [00:19<01:30,  8.34it/s]
10/09/2021 10:21:30 - INFO - trainer -     Num examples = 3258
10/09/2021 10:21:30 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.23it/s]
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
C:\Users\k3lan\anaconda3\envs\jbert\lib\site-packages\sklearn\metrics\_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
10/09/2021 10:21:34 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:21:34 - INFO - trainer -     T-F1 = 0.9885745375408052
10/09/2021 10:21:34 - INFO - trainer -     T-F1(C) = 0.9645390070921986
10/09/2021 10:21:34 - INFO - trainer -     T-F1(L) = 0.9852507374631269
10/09/2021 10:21:34 - INFO - trainer -     T-F1(O) = 0.9954407294832827
10/09/2021 10:21:34 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/09/2021 10:21:34 - INFO - trainer -     T-F1(S) = 0.9912402028584602
10/09/2021 10:21:34 - INFO - trainer -     T-F1(T) = 0.9653259361997226
10/09/2021 10:21:34 - INFO - trainer -     U-F1(A) = 0.7884615384615384
10/09/2021 10:21:34 - INFO - trainer -     U-F1(E) = 0.7697594501718213
10/09/2021 10:21:34 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:21:34 - INFO - trainer -     U-F1(O) = 0.9664853483067205
10/09/2021 10:21:34 - INFO - trainer -     intent_acc = 0.9392265193370166
10/09/2021 10:21:34 - INFO - trainer -     loss = 0.1912136897444725
10/09/2021 10:21:34 - INFO - trainer -     semantic_frame_acc = 0.91804788213628
10/09/2021 10:21:34 - INFO - trainer -     slot_f1 = 0.9880356149137451
10/09/2021 10:21:34 - INFO - trainer -     slot_precision = 0.9844746326587192
10/09/2021 10:21:34 - INFO - trainer -     slot_recall = 0.991622451829098

10/09/2021 10:21:34 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:21:34 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:21:34 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:21:34 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:21:34 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:21:34 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:21:34 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:21:34 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:21:34 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:21:34 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:21:34 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:21:34 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:21:34 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:21:34 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:21:34 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:21:34 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:21:34 - INFO - trainer -     slot_recall = 0.9860374197151633
                                                                                                                       10/09/2021 10:21:58 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 363/918 [00:47<01:07,  8.27it/s]
10/09/2021 10:21:58 - INFO - trainer -     Num examples = 3258
10/09/2021 10:21:58 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.13it/s]
10/09/2021 10:22:02 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:22:02 - INFO - trainer -     T-F1 = 0.9856212696690179 | 50/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:22:02 - INFO - trainer -     T-F1(C) = 0.9501385041551247
10/09/2021 10:22:02 - INFO - trainer -     T-F1(L) = 0.9823529411764705
10/09/2021 10:22:02 - INFO - trainer -     T-F1(O) = 0.9942403825255379
10/09/2021 10:22:02 - INFO - trainer -     T-F1(P) = 0.9956101843722563
10/09/2021 10:22:02 - INFO - trainer -     T-F1(S) = 0.9944341372912802
10/09/2021 10:22:02 - INFO - trainer -     T-F1(T) = 0.9497964721845319
10/09/2021 10:22:02 - INFO - trainer -     U-F1(A) = 0.7924528301886793
10/09/2021 10:22:02 - INFO - trainer -     U-F1(E) = 0.7689594356261023
10/09/2021 10:22:02 - INFO - trainer -     U-F1(I) = 0.0
10/09/2021 10:22:02 - INFO - trainer -     U-F1(O) = 0.9672447013487476
10/09/2021 10:22:02 - INFO - trainer -     intent_acc = 0.9401473296500921
10/09/2021 10:22:02 - INFO - trainer -     loss = 0.1946276505203808
10/09/2021 10:22:02 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/09/2021 10:22:02 - INFO - trainer -     slot_f1 = 0.9848716169326857
10/09/2021 10:22:02 - INFO - trainer -     slot_precision = 0.9790286975717439
10/09/2021 10:22:02 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:22:02 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:22:02 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:22:02 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:22:02 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:22:02 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:22:02 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:22:02 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:22:02 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:22:02 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:22:02 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:22:02 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:22:02 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:22:02 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:22:02 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:22:02 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:22:02 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:22:02 - INFO - trainer -     slot_recall = 0.9860374197151633
                                                                                                                       10/09/2021 10:22:26 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 563/918 [01:15<00:42,  8.27it/s]
10/09/2021 10:22:26 - INFO - trainer -     Num examples = 3258
10/09/2021 10:22:26 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.18it/s]
10/09/2021 10:22:30 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:22:30 - INFO - trainer -     T-F1 = 0.9890859481582537 | 50/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:22:30 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:22:30 - INFO - trainer -     T-F1(L) = 0.9791044776119404
10/09/2021 10:22:30 - INFO - trainer -     T-F1(O) = 0.9957714409628103
10/09/2021 10:22:30 - INFO - trainer -     T-F1(P) = 0.9967826849956127
10/09/2021 10:22:30 - INFO - trainer -     T-F1(S) = 0.9935245143385754
10/09/2021 10:22:30 - INFO - trainer -     T-F1(T) = 0.9676511954992968
10/09/2021 10:22:30 - INFO - trainer -     U-F1(A) = 0.7757009345794392
10/09/2021 10:22:30 - INFO - trainer -     U-F1(E) = 0.7676419965576592
10/09/2021 10:22:30 - INFO - trainer -     U-F1(I) = 0.3157894736842105
10/09/2021 10:22:30 - INFO - trainer -     U-F1(O) = 0.9646892655367231
10/09/2021 10:22:30 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:22:30 - INFO - trainer -     loss = 0.19251394848905357
10/09/2021 10:22:30 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:22:30 - INFO - trainer -     slot_f1 = 0.988698200083717
10/09/2021 10:22:30 - INFO - trainer -     slot_precision = 0.9880089235917456
10/09/2021 10:22:30 - INFO - trainer -     slot_recall = 0.9893884389835241

10/09/2021 10:22:30 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:22:30 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:22:30 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:22:30 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:22:30 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:22:30 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:22:30 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:22:30 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:22:30 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:22:30 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:22:30 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:22:30 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:22:30 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:22:30 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:22:30 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:22:30 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:22:30 - INFO - trainer -     slot_recall = 0.9860374197151633
                                                                                                                       10/09/2021 10:22:54 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 763/918 [01:43<00:18,  8.36it/s]
10/09/2021 10:22:54 - INFO - trainer -     Num examples = 3258
10/09/2021 10:22:54 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.13it/s]
10/09/2021 10:22:58 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:22:58 - INFO - trainer -     T-F1 = 0.9900477164280845 | 50/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:22:58 - INFO - trainer -     T-F1(C) = 0.9699570815450644
10/09/2021 10:22:58 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:22:58 - INFO - trainer -     T-F1(O) = 0.996041429423567
10/09/2021 10:22:58 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:22:58 - INFO - trainer -     T-F1(S) = 0.993984266543267
10/09/2021 10:22:58 - INFO - trainer -     T-F1(T) = 0.9722222222222221
10/09/2021 10:22:58 - INFO - trainer -     U-F1(A) = 0.7772511848341231
10/09/2021 10:22:58 - INFO - trainer -     U-F1(E) = 0.7695035460992907
10/09/2021 10:22:58 - INFO - trainer -     U-F1(I) = 0.27906976744186046
10/09/2021 10:22:58 - INFO - trainer -     U-F1(O) = 0.9666549666549666
10/09/2021 10:22:58 - INFO - trainer -     intent_acc = 0.9389195825659914
10/09/2021 10:22:58 - INFO - trainer -     loss = 0.19027692203720412
10/09/2021 10:22:58 - INFO - trainer -     semantic_frame_acc = 0.9208103130755064
10/09/2021 10:22:58 - INFO - trainer -     slot_f1 = 0.9894002789400278
10/09/2021 10:22:58 - INFO - trainer -     slot_precision = 0.9882975759264419
10/09/2021 10:22:58 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:22:58 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:22:58 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:22:58 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:22:58 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:22:58 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:22:58 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:22:58 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:22:58 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:22:58 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:22:58 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:22:58 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:22:58 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:22:58 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:22:58 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:22:58 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:22:58 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:22:58 - INFO - trainer -     slot_recall = 0.9860374197151633
Iteration: 100%|| 918/918 [02:06<00:00,  7.27it/s]
Epoch:  15%|                                                               | 3/20 [06:22<36:06, 127.41s/it]10/09/2021 10:23:22 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 45/918 [00:05<01:45,  8.30it/s]
10/09/2021 10:23:22 - INFO - trainer -     Num examples = 3258
10/09/2021 10:23:22 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.17it/s]
10/09/2021 10:23:26 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:23:26 - INFO - trainer -     T-F1 = 0.9886936384688735 | 50/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:23:26 - INFO - trainer -     T-F1(C) = 0.9683908045977011
10/09/2021 10:23:26 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:23:26 - INFO - trainer -     T-F1(O) = 0.995497694602658
10/09/2021 10:23:26 - INFO - trainer -     T-F1(P) = 0.9953325554259044
10/09/2021 10:23:26 - INFO - trainer -     T-F1(S) = 0.9898148148148149
10/09/2021 10:23:26 - INFO - trainer -     T-F1(T) = 0.975
10/09/2021 10:23:26 - INFO - trainer -     U-F1(A) = 0.7216494845360824
10/09/2021 10:23:26 - INFO - trainer -     U-F1(E) = 0.7577854671280277
10/09/2021 10:23:26 - INFO - trainer -     U-F1(I) = 0.1111111111111111
10/09/2021 10:23:26 - INFO - trainer -     U-F1(O) = 0.9625087596355992
10/09/2021 10:23:26 - INFO - trainer -     intent_acc = 0.9324739103744628
10/09/2021 10:23:26 - INFO - trainer -     loss = 0.20356190226533832
10/09/2021 10:23:26 - INFO - trainer -     semantic_frame_acc = 0.9100675260896255
10/09/2021 10:23:26 - INFO - trainer -     slot_f1 = 0.9880122665179817
10/09/2021 10:23:26 - INFO - trainer -     slot_precision = 0.986362371277484
10/09/2021 10:23:26 - INFO - trainer -     slot_recall = 0.9896676905892209

10/09/2021 10:23:26 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:23:26 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:23:26 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:23:26 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:23:26 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:23:26 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:23:26 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:23:26 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:23:26 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:23:26 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:23:26 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:23:26 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:23:26 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:23:26 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:23:26 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:23:26 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:23:26 - INFO - trainer -     slot_recall = 0.9860374197151633
                                                                                                                       10/09/2021 10:23:50 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 245/918 [00:33<01:21,  8.29it/s]
10/09/2021 10:23:50 - INFO - trainer -     Num examples = 3258
10/09/2021 10:23:50 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.19it/s]
10/09/2021 10:23:54 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:23:54 - INFO - trainer -     T-F1 = 0.989623156744948 | 50/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:23:54 - INFO - trainer -     T-F1(C) = 0.9715099715099714
10/09/2021 10:23:54 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:23:54 - INFO - trainer -     T-F1(O) = 0.995989594623889
10/09/2021 10:23:54 - INFO - trainer -     T-F1(P) = 0.995605039554644
10/09/2021 10:23:54 - INFO - trainer -     T-F1(S) = 0.993077988001846
10/09/2021 10:23:54 - INFO - trainer -     T-F1(T) = 0.9702127659574469
10/09/2021 10:23:54 - INFO - trainer -     U-F1(A) = 0.75
10/09/2021 10:23:54 - INFO - trainer -     U-F1(E) = 0.6781115879828327
10/09/2021 10:23:54 - INFO - trainer -     U-F1(I) = 0.10810810810810811
10/09/2021 10:23:54 - INFO - trainer -     U-F1(O) = 0.9623258214347153
10/09/2021 10:23:54 - INFO - trainer -     intent_acc = 0.9306322897483118
10/09/2021 10:23:54 - INFO - trainer -     loss = 0.22622634506985254
10/09/2021 10:23:54 - INFO - trainer -     semantic_frame_acc = 0.9125230202578268
10/09/2021 10:23:54 - INFO - trainer -     slot_f1 = 0.9892473118279571
10/09/2021 10:23:54 - INFO - trainer -     slot_precision = 0.9893854748603352
10/09/2021 10:23:54 - INFO - trainer -     slot_recall = 0.9891091873778274

10/09/2021 10:23:54 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:23:54 - INFO - trainer -     T-F1 = 0.9866339334424441
10/09/2021 10:23:54 - INFO - trainer -     T-F1(C) = 0.966325036603221
10/09/2021 10:23:54 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:23:54 - INFO - trainer -     T-F1(O) = 0.9947950553025373
10/09/2021 10:23:54 - INFO - trainer -     T-F1(P) = 0.9933120093050305
10/09/2021 10:23:54 - INFO - trainer -     T-F1(S) = 0.9893862482694971
10/09/2021 10:23:54 - INFO - trainer -     T-F1(T) = 0.9661016949152543
10/09/2021 10:23:54 - INFO - trainer -     U-F1(A) = 0.5271317829457364
10/09/2021 10:23:54 - INFO - trainer -     U-F1(E) = 0.7413127413127413
10/09/2021 10:23:54 - INFO - trainer -     U-F1(I) = 0.33962264150943394
10/09/2021 10:23:54 - INFO - trainer -     U-F1(O) = 0.9607977991746905
10/09/2021 10:23:54 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:23:54 - INFO - trainer -     semantic_frame_acc = 0.9051565377532228
10/09/2021 10:23:54 - INFO - trainer -     slot_f1 = 0.9856245638520585
10/09/2021 10:23:54 - INFO - trainer -     slot_precision = 0.9852120535714286
10/09/2021 10:23:54 - INFO - trainer -     slot_recall = 0.9860374197151633
                                                                                                                       10/09/2021 10:24:18 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 445/918 [01:01<00:56,  8.38it/s]
10/09/2021 10:24:18 - INFO - trainer -     Num examples = 3258
10/09/2021 10:24:18 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:24:22 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:24:22 - INFO - trainer -     T-F1 = 0.987892803700177 | 50/51 [00:03<00:00, 13.61it/s]
10/09/2021 10:24:22 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:24:22 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:24:22 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:24:22 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:24:22 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:24:22 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:24:22 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:24:22 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:24:22 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:24:22 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:24:22 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:24:22 - INFO - trainer -     loss = 0.21968809398365954
10/09/2021 10:24:22 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:24:22 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:24:22 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:24:22 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:24:22 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:24:22 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:24:22 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:24:22 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:24:22 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:24:22 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:24:22 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:24:22 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:24:22 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:24:22 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:24:22 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:24:22 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:24:22 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:24:22 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:24:22 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:24:22 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:24:22 - INFO - trainer -     slot_recall = 0.9907846970120078
10/09/2021 10:24:22 - INFO - transformers.configuration_utils -   Configuration saved in final_low_distilbert_de_model\config.json
10/09/2021 10:24:23 - INFO - transformers.modeling_utils -   Model weights saved in final_low_distilbert_de_model\pytorch_model.bin
10/09/2021 10:24:23 - INFO - trainer -   Saving model checkpoint to final_low_distilbert_de_model
Best model saved
                                                                                                                       10/09/2021 10:24:47 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 645/918 [01:30<00:32,  8.29it/s]
10/09/2021 10:24:47 - INFO - trainer -     Num examples = 3258
10/09/2021 10:24:47 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.16it/s]
10/09/2021 10:24:51 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:24:51 - INFO - trainer -     T-F1 = 0.9900639716891249 | 50/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:24:51 - INFO - trainer -     T-F1(C) = 0.9689265536723164
10/09/2021 10:24:51 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/09/2021 10:24:51 - INFO - trainer -     T-F1(O) = 0.9961473764176025
10/09/2021 10:24:51 - INFO - trainer -     T-F1(P) = 0.9965034965034966
10/09/2021 10:24:51 - INFO - trainer -     T-F1(S) = 0.9925650557620818
10/09/2021 10:24:51 - INFO - trainer -     T-F1(T) = 0.9762900976290098
10/09/2021 10:24:51 - INFO - trainer -     U-F1(A) = 0.7195767195767195
10/09/2021 10:24:51 - INFO - trainer -     U-F1(E) = 0.7741935483870969
10/09/2021 10:24:51 - INFO - trainer -     U-F1(I) = 0.2
10/09/2021 10:24:51 - INFO - trainer -     U-F1(O) = 0.9641979641979642
10/09/2021 10:24:51 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:24:51 - INFO - trainer -     loss = 0.19133380639786815
10/09/2021 10:24:51 - INFO - trainer -     semantic_frame_acc = 0.9168201350521793
10/09/2021 10:24:51 - INFO - trainer -     slot_f1 = 0.9895557721765771
10/09/2021 10:24:51 - INFO - trainer -     slot_precision = 0.9869444444444444
10/09/2021 10:24:51 - INFO - trainer -     slot_recall = 0.9921809550404915

10/09/2021 10:24:51 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:24:51 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:24:51 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:24:51 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:24:51 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:24:51 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:24:51 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:24:51 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:24:51 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:24:51 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:24:51 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:24:51 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:24:51 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:24:51 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:24:51 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:24:51 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:24:51 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:25:15 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 845/918 [01:58<00:08,  8.23it/s]
10/09/2021 10:25:15 - INFO - trainer -     Num examples = 3258
10/09/2021 10:25:15 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.15it/s]
10/09/2021 10:25:19 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:25:19 - INFO - trainer -     T-F1 = 0.9904554131442597 | 50/51 [00:03<00:00, 14.10it/s]
10/09/2021 10:25:19 - INFO - trainer -     T-F1(C) = 0.9712643678160919
10/09/2021 10:25:19 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:25:19 - INFO - trainer -     T-F1(O) = 0.9962043162346818
10/09/2021 10:25:19 - INFO - trainer -     T-F1(P) = 0.996780801872988
10/09/2021 10:25:19 - INFO - trainer -     T-F1(S) = 0.993077988001846
10/09/2021 10:25:19 - INFO - trainer -     T-F1(T) = 0.9735006973500697
10/09/2021 10:25:19 - INFO - trainer -     U-F1(A) = 0.708994708994709
10/09/2021 10:25:19 - INFO - trainer -     U-F1(E) = 0.789655172413793
10/09/2021 10:25:19 - INFO - trainer -     U-F1(I) = 0.22727272727272727
10/09/2021 10:25:19 - INFO - trainer -     U-F1(O) = 0.9654567771348413
10/09/2021 10:25:19 - INFO - trainer -     intent_acc = 0.9373848987108656
10/09/2021 10:25:19 - INFO - trainer -     loss = 0.20463823066914782
10/09/2021 10:25:19 - INFO - trainer -     semantic_frame_acc = 0.9186617556783303
10/09/2021 10:25:19 - INFO - trainer -     slot_f1 = 0.9900990099009901
10/09/2021 10:25:19 - INFO - trainer -     slot_precision = 0.9888579387186629
10/09/2021 10:25:19 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:25:19 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:25:19 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:25:19 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:25:19 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:25:19 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:25:19 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:25:19 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:25:19 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:25:19 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:25:19 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:25:19 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:25:19 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:25:19 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:25:19 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:25:19 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:25:19 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:25:19 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:11<00:00,  7.00it/s]
Epoch:  20%|                                                            | 4/20 [08:33<34:22, 128.91s/it]10/09/2021 10:25:43 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 127/918 [00:15<01:35,  8.29it/s]
10/09/2021 10:25:43 - INFO - trainer -     Num examples = 3258
10/09/2021 10:25:43 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.12it/s]
10/09/2021 10:25:47 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:25:47 - INFO - trainer -     T-F1 = 0.9923329682365826 | 50/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:25:47 - INFO - trainer -     T-F1(C) = 0.9782923299565845
10/09/2021 10:25:47 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:25:47 - INFO - trainer -     T-F1(O) = 0.9969683845820702
10/09/2021 10:25:47 - INFO - trainer -     T-F1(P) = 0.996487119437939
10/09/2021 10:25:47 - INFO - trainer -     T-F1(S) = 0.9949003245248029
10/09/2021 10:25:47 - INFO - trainer -     T-F1(T) = 0.9787234042553191
10/09/2021 10:25:47 - INFO - trainer -     U-F1(A) = 0.7076923076923077
10/09/2021 10:25:47 - INFO - trainer -     U-F1(E) = 0.7787934186471663
10/09/2021 10:25:47 - INFO - trainer -     U-F1(I) = 0.22857142857142856
10/09/2021 10:25:47 - INFO - trainer -     U-F1(O) = 0.9667189405819829
10/09/2021 10:25:47 - INFO - trainer -     intent_acc = 0.9392265193370166
10/09/2021 10:25:47 - INFO - trainer -     loss = 0.22177312676520908
10/09/2021 10:25:47 - INFO - trainer -     semantic_frame_acc = 0.925414364640884
10/09/2021 10:25:47 - INFO - trainer -     slot_f1 = 0.9920179246604116
10/09/2021 10:25:47 - INFO - trainer -     slot_precision = 0.994943820224719
10/09/2021 10:25:47 - INFO - trainer -     slot_recall = 0.9891091873778274

10/09/2021 10:25:47 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:25:47 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:25:47 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:25:47 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:25:47 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:25:47 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:25:47 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:25:47 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:25:47 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:25:47 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:25:47 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:25:47 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:25:47 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:25:47 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:25:47 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:25:47 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:25:47 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:26:11 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 327/918 [00:43<01:10,  8.34it/s]
10/09/2021 10:26:11 - INFO - trainer -     Num examples = 3258
10/09/2021 10:26:11 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.12it/s]
10/09/2021 10:26:15 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:26:15 - INFO - trainer -     T-F1 = 0.9907129199672222 | 50/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:26:15 - INFO - trainer -     T-F1(C) = 0.972818311874106
10/09/2021 10:26:15 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:26:15 - INFO - trainer -     T-F1(O) = 0.9963151620244934
10/09/2021 10:26:15 - INFO - trainer -     T-F1(P) = 0.9964891749561146
10/09/2021 10:26:15 - INFO - trainer -     T-F1(S) = 0.9939675174013921
10/09/2021 10:26:15 - INFO - trainer -     T-F1(T) = 0.9733520336605891
10/09/2021 10:26:15 - INFO - trainer -     U-F1(A) = 0.6842105263157896
10/09/2021 10:26:15 - INFO - trainer -     U-F1(E) = 0.7594936708860759
10/09/2021 10:26:15 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:26:15 - INFO - trainer -     U-F1(O) = 0.9633763515870248
10/09/2021 10:26:15 - INFO - trainer -     intent_acc = 0.9330877839165131
10/09/2021 10:26:15 - INFO - trainer -     loss = 0.24276674443892404
10/09/2021 10:26:15 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/09/2021 10:26:15 - INFO - trainer -     slot_f1 = 0.9903590890037725
10/09/2021 10:26:15 - INFO - trainer -     slot_precision = 0.9910514541387024
10/09/2021 10:26:15 - INFO - trainer -     slot_recall = 0.9896676905892209

10/09/2021 10:26:15 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:26:15 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:26:15 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:26:15 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:26:15 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:26:15 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:26:15 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:26:15 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:26:15 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:26:15 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:26:15 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:26:15 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:26:15 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:26:15 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:26:15 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:26:15 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:26:15 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:26:40 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 527/918 [01:11<00:47,  8.25it/s]
10/09/2021 10:26:40 - INFO - trainer -     Num examples = 3258
10/09/2021 10:26:40 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.11it/s]
10/09/2021 10:26:44 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:26:44 - INFO - trainer -     T-F1 = 0.9881713120326309 | 50/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:26:44 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/09/2021 10:26:44 - INFO - trainer -     T-F1(L) = 0.9736070381231672
10/09/2021 10:26:44 - INFO - trainer -     T-F1(O) = 0.9952771293632268
10/09/2021 10:26:44 - INFO - trainer -     T-F1(P) = 0.9962065946892326
10/09/2021 10:26:44 - INFO - trainer -     T-F1(S) = 0.9903181189488244
10/09/2021 10:26:44 - INFO - trainer -     T-F1(T) = 0.9692737430167597
10/09/2021 10:26:44 - INFO - trainer -     U-F1(A) = 0.6444444444444444
10/09/2021 10:26:44 - INFO - trainer -     U-F1(E) = 0.7063063063063063
10/09/2021 10:26:44 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:26:44 - INFO - trainer -     U-F1(O) = 0.9585510275165448
10/09/2021 10:26:44 - INFO - trainer -     intent_acc = 0.923572744014733
10/09/2021 10:26:44 - INFO - trainer -     loss = 0.2518765631259656
10/09/2021 10:26:44 - INFO - trainer -     semantic_frame_acc = 0.9005524861878453
10/09/2021 10:26:44 - INFO - trainer -     slot_f1 = 0.9876234181615908
10/09/2021 10:26:44 - INFO - trainer -     slot_precision = 0.9836565096952908
10/09/2021 10:26:44 - INFO - trainer -     slot_recall = 0.991622451829098

10/09/2021 10:26:44 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:26:44 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:26:44 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:26:44 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:26:44 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:26:44 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:26:44 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:26:44 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:26:44 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:26:44 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:26:44 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:26:44 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:26:44 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:26:44 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:26:44 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:26:44 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:26:44 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:27:08 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 727/918 [01:39<00:23,  8.28it/s]
10/09/2021 10:27:08 - INFO - trainer -     Num examples = 3258
10/09/2021 10:27:08 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.10it/s]
10/09/2021 10:27:12 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:27:12 - INFO - trainer -     T-F1 = 0.9904475982532751 | 50/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:27:12 - INFO - trainer -     T-F1(C) = 0.9658119658119658
10/09/2021 10:27:12 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:27:12 - INFO - trainer -     T-F1(O) = 0.9962055507372073
10/09/2021 10:27:12 - INFO - trainer -     T-F1(P) = 0.9961954931226222
10/09/2021 10:27:12 - INFO - trainer -     T-F1(S) = 0.9935125115848007
10/09/2021 10:27:12 - INFO - trainer -     T-F1(T) = 0.9762237762237763
10/09/2021 10:27:12 - INFO - trainer -     U-F1(A) = 0.7411167512690354
10/09/2021 10:27:12 - INFO - trainer -     U-F1(E) = 0.7504761904761904
10/09/2021 10:27:12 - INFO - trainer -     U-F1(I) = 0.12903225806451613
10/09/2021 10:27:12 - INFO - trainer -     U-F1(O) = 0.9661634565330558
10/09/2021 10:27:12 - INFO - trainer -     intent_acc = 0.937998772252916
10/09/2021 10:27:12 - INFO - trainer -     loss = 0.23232939927016988
10/09/2021 10:27:12 - INFO - trainer -     semantic_frame_acc = 0.9186617556783303
10/09/2021 10:27:12 - INFO - trainer -     slot_f1 = 0.9899497487437187
10/09/2021 10:27:12 - INFO - trainer -     slot_precision = 0.9896734579960926
10/09/2021 10:27:12 - INFO - trainer -     slot_recall = 0.9902261938006144

10/09/2021 10:27:12 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:27:12 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:27:12 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:27:12 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:27:12 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:27:12 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:27:12 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:27:12 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:27:12 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:27:12 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:27:12 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:27:12 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:27:12 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:27:12 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:27:12 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:27:12 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:27:12 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.24it/s]
Epoch:  25%|                                                        | 5/20 [10:40<32:01, 128.13s/it]10/09/2021 10:27:36 - INFO - trainer -   ***** Running evaluation on dev dataset *****  | 9/918 [00:01<01:49,  8.33it/s]
10/09/2021 10:27:36 - INFO - trainer -     Num examples = 3258
10/09/2021 10:27:36 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.07it/s]
10/09/2021 10:27:40 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:27:40 - INFO - trainer -     T-F1 = 0.9909934497816593 | 50/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:27:40 - INFO - trainer -     T-F1(C) = 0.9673758865248228
10/09/2021 10:27:40 - INFO - trainer -     T-F1(L) = 0.9940119760479043
10/09/2021 10:27:40 - INFO - trainer -     T-F1(O) = 0.9964223764093669
10/09/2021 10:27:40 - INFO - trainer -     T-F1(P) = 0.9959016393442623
10/09/2021 10:27:40 - INFO - trainer -     T-F1(S) = 0.995366079703429
10/09/2021 10:27:40 - INFO - trainer -     T-F1(T) = 0.9762237762237763
10/09/2021 10:27:40 - INFO - trainer -     U-F1(A) = 0.7326732673267328
10/09/2021 10:27:40 - INFO - trainer -     U-F1(E) = 0.7791304347826087
10/09/2021 10:27:40 - INFO - trainer -     U-F1(I) = 0.32653061224489793
10/09/2021 10:27:40 - INFO - trainer -     U-F1(O) = 0.9652021089630931
10/09/2021 10:27:40 - INFO - trainer -     intent_acc = 0.9367710251688153
10/09/2021 10:27:40 - INFO - trainer -     loss = 0.21915343031287193
10/09/2021 10:27:40 - INFO - trainer -     semantic_frame_acc = 0.9189686924493554
10/09/2021 10:27:40 - INFO - trainer -     slot_f1 = 0.9905080960357341
10/09/2021 10:27:40 - INFO - trainer -     slot_precision = 0.9902316494557634
10/09/2021 10:27:40 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:27:40 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:27:40 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:27:40 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:27:40 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:27:40 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:27:40 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:27:40 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:27:40 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:27:40 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:27:40 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:27:40 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:27:40 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:27:40 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:27:40 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:27:40 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:27:40 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:27:40 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:28:04 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 209/918 [00:29<01:25,  8.32it/s]
10/09/2021 10:28:04 - INFO - trainer -     Num examples = 3258
10/09/2021 10:28:04 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.13it/s]
10/09/2021 10:28:08 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:28:08 - INFO - trainer -     T-F1 = 0.9888405008165487 | 50/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:28:08 - INFO - trainer -     T-F1(C) = 0.9590973201692525
10/09/2021 10:28:08 - INFO - trainer -     T-F1(L) = 0.9881656804733727
10/09/2021 10:28:08 - INFO - trainer -     T-F1(O) = 0.995658780117213
10/09/2021 10:28:08 - INFO - trainer -     T-F1(P) = 0.9964891749561146
10/09/2021 10:28:08 - INFO - trainer -     T-F1(S) = 0.9930651872399444
10/09/2021 10:28:08 - INFO - trainer -     T-F1(T) = 0.9694444444444444
10/09/2021 10:28:08 - INFO - trainer -     U-F1(A) = 0.75
10/09/2021 10:28:08 - INFO - trainer -     U-F1(E) = 0.7871198568872988
10/09/2021 10:28:08 - INFO - trainer -     U-F1(I) = 0.2142857142857143
10/09/2021 10:28:08 - INFO - trainer -     U-F1(O) = 0.9657954744781617
10/09/2021 10:28:08 - INFO - trainer -     intent_acc = 0.9373848987108656
10/09/2021 10:28:08 - INFO - trainer -     loss = 0.25425014568164067
10/09/2021 10:28:08 - INFO - trainer -     semantic_frame_acc = 0.9158993247391037
10/09/2021 10:28:08 - INFO - trainer -     slot_f1 = 0.9881697981906751
10/09/2021 10:28:08 - INFO - trainer -     slot_precision = 0.9850166481687015
10/09/2021 10:28:08 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:28:08 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:28:08 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:28:08 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:28:08 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:28:08 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:28:08 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:28:08 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:28:08 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:28:08 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:28:08 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:28:08 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:28:08 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:28:08 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:28:08 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:28:08 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:28:08 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:28:08 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:28:32 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 409/918 [00:57<01:01,  8.30it/s]
10/09/2021 10:28:32 - INFO - trainer -     Num examples = 3258
10/09/2021 10:28:32 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:28:36 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:28:36 - INFO - trainer -     T-F1 = 0.9894966580275543 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:28:36 - INFO - trainer -     T-F1(C) = 0.9645390070921986
10/09/2021 10:28:36 - INFO - trainer -     T-F1(L) = 0.9881656804733727
10/09/2021 10:28:36 - INFO - trainer -     T-F1(O) = 0.9959338574139334
10/09/2021 10:28:36 - INFO - trainer -     T-F1(P) = 0.9953216374269005
10/09/2021 10:28:36 - INFO - trainer -     T-F1(S) = 0.9935064935064934
10/09/2021 10:28:36 - INFO - trainer -     T-F1(T) = 0.9747191011235954
10/09/2021 10:28:36 - INFO - trainer -     U-F1(A) = 0.6984126984126984
10/09/2021 10:28:36 - INFO - trainer -     U-F1(E) = 0.7532956685499058
10/09/2021 10:28:36 - INFO - trainer -     U-F1(I) = 0.15
10/09/2021 10:28:36 - INFO - trainer -     U-F1(O) = 0.9652536483669215
10/09/2021 10:28:36 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:28:36 - INFO - trainer -     loss = 0.2603354931023775
10/09/2021 10:28:36 - INFO - trainer -     semantic_frame_acc = 0.9149785144260283
10/09/2021 10:28:36 - INFO - trainer -     slot_f1 = 0.9889772568717733
10/09/2021 10:28:36 - INFO - trainer -     slot_precision = 0.9882877858337981
10/09/2021 10:28:36 - INFO - trainer -     slot_recall = 0.9896676905892209

10/09/2021 10:28:36 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:28:36 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:28:36 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:28:36 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:28:36 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:28:36 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:28:36 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:28:36 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:28:36 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:28:36 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:28:36 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:28:36 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:28:36 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:28:36 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:28:36 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:28:36 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:28:36 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:29:00 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 609/918 [01:25<00:37,  8.24it/s]
10/09/2021 10:29:00 - INFO - trainer -     Num examples = 3258
10/09/2021 10:29:00 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:29:04 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:29:04 - INFO - trainer -     T-F1 = 0.9888252929953665 | 50/51 [00:03<00:00, 13.93it/s]
10/09/2021 10:29:04 - INFO - trainer -     T-F1(C) = 0.9632768361581922
10/09/2021 10:29:04 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/09/2021 10:29:04 - INFO - trainer -     T-F1(O) = 0.9957696062479661
10/09/2021 10:29:04 - INFO - trainer -     T-F1(P) = 0.9956076134699853
10/09/2021 10:29:04 - INFO - trainer -     T-F1(S) = 0.9921332716335031
10/09/2021 10:29:04 - INFO - trainer -     T-F1(T) = 0.9721448467966575
10/09/2021 10:29:04 - INFO - trainer -     U-F1(A) = 0.6979166666666666
10/09/2021 10:29:04 - INFO - trainer -     U-F1(E) = 0.76
10/09/2021 10:29:04 - INFO - trainer -     U-F1(I) = 0.11764705882352941
10/09/2021 10:29:04 - INFO - trainer -     U-F1(O) = 0.9648083623693379
10/09/2021 10:29:04 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:29:04 - INFO - trainer -     loss = 0.2637876829534185
10/09/2021 10:29:04 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:29:04 - INFO - trainer -     slot_f1 = 0.9882910510175634
10/09/2021 10:29:04 - INFO - trainer -     slot_precision = 0.9866406902310048
10/09/2021 10:29:04 - INFO - trainer -     slot_recall = 0.9899469421949176

10/09/2021 10:29:04 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:29:04 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:29:04 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:29:04 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:29:04 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:29:04 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:29:04 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:29:04 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:29:04 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:29:04 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:29:04 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:29:04 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:29:04 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:29:04 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:29:04 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:29:04 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:29:04 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:29:28 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 809/918 [01:53<00:13,  8.28it/s]
10/09/2021 10:29:28 - INFO - trainer -     Num examples = 3258
10/09/2021 10:29:28 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:29:32 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:29:32 - INFO - trainer -     T-F1 = 0.9896401308615049 | 50/51 [00:03<00:00, 13.98it/s]
10/09/2021 10:29:32 - INFO - trainer -     T-F1(C) = 0.9671897289586305
10/09/2021 10:29:32 - INFO - trainer -     T-F1(L) = 0.9852507374631269
10/09/2021 10:29:32 - INFO - trainer -     T-F1(O) = 0.9959869848156183
10/09/2021 10:29:32 - INFO - trainer -     T-F1(P) = 0.9967826849956127
10/09/2021 10:29:32 - INFO - trainer -     T-F1(S) = 0.9903270382312298
10/09/2021 10:29:32 - INFO - trainer -     T-F1(T) = 0.9773371104815864
10/09/2021 10:29:32 - INFO - trainer -     U-F1(A) = 0.7464114832535885
10/09/2021 10:29:32 - INFO - trainer -     U-F1(E) = 0.7490347490347491
10/09/2021 10:29:32 - INFO - trainer -     U-F1(I) = 0.1111111111111111
10/09/2021 10:29:32 - INFO - trainer -     U-F1(O) = 0.9661046410568399
10/09/2021 10:29:32 - INFO - trainer -     intent_acc = 0.9370779619398404
10/09/2021 10:29:32 - INFO - trainer -     loss = 0.30775730136562796
10/09/2021 10:29:32 - INFO - trainer -     semantic_frame_acc = 0.91804788213628
10/09/2021 10:29:32 - INFO - trainer -     slot_f1 = 0.9891243725599553
10/09/2021 10:29:32 - INFO - trainer -     slot_precision = 0.9877471456418825
10/09/2021 10:29:32 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:29:32 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:29:32 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:29:32 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:29:32 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:29:32 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:29:32 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:29:32 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:29:32 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:29:32 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:29:32 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:29:32 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:29:32 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:29:32 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:29:32 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:29:32 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:29:32 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:29:32 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.03it/s]
Epoch:  30%|                                                    | 6/20 [12:50<30:05, 128.98s/it]10/09/2021 10:29:56 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 91/918 [00:10<01:40,  8.24it/s]
10/09/2021 10:29:56 - INFO - trainer -     Num examples = 3258
10/09/2021 10:29:56 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:30:00 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:30:00 - INFO - trainer -     T-F1 = 0.9900585591720006 | 50/51 [00:03<00:00, 13.92it/s]
10/09/2021 10:30:00 - INFO - trainer -     T-F1(C) = 0.9673758865248228
10/09/2021 10:30:00 - INFO - trainer -     T-F1(L) = 0.9852507374631269
10/09/2021 10:30:00 - INFO - trainer -     T-F1(O) = 0.9961482124450713
10/09/2021 10:30:00 - INFO - trainer -     T-F1(P) = 0.9967826849956127
10/09/2021 10:30:00 - INFO - trainer -     T-F1(S) = 0.9930651872399444
10/09/2021 10:30:00 - INFO - trainer -     T-F1(T) = 0.9735006973500697
10/09/2021 10:30:00 - INFO - trainer -     U-F1(A) = 0.694300518134715
10/09/2021 10:30:00 - INFO - trainer -     U-F1(E) = 0.7401574803149606
10/09/2021 10:30:00 - INFO - trainer -     U-F1(I) = 0.15789473684210525
10/09/2021 10:30:00 - INFO - trainer -     U-F1(O) = 0.9648606543188505
10/09/2021 10:30:00 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:30:00 - INFO - trainer -     loss = 0.3018226667362101
10/09/2021 10:30:00 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/09/2021 10:30:00 - INFO - trainer -     slot_f1 = 0.9895528625156707
10/09/2021 10:30:00 - INFO - trainer -     slot_precision = 0.9872151195108394
10/09/2021 10:30:00 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:30:00 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:30:00 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:30:00 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:30:00 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:30:00 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:30:00 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:30:00 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:30:00 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:30:00 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:30:00 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:30:00 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:30:00 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:30:00 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:30:00 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:30:00 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:30:00 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:30:00 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:30:25 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 291/918 [00:39<01:17,  8.12it/s]
10/09/2021 10:30:25 - INFO - trainer -     Num examples = 3258
10/09/2021 10:30:25 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.02it/s]
10/09/2021 10:30:29 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:30:29 - INFO - trainer -     T-F1 = 0.9885714285714284 | 50/51 [00:03<00:00, 13.85it/s]
10/09/2021 10:30:29 - INFO - trainer -     T-F1(C) = 0.9605633802816902
10/09/2021 10:30:29 - INFO - trainer -     T-F1(L) = 0.9822485207100591
10/09/2021 10:30:29 - INFO - trainer -     T-F1(O) = 0.995549766634104
10/09/2021 10:30:29 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:30:29 - INFO - trainer -     T-F1(S) = 0.9907918968692448
10/09/2021 10:30:29 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:30:29 - INFO - trainer -     U-F1(A) = 0.6965174129353233
10/09/2021 10:30:29 - INFO - trainer -     U-F1(E) = 0.689795918367347
10/09/2021 10:30:29 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/09/2021 10:30:29 - INFO - trainer -     U-F1(O) = 0.9619771863117871
10/09/2021 10:30:29 - INFO - trainer -     intent_acc = 0.929097605893186
10/09/2021 10:30:29 - INFO - trainer -     loss = 0.3314002647849859
10/09/2021 10:30:29 - INFO - trainer -     semantic_frame_acc = 0.9076120319214241
10/09/2021 10:30:29 - INFO - trainer -     slot_f1 = 0.9880322849986084
10/09/2021 10:30:29 - INFO - trainer -     slot_precision = 0.984743411927878
10/09/2021 10:30:29 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:30:29 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:30:29 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:30:29 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:30:29 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:30:29 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:30:29 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:30:29 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:30:29 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:30:29 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:30:29 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:30:29 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:30:29 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:30:29 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:30:29 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:30:29 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:30:29 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:30:29 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:30:53 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 491/918 [01:07<00:51,  8.28it/s]
10/09/2021 10:30:53 - INFO - trainer -     Num examples = 3258
10/09/2021 10:30:53 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:30:57 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:30:57 - INFO - trainer -     T-F1 = 0.9893848666303756 | 50/51 [00:03<00:00, 13.89it/s]
10/09/2021 10:30:57 - INFO - trainer -     T-F1(C) = 0.9674681753889673
10/09/2021 10:30:57 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:30:57 - INFO - trainer -     T-F1(O) = 0.9958758411113523
10/09/2021 10:30:57 - INFO - trainer -     T-F1(P) = 0.9956178790534619
10/09/2021 10:30:57 - INFO - trainer -     T-F1(S) = 0.9921478060046189
10/09/2021 10:30:57 - INFO - trainer -     T-F1(T) = 0.9748603351955307
10/09/2021 10:30:57 - INFO - trainer -     U-F1(A) = 0.7227722772277227
10/09/2021 10:30:57 - INFO - trainer -     U-F1(E) = 0.7438330170777988
10/09/2021 10:30:57 - INFO - trainer -     U-F1(I) = 0.2
10/09/2021 10:30:57 - INFO - trainer -     U-F1(O) = 0.9646772228989039
10/09/2021 10:30:57 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:30:57 - INFO - trainer -     loss = 0.3288367434459574
10/09/2021 10:30:57 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:30:57 - INFO - trainer -     slot_f1 = 0.9890048712595685
10/09/2021 10:30:57 - INFO - trainer -     slot_precision = 0.9858490566037735
10/09/2021 10:30:57 - INFO - trainer -     slot_recall = 0.9921809550404915

10/09/2021 10:30:57 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:30:57 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:30:57 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:30:57 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:30:57 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:30:57 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:30:57 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:30:57 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:30:57 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:30:57 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:30:57 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:30:57 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:30:57 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:30:57 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:30:57 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:30:57 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:30:57 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:31:21 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 691/918 [01:35<00:27,  8.28it/s]
10/09/2021 10:31:21 - INFO - trainer -     Num examples = 3258
10/09/2021 10:31:21 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.02it/s]
10/09/2021 10:31:25 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:31:25 - INFO - trainer -     T-F1 = 0.9892706777128888 | 50/51 [00:03<00:00, 13.97it/s]
10/09/2021 10:31:25 - INFO - trainer -     T-F1(C) = 0.9620253164556962
10/09/2021 10:31:25 - INFO - trainer -     T-F1(L) = 0.9881656804733727
10/09/2021 10:31:25 - INFO - trainer -     T-F1(O) = 0.9958181719437355
10/09/2021 10:31:25 - INFO - trainer -     T-F1(P) = 0.9964994165694282
10/09/2021 10:31:25 - INFO - trainer -     T-F1(S) = 0.993077988001846
10/09/2021 10:31:25 - INFO - trainer -     T-F1(T) = 0.9707927677329623
10/09/2021 10:31:25 - INFO - trainer -     U-F1(A) = 0.7024390243902437
10/09/2021 10:31:25 - INFO - trainer -     U-F1(E) = 0.7364341085271319
10/09/2021 10:31:25 - INFO - trainer -     U-F1(I) = 0.17777777777777776
10/09/2021 10:31:25 - INFO - trainer -     U-F1(O) = 0.9634782608695652
10/09/2021 10:31:25 - INFO - trainer -     intent_acc = 0.9318600368324125
10/09/2021 10:31:25 - INFO - trainer -     loss = 0.3104909117899689
10/09/2021 10:31:25 - INFO - trainer -     semantic_frame_acc = 0.910988336402701
10/09/2021 10:31:25 - INFO - trainer -     slot_f1 = 0.9887484372829559
10/09/2021 10:31:25 - INFO - trainer -     slot_precision = 0.9836926478717524
10/09/2021 10:31:25 - INFO - trainer -     slot_recall = 0.9938564646746719

10/09/2021 10:31:25 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:31:25 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:31:25 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:31:25 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:31:25 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:31:25 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:31:25 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:31:25 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:31:25 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:31:25 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:31:25 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:31:25 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:31:25 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:31:25 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:31:25 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:31:25 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:31:25 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:31:49 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 891/918 [02:03<00:03,  8.26it/s]
10/09/2021 10:31:49 - INFO - trainer -     Num examples = 3258
10/09/2021 10:31:49 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:31:53 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:31:53 - INFO - trainer -     T-F1 = 0.9895195317816795 | 50/51 [00:03<00:00, 13.90it/s]
10/09/2021 10:31:53 - INFO - trainer -     T-F1(C) = 0.9701280227596016
10/09/2021 10:31:53 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:31:53 - INFO - trainer -     T-F1(O) = 0.9959303272016929
10/09/2021 10:31:53 - INFO - trainer -     T-F1(P) = 0.9953298307063632
10/09/2021 10:31:53 - INFO - trainer -     T-F1(S) = 0.9944547134935305
10/09/2021 10:31:53 - INFO - trainer -     T-F1(T) = 0.9666666666666666
10/09/2021 10:31:53 - INFO - trainer -     U-F1(A) = 0.7281553398058253
10/09/2021 10:31:53 - INFO - trainer -     U-F1(E) = 0.7390476190476191
10/09/2021 10:31:53 - INFO - trainer -     U-F1(I) = 0.2702702702702703
10/09/2021 10:31:53 - INFO - trainer -     U-F1(O) = 0.964857341684064
10/09/2021 10:31:53 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:31:53 - INFO - trainer -     loss = 0.31244785112201
10/09/2021 10:31:53 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/09/2021 10:31:53 - INFO - trainer -     slot_f1 = 0.9891425389755011
10/09/2021 10:31:53 - INFO - trainer -     slot_precision = 0.9861226755481544
10/09/2021 10:31:53 - INFO - trainer -     slot_recall = 0.9921809550404915

10/09/2021 10:31:53 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:31:53 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:31:53 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:31:53 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:31:53 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:31:53 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:31:53 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:31:53 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:31:53 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:31:53 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:31:53 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:31:53 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:31:53 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:31:53 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:31:53 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:31:53 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:31:53 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:11<00:00,  7.00it/s]
Epoch:  35%|                                                | 7/20 [15:01<28:06, 129.70s/it]10/09/2021 10:32:17 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 173/918 [00:20<01:29,  8.32it/s]
10/09/2021 10:32:17 - INFO - trainer -     Num examples = 3258
10/09/2021 10:32:17 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:32:21 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:32:21 - INFO - trainer -     T-F1 = 0.9912639912639912 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:32:21 - INFO - trainer -     T-F1(C) = 0.9741379310344828
10/09/2021 10:32:21 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:32:21 - INFO - trainer -     T-F1(O) = 0.996639566395664
10/09/2021 10:32:21 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/09/2021 10:32:21 - INFO - trainer -     T-F1(S) = 0.995827538247566
10/09/2021 10:32:21 - INFO - trainer -     T-F1(T) = 0.9692737430167597
10/09/2021 10:32:21 - INFO - trainer -     U-F1(A) = 0.72636815920398
10/09/2021 10:32:21 - INFO - trainer -     U-F1(E) = 0.7380073800738007
10/09/2021 10:32:21 - INFO - trainer -     U-F1(I) = 0.19999999999999998
10/09/2021 10:32:21 - INFO - trainer -     U-F1(O) = 0.9627817578193254
10/09/2021 10:32:21 - INFO - trainer -     intent_acc = 0.930939226519337
10/09/2021 10:32:21 - INFO - trainer -     loss = 0.35114103552027076
10/09/2021 10:32:21 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:32:21 - INFO - trainer -     slot_f1 = 0.9909255898366607
10/09/2021 10:32:21 - INFO - trainer -     slot_precision = 0.990787269681742
10/09/2021 10:32:21 - INFO - trainer -     slot_recall = 0.9910639486177045

10/09/2021 10:32:21 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:32:21 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:32:21 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:32:21 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:32:21 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:32:21 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:32:21 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:32:21 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:32:21 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:32:21 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:32:21 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:32:21 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:32:21 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:32:21 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:32:21 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:32:21 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:32:21 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:32:46 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 373/918 [00:48<01:05,  8.30it/s]
10/09/2021 10:32:46 - INFO - trainer -     Num examples = 3258
10/09/2021 10:32:46 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:32:50 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:32:50 - INFO - trainer -     T-F1 = 0.9912711402073106 | 50/51 [00:03<00:00, 13.96it/s]
10/09/2021 10:32:50 - INFO - trainer -     T-F1(C) = 0.9742857142857144
10/09/2021 10:32:50 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:32:50 - INFO - trainer -     T-F1(O) = 0.996638473216222
10/09/2021 10:32:50 - INFO - trainer -     T-F1(P) = 0.9967864446392054
10/09/2021 10:32:50 - INFO - trainer -     T-F1(S) = 0.9953617810760669
10/09/2021 10:32:50 - INFO - trainer -     T-F1(T) = 0.9721448467966575
10/09/2021 10:32:50 - INFO - trainer -     U-F1(A) = 0.5662650602409639
10/09/2021 10:32:50 - INFO - trainer -     U-F1(E) = 0.7537878787878789
10/09/2021 10:32:50 - INFO - trainer -     U-F1(I) = 0.2222222222222222
10/09/2021 10:32:50 - INFO - trainer -     U-F1(O) = 0.9626685101970274
10/09/2021 10:32:50 - INFO - trainer -     intent_acc = 0.9315531000613874
10/09/2021 10:32:50 - INFO - trainer -     loss = 0.3541867551850338
10/09/2021 10:32:50 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:32:50 - INFO - trainer -     slot_f1 = 0.9909331845445669
10/09/2021 10:32:50 - INFO - trainer -     slot_precision = 0.9899665551839465
10/09/2021 10:32:50 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:32:50 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:32:50 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:32:50 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:32:50 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:32:50 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:32:50 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:32:50 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:32:50 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:32:50 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:32:50 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:32:50 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:32:50 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:32:50 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:32:50 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:32:50 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:32:50 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:32:50 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:33:14 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 573/918 [01:17<00:41,  8.29it/s]
10/09/2021 10:33:14 - INFO - trainer -     Num examples = 3258
10/09/2021 10:33:14 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:33:18 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:33:18 - INFO - trainer -     T-F1 = 0.9907306434023991 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:33:18 - INFO - trainer -     T-F1(C) = 0.9728958630527819
10/09/2021 10:33:18 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:33:18 - INFO - trainer -     T-F1(O) = 0.9964208242950109
10/09/2021 10:33:18 - INFO - trainer -     T-F1(P) = 0.9964891749561146
10/09/2021 10:33:18 - INFO - trainer -     T-F1(S) = 0.9926131117266852
10/09/2021 10:33:18 - INFO - trainer -     T-F1(T) = 0.9775910364145658
10/09/2021 10:33:18 - INFO - trainer -     U-F1(A) = 0.6896551724137931
10/09/2021 10:33:18 - INFO - trainer -     U-F1(E) = 0.7374517374517374
10/09/2021 10:33:18 - INFO - trainer -     U-F1(I) = 0.18604651162790697
10/09/2021 10:33:18 - INFO - trainer -     U-F1(O) = 0.9638386648122393
10/09/2021 10:33:18 - INFO - trainer -     intent_acc = 0.9321669736034377
10/09/2021 10:33:18 - INFO - trainer -     loss = 0.36676443170975237
10/09/2021 10:33:18 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/09/2021 10:33:18 - INFO - trainer -     slot_f1 = 0.9903805938937683
10/09/2021 10:33:18 - INFO - trainer -     slot_precision = 0.9888641425389755
10/09/2021 10:33:18 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:33:18 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:33:18 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:33:18 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:33:18 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:33:18 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:33:18 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:33:18 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:33:18 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:33:18 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:33:18 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:33:18 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:33:18 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:33:18 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:33:18 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:33:18 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:33:18 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:33:18 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:33:42 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 773/918 [01:45<00:17,  8.25it/s]
10/09/2021 10:33:42 - INFO - trainer -     Num examples = 3258
10/09/2021 10:33:42 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.07it/s]
10/09/2021 10:33:46 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:33:46 - INFO - trainer -     T-F1 = 0.9909959072305594 | 50/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:33:46 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:33:46 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:33:46 - INFO - trainer -     T-F1(O) = 0.9965304130976905
10/09/2021 10:33:46 - INFO - trainer -     T-F1(P) = 0.9964891749561146
10/09/2021 10:33:46 - INFO - trainer -     T-F1(S) = 0.9926062846580406
10/09/2021 10:33:46 - INFO - trainer -     T-F1(T) = 0.9761570827489481
10/09/2021 10:33:46 - INFO - trainer -     U-F1(A) = 0.7010309278350515
10/09/2021 10:33:46 - INFO - trainer -     U-F1(E) = 0.7104247104247104
10/09/2021 10:33:46 - INFO - trainer -     U-F1(I) = 0.21621621621621623
10/09/2021 10:33:46 - INFO - trainer -     U-F1(O) = 0.9620253164556962
10/09/2021 10:33:46 - INFO - trainer -     intent_acc = 0.9300184162062615
10/09/2021 10:33:46 - INFO - trainer -     loss = 0.38574712066089406
10/09/2021 10:33:46 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/09/2021 10:33:46 - INFO - trainer -     slot_f1 = 0.9906515976001117
10/09/2021 10:33:46 - INFO - trainer -     slot_precision = 0.9899609592861126
10/09/2021 10:33:46 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:33:46 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:33:46 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:33:46 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:33:46 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:33:46 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:33:46 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:33:46 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:33:46 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:33:46 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:33:46 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:33:46 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:33:46 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:33:46 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:33:46 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:33:46 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:33:46 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:33:46 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.25it/s]
Epoch:  40%|                                             | 8/20 [17:08<25:44, 128.71s/it]10/09/2021 10:34:10 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 55/918 [00:06<01:43,  8.34it/s]
10/09/2021 10:34:10 - INFO - trainer -     Num examples = 3258
10/09/2021 10:34:10 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.08it/s]
10/09/2021 10:34:14 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:34:14 - INFO - trainer -     T-F1 = 0.9889870836165873 | 50/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:34:14 - INFO - trainer -     T-F1(C) = 0.9688385269121813
10/09/2021 10:34:14 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:34:14 - INFO - trainer -     T-F1(O) = 0.9957114163183324
10/09/2021 10:34:14 - INFO - trainer -     T-F1(P) = 0.9953379953379954
10/09/2021 10:34:14 - INFO - trainer -     T-F1(S) = 0.9912077741786209
10/09/2021 10:34:14 - INFO - trainer -     T-F1(T) = 0.9694444444444444
10/09/2021 10:34:14 - INFO - trainer -     U-F1(A) = 0.7135678391959799
10/09/2021 10:34:14 - INFO - trainer -     U-F1(E) = 0.7437379576107901
10/09/2021 10:34:14 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/09/2021 10:34:14 - INFO - trainer -     U-F1(O) = 0.9650981073102969
10/09/2021 10:34:14 - INFO - trainer -     intent_acc = 0.9355432780847146
10/09/2021 10:34:14 - INFO - trainer -     loss = 0.38827138116546706
10/09/2021 10:34:14 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/09/2021 10:34:14 - INFO - trainer -     slot_f1 = 0.9883138564273789
10/09/2021 10:34:14 - INFO - trainer -     slot_precision = 0.984751871361242
10/09/2021 10:34:14 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:34:14 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:34:14 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:34:14 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:34:14 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:34:14 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:34:14 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:34:14 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:34:14 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:34:14 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:34:14 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:34:14 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:34:14 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:34:14 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:34:14 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:34:14 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:34:14 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:34:14 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:34:38 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 255/918 [00:34<01:19,  8.35it/s]
10/09/2021 10:34:38 - INFO - trainer -     Num examples = 3258
10/09/2021 10:34:38 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:34:42 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:34:42 - INFO - trainer -     T-F1 = 0.9889900774772327 | 50/51 [00:03<00:00, 13.96it/s]
10/09/2021 10:34:42 - INFO - trainer -     T-F1(C) = 0.9661016949152542
10/09/2021 10:34:42 - INFO - trainer -     T-F1(L) = 0.9852507374631269
10/09/2021 10:34:42 - INFO - trainer -     T-F1(O) = 0.9957109506487867
10/09/2021 10:34:42 - INFO - trainer -     T-F1(P) = 0.9961999415375621
10/09/2021 10:34:42 - INFO - trainer -     T-F1(S) = 0.9903359410952601
10/09/2021 10:34:42 - INFO - trainer -     T-F1(T) = 0.9748603351955307
10/09/2021 10:34:42 - INFO - trainer -     U-F1(A) = 0.7464114832535885
10/09/2021 10:34:42 - INFO - trainer -     U-F1(E) = 0.761904761904762
10/09/2021 10:34:42 - INFO - trainer -     U-F1(I) = 0.24390243902439024
10/09/2021 10:34:42 - INFO - trainer -     U-F1(O) = 0.9677756488416651
10/09/2021 10:34:42 - INFO - trainer -     intent_acc = 0.9395334561080417
10/09/2021 10:34:42 - INFO - trainer -     loss = 0.41283707824700017
10/09/2021 10:34:42 - INFO - trainer -     semantic_frame_acc = 0.9189686924493554
10/09/2021 10:34:42 - INFO - trainer -     slot_f1 = 0.988739051855971
10/09/2021 10:34:42 - INFO - trainer -     slot_precision = 0.9844961240310077
10/09/2021 10:34:42 - INFO - trainer -     slot_recall = 0.9930187098575817

10/09/2021 10:34:42 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:34:42 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:34:42 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:34:42 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:34:42 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:34:42 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:34:42 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:34:42 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:34:42 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:34:42 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:34:42 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:34:42 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:34:42 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:34:42 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:34:42 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:34:42 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:34:42 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:35:06 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 455/918 [01:02<00:56,  8.25it/s]
10/09/2021 10:35:06 - INFO - trainer -     Num examples = 3258
10/09/2021 10:35:06 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.08it/s]
10/09/2021 10:35:10 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:35:10 - INFO - trainer -     T-F1 = 0.9899100081810744 | 50/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:35:10 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:35:10 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:35:10 - INFO - trainer -     T-F1(O) = 0.9960958681271013
10/09/2021 10:35:10 - INFO - trainer -     T-F1(P) = 0.9964891749561146
10/09/2021 10:35:10 - INFO - trainer -     T-F1(S) = 0.9903091832025841
10/09/2021 10:35:10 - INFO - trainer -     T-F1(T) = 0.9720670391061452
10/09/2021 10:35:10 - INFO - trainer -     U-F1(A) = 0.7076923076923077
10/09/2021 10:35:10 - INFO - trainer -     U-F1(E) = 0.770949720670391
10/09/2021 10:35:10 - INFO - trainer -     U-F1(I) = 0.25
10/09/2021 10:35:10 - INFO - trainer -     U-F1(O) = 0.9658774373259054
10/09/2021 10:35:10 - INFO - trainer -     intent_acc = 0.9376918354818907
10/09/2021 10:35:10 - INFO - trainer -     loss = 0.3973454945702471
10/09/2021 10:35:10 - INFO - trainer -     semantic_frame_acc = 0.9192756292203806
10/09/2021 10:35:10 - INFO - trainer -     slot_f1 = 0.989679218967922
10/09/2021 10:35:10 - INFO - trainer -     slot_precision = 0.9885762050710505
10/09/2021 10:35:10 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:35:10 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:35:10 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:35:10 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:35:10 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:35:10 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:35:10 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:35:10 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:35:10 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:35:10 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:35:10 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:35:10 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:35:10 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:35:10 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:35:10 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:35:10 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:35:10 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:35:10 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:35:34 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 655/918 [01:30<00:31,  8.24it/s]
10/09/2021 10:35:34 - INFO - trainer -     Num examples = 3258
10/09/2021 10:35:34 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.11it/s]
10/09/2021 10:35:38 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:35:38 - INFO - trainer -     T-F1 = 0.9920851528384279 | 50/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:35:38 - INFO - trainer -     T-F1(C) = 0.9784791965566715
10/09/2021 10:35:38 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:35:38 - INFO - trainer -     T-F1(O) = 0.9969644405897659
10/09/2021 10:35:38 - INFO - trainer -     T-F1(P) = 0.9967901955062738
10/09/2021 10:35:38 - INFO - trainer -     T-F1(S) = 0.9925994449583718
10/09/2021 10:35:38 - INFO - trainer -     T-F1(T) = 0.9801699716713882
10/09/2021 10:35:38 - INFO - trainer -     U-F1(A) = 0.7389162561576355
10/09/2021 10:35:38 - INFO - trainer -     U-F1(E) = 0.7434944237918216
10/09/2021 10:35:38 - INFO - trainer -     U-F1(I) = 0.27999999999999997
10/09/2021 10:35:38 - INFO - trainer -     U-F1(O) = 0.9645414847161572
10/09/2021 10:35:38 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:35:38 - INFO - trainer -     loss = 0.40164083530943767
10/09/2021 10:35:38 - INFO - trainer -     semantic_frame_acc = 0.919889502762431
10/09/2021 10:35:38 - INFO - trainer -     slot_f1 = 0.9917655268667132
10/09/2021 10:35:38 - INFO - trainer -     slot_precision = 0.9913504464285714
10/09/2021 10:35:38 - INFO - trainer -     slot_recall = 0.9921809550404915

10/09/2021 10:35:38 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:35:38 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:35:38 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:35:38 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:35:38 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:35:38 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:35:38 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:35:38 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:35:38 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:35:38 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:35:38 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:35:38 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:35:38 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:35:38 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:35:38 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:35:38 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:35:38 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:36:02 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 855/918 [01:58<00:07,  8.27it/s]
10/09/2021 10:36:02 - INFO - trainer -     Num examples = 3258
10/09/2021 10:36:02 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.10it/s]
10/09/2021 10:36:06 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:36:06 - INFO - trainer -     T-F1 = 0.9905956112852664 | 50/51 [00:03<00:00, 14.00it/s]
10/09/2021 10:36:06 - INFO - trainer -     T-F1(C) = 0.9674681753889673
10/09/2021 10:36:06 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/09/2021 10:36:06 - INFO - trainer -     T-F1(O) = 0.9963663973100494
10/09/2021 10:36:06 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:36:06 - INFO - trainer -     T-F1(S) = 0.9930651872399444
10/09/2021 10:36:06 - INFO - trainer -     T-F1(T) = 0.9776536312849162
10/09/2021 10:36:06 - INFO - trainer -     U-F1(A) = 0.6871794871794871
10/09/2021 10:36:06 - INFO - trainer -     U-F1(E) = 0.7354085603112841
10/09/2021 10:36:06 - INFO - trainer -     U-F1(I) = 0.2564102564102564
10/09/2021 10:36:06 - INFO - trainer -     U-F1(O) = 0.9635922330097089
10/09/2021 10:36:06 - INFO - trainer -     intent_acc = 0.9330877839165131
10/09/2021 10:36:06 - INFO - trainer -     loss = 0.3792365768960878
10/09/2021 10:36:06 - INFO - trainer -     semantic_frame_acc = 0.9155923879680786
10/09/2021 10:36:06 - INFO - trainer -     slot_f1 = 0.9903805938937683
10/09/2021 10:36:06 - INFO - trainer -     slot_precision = 0.9888641425389755
10/09/2021 10:36:06 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:36:06 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:36:06 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:36:06 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:36:06 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:36:06 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:36:06 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:36:06 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:36:06 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:36:06 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:36:06 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:36:06 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:36:06 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:36:06 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:36:06 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:36:06 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:36:06 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:36:06 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.04it/s]
Epoch:  45%|                                         | 9/20 [19:18<23:41, 129.21s/it]10/09/2021 10:36:30 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 137/918 [00:16<01:34,  8.29it/s]
10/09/2021 10:36:30 - INFO - trainer -     Num examples = 3258
10/09/2021 10:36:30 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.07it/s]
10/09/2021 10:36:34 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:36:34 - INFO - trainer -     T-F1 = 0.9899264906071332 | 50/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:36:34 - INFO - trainer -     T-F1(C) = 0.9645390070921986
10/09/2021 10:36:34 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:36:34 - INFO - trainer -     T-F1(O) = 0.9960933260987519
10/09/2021 10:36:34 - INFO - trainer -     T-F1(P) = 0.9973722627737226
10/09/2021 10:36:34 - INFO - trainer -     T-F1(S) = 0.9916820702402956
10/09/2021 10:36:34 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:36:34 - INFO - trainer -     U-F1(A) = 0.7052631578947368
10/09/2021 10:36:34 - INFO - trainer -     U-F1(E) = 0.7678883071553227
10/09/2021 10:36:34 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/09/2021 10:36:34 - INFO - trainer -     U-F1(O) = 0.9632224168126094
10/09/2021 10:36:34 - INFO - trainer -     intent_acc = 0.9337016574585635
10/09/2021 10:36:34 - INFO - trainer -     loss = 0.42319515822272674
10/09/2021 10:36:34 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:36:34 - INFO - trainer -     slot_f1 = 0.9896964633806739
10/09/2021 10:36:34 - INFO - trainer -     slot_precision = 0.986948069980561
10/09/2021 10:36:34 - INFO - trainer -     slot_recall = 0.9924602066461882

10/09/2021 10:36:34 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:36:34 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:36:34 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:36:34 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:36:34 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:36:34 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:36:34 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:36:34 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:36:34 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:36:34 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:36:34 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:36:34 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:36:34 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:36:34 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:36:34 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:36:34 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:36:34 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:36:58 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 337/918 [00:44<01:10,  8.26it/s]
10/09/2021 10:36:58 - INFO - trainer -     Num examples = 3258
10/09/2021 10:36:58 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:37:02 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:37:02 - INFO - trainer -     T-F1 = 0.9893761917733588 | 50/51 [00:03<00:00, 13.97it/s]
10/09/2021 10:37:02 - INFO - trainer -     T-F1(C) = 0.9685714285714285
10/09/2021 10:37:02 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:37:02 - INFO - trainer -     T-F1(O) = 0.9958771834653358
10/09/2021 10:37:02 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:37:02 - INFO - trainer -     T-F1(S) = 0.9907749077490775
10/09/2021 10:37:02 - INFO - trainer -     T-F1(T) = 0.9748603351955307
10/09/2021 10:37:02 - INFO - trainer -     U-F1(A) = 0.6594594594594595
10/09/2021 10:37:02 - INFO - trainer -     U-F1(E) = 0.714859437751004
10/09/2021 10:37:02 - INFO - trainer -     U-F1(I) = 0.27272727272727276
10/09/2021 10:37:02 - INFO - trainer -     U-F1(O) = 0.9611331836241147
10/09/2021 10:37:02 - INFO - trainer -     intent_acc = 0.929097605893186
10/09/2021 10:37:02 - INFO - trainer -     loss = 0.463076309209653
10/09/2021 10:37:02 - INFO - trainer -     semantic_frame_acc = 0.9088397790055248
10/09/2021 10:37:02 - INFO - trainer -     slot_f1 = 0.9891334633602674
10/09/2021 10:37:02 - INFO - trainer -     slot_precision = 0.9869335557408951
10/09/2021 10:37:02 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:37:02 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:37:02 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:37:02 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:37:02 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:37:02 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:37:02 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:37:02 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:37:02 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:37:02 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:37:02 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:37:02 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:37:02 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:37:02 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:37:02 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:37:02 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:37:02 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:37:02 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:37:26 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 537/918 [01:12<00:45,  8.31it/s]
10/09/2021 10:37:26 - INFO - trainer -     Num examples = 3258
10/09/2021 10:37:26 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.07it/s]
10/09/2021 10:37:30 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:37:30 - INFO - trainer -     T-F1 = 0.9899100081810744 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:37:30 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/09/2021 10:37:30 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:37:30 - INFO - trainer -     T-F1(O) = 0.9960958681271013
10/09/2021 10:37:30 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/09/2021 10:37:30 - INFO - trainer -     T-F1(S) = 0.9925994449583718
10/09/2021 10:37:30 - INFO - trainer -     T-F1(T) = 0.97054698457223
10/09/2021 10:37:30 - INFO - trainer -     U-F1(A) = 0.7164179104477612
10/09/2021 10:37:30 - INFO - trainer -     U-F1(E) = 0.7247524752475247
10/09/2021 10:37:30 - INFO - trainer -     U-F1(I) = 0.1818181818181818
10/09/2021 10:37:30 - INFO - trainer -     U-F1(O) = 0.96451445386879
10/09/2021 10:37:30 - INFO - trainer -     intent_acc = 0.9343155310006138
10/09/2021 10:37:30 - INFO - trainer -     loss = 0.4441074345114769
10/09/2021 10:37:30 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/09/2021 10:37:30 - INFO - trainer -     slot_f1 = 0.9895412076418909
10/09/2021 10:37:30 - INFO - trainer -     slot_precision = 0.9883008356545961
10/09/2021 10:37:30 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:37:30 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:37:30 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:37:30 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:37:30 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:37:30 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:37:30 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:37:30 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:37:30 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:37:30 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:37:30 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:37:30 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:37:30 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:37:30 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:37:30 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:37:30 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:37:30 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:37:30 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:37:54 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 737/918 [01:40<00:21,  8.28it/s]
10/09/2021 10:37:54 - INFO - trainer -     Num examples = 3258
10/09/2021 10:37:54 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.02it/s]
10/09/2021 10:37:58 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:37:58 - INFO - trainer -     T-F1 = 0.9897750511247444 | 50/51 [00:03<00:00, 13.86it/s]
10/09/2021 10:37:58 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/09/2021 10:37:58 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:37:58 - INFO - trainer -     T-F1(O) = 0.996041429423567
10/09/2021 10:37:58 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/09/2021 10:37:58 - INFO - trainer -     T-F1(S) = 0.9916666666666667
10/09/2021 10:37:58 - INFO - trainer -     T-F1(T) = 0.9733520336605891
10/09/2021 10:37:58 - INFO - trainer -     U-F1(A) = 0.6868686868686869
10/09/2021 10:37:58 - INFO - trainer -     U-F1(E) = 0.7392996108949417
10/09/2021 10:37:58 - INFO - trainer -     U-F1(I) = 0.2325581395348837
10/09/2021 10:37:58 - INFO - trainer -     U-F1(O) = 0.9633744141642077
10/09/2021 10:37:58 - INFO - trainer -     intent_acc = 0.9324739103744628
10/09/2021 10:37:58 - INFO - trainer -     loss = 0.46088229800921443
10/09/2021 10:37:58 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/09/2021 10:37:58 - INFO - trainer -     slot_f1 = 0.9895412076418909
10/09/2021 10:37:58 - INFO - trainer -     slot_precision = 0.9883008356545961
10/09/2021 10:37:58 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:37:58 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:37:58 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:37:58 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:37:58 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:37:58 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:37:58 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:37:58 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:37:58 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:37:58 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:37:58 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:37:58 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:37:58 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:37:58 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:37:58 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:37:58 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:37:58 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:37:58 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.27it/s]
Epoch:  50%|                                     | 10/20 [21:25<21:22, 128.29s/it]10/09/2021 10:38:22 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 19/918 [00:02<01:48,  8.32it/s]
10/09/2021 10:38:22 - INFO - trainer -     Num examples = 3258
10/09/2021 10:38:22 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:38:26 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:38:26 - INFO - trainer -     T-F1 = 0.9888556673008969 | 50/51 [00:03<00:00, 14.00it/s]
10/09/2021 10:38:26 - INFO - trainer -     T-F1(C) = 0.9715909090909091
10/09/2021 10:38:26 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:38:26 - INFO - trainer -     T-F1(O) = 0.9956564230643936
10/09/2021 10:38:26 - INFO - trainer -     T-F1(P) = 0.9962088072324293
10/09/2021 10:38:26 - INFO - trainer -     T-F1(S) = 0.9898430286241922
10/09/2021 10:38:26 - INFO - trainer -     T-F1(T) = 0.9654218533886584
10/09/2021 10:38:26 - INFO - trainer -     U-F1(A) = 0.71
10/09/2021 10:38:26 - INFO - trainer -     U-F1(E) = 0.7453183520599251
10/09/2021 10:38:26 - INFO - trainer -     U-F1(I) = 0.14634146341463417
10/09/2021 10:38:26 - INFO - trainer -     U-F1(O) = 0.9635951924751786
10/09/2021 10:38:26 - INFO - trainer -     intent_acc = 0.932780847145488
10/09/2021 10:38:26 - INFO - trainer -     loss = 0.4430103488996917
10/09/2021 10:38:26 - INFO - trainer -     semantic_frame_acc = 0.9116022099447514
10/09/2021 10:38:26 - INFO - trainer -     slot_f1 = 0.9886016124548235
10/09/2021 10:38:26 - INFO - trainer -     slot_precision = 0.9842236368668696
10/09/2021 10:38:26 - INFO - trainer -     slot_recall = 0.9930187098575817

10/09/2021 10:38:26 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:38:26 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:38:26 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:38:26 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:38:26 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:38:26 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:38:26 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:38:26 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:38:26 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:38:26 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:38:26 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:38:26 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:38:26 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:38:26 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:38:26 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:38:26 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:38:26 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:38:50 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 219/918 [00:30<01:24,  8.32it/s]
10/09/2021 10:38:50 - INFO - trainer -     Num examples = 3258
10/09/2021 10:38:50 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:38:54 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:38:54 - INFO - trainer -     T-F1 = 0.9883247352701602 | 50/51 [00:03<00:00, 14.02it/s]
10/09/2021 10:38:54 - INFO - trainer -     T-F1(C) = 0.9673758865248228
10/09/2021 10:38:54 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:38:54 - INFO - trainer -     T-F1(O) = 0.9954372623574145
10/09/2021 10:38:54 - INFO - trainer -     T-F1(P) = 0.9964953271028038
10/09/2021 10:38:54 - INFO - trainer -     T-F1(S) = 0.9885163068442812
10/09/2021 10:38:54 - INFO - trainer -     T-F1(T) = 0.9668508287292817
10/09/2021 10:38:54 - INFO - trainer -     U-F1(A) = 0.6519337016574585
10/09/2021 10:38:54 - INFO - trainer -     U-F1(E) = 0.7325581395348838
10/09/2021 10:38:54 - INFO - trainer -     U-F1(I) = 0.17777777777777776
10/09/2021 10:38:54 - INFO - trainer -     U-F1(O) = 0.9608590232074818
10/09/2021 10:38:54 - INFO - trainer -     intent_acc = 0.9287906691221608
10/09/2021 10:38:54 - INFO - trainer -     loss = 0.4643379465731628
10/09/2021 10:38:54 - INFO - trainer -     semantic_frame_acc = 0.9060773480662984
10/09/2021 10:38:54 - INFO - trainer -     slot_f1 = 0.9879183446743507
10/09/2021 10:38:54 - INFO - trainer -     slot_precision = 0.9825966850828729
10/09/2021 10:38:54 - INFO - trainer -     slot_recall = 0.9932979614632784

10/09/2021 10:38:54 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:38:54 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:38:54 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:38:54 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:38:54 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:38:54 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:38:54 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:38:54 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:38:54 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:38:54 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:38:54 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:38:54 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:38:54 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:38:54 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:38:54 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:38:54 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:38:54 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:39:18 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 419/918 [00:58<01:00,  8.28it/s]
10/09/2021 10:39:18 - INFO - trainer -     Num examples = 3258
10/09/2021 10:39:18 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:39:22 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:39:22 - INFO - trainer -     T-F1 = 0.9900612661674608 | 50/51 [00:03<00:00, 13.94it/s]
10/09/2021 10:39:22 - INFO - trainer -     T-F1(C) = 0.9729729729729729
10/09/2021 10:39:22 - INFO - trainer -     T-F1(L) = 0.9823529411764705
10/09/2021 10:39:22 - INFO - trainer -     T-F1(O) = 0.9961477944766968
10/09/2021 10:39:22 - INFO - trainer -     T-F1(P) = 0.9956204379562044
10/09/2021 10:39:22 - INFO - trainer -     T-F1(S) = 0.9925994449583718
10/09/2021 10:39:22 - INFO - trainer -     T-F1(T) = 0.9762237762237763
10/09/2021 10:39:22 - INFO - trainer -     U-F1(A) = 0.6903553299492385
10/09/2021 10:39:22 - INFO - trainer -     U-F1(E) = 0.7532956685499058
10/09/2021 10:39:22 - INFO - trainer -     U-F1(I) = 0.25531914893617025
10/09/2021 10:39:22 - INFO - trainer -     U-F1(O) = 0.9632468211113046
10/09/2021 10:39:22 - INFO - trainer -     intent_acc = 0.932780847145488
10/09/2021 10:39:22 - INFO - trainer -     loss = 0.46991579892004237
10/09/2021 10:39:22 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/09/2021 10:39:22 - INFO - trainer -     slot_f1 = 0.9898342849185349
10/09/2021 10:39:22 - INFO - trainer -     slot_precision = 0.9872222222222222
10/09/2021 10:39:22 - INFO - trainer -     slot_recall = 0.9924602066461882

10/09/2021 10:39:22 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:39:22 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:39:22 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:39:22 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:39:22 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:39:22 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:39:22 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:39:22 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:39:22 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:39:22 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:39:22 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:39:22 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:39:22 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:39:22 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:39:22 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:39:22 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:39:22 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:39:46 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 619/918 [01:26<00:35,  8.37it/s]
10/09/2021 10:39:46 - INFO - trainer -     Num examples = 3258
10/09/2021 10:39:46 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:39:50 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:39:50 - INFO - trainer -     T-F1 = 0.990312457361168 | 50/51 [00:03<00:00, 13.96it/s]
10/09/2021 10:39:50 - INFO - trainer -     T-F1(C) = 0.9701280227596016
10/09/2021 10:39:50 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:39:50 - INFO - trainer -     T-F1(O) = 0.9962595543990893
10/09/2021 10:39:50 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:39:50 - INFO - trainer -     T-F1(S) = 0.9916589434661723
10/09/2021 10:39:50 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/09/2021 10:39:50 - INFO - trainer -     U-F1(A) = 0.7093596059113301
10/09/2021 10:39:50 - INFO - trainer -     U-F1(E) = 0.7364485981308411
10/09/2021 10:39:50 - INFO - trainer -     U-F1(I) = 0.22727272727272727
10/09/2021 10:39:50 - INFO - trainer -     U-F1(O) = 0.9633763515870248
10/09/2021 10:39:50 - INFO - trainer -     intent_acc = 0.9318600368324125
10/09/2021 10:39:50 - INFO - trainer -     loss = 0.47483283336109977
10/09/2021 10:39:50 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/09/2021 10:39:50 - INFO - trainer -     slot_f1 = 0.9900907187718073
10/09/2021 10:39:50 - INFO - trainer -     slot_precision = 0.9896763392857143
10/09/2021 10:39:50 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:39:50 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:39:50 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:39:50 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:39:50 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:39:50 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:39:50 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:39:50 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:39:50 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:39:50 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:39:50 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:39:50 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:39:50 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:39:50 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:39:50 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:39:50 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:39:50 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:39:50 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:40:14 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 819/918 [01:54<00:11,  8.29it/s]
10/09/2021 10:40:14 - INFO - trainer -     Num examples = 3258
10/09/2021 10:40:14 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:40:18 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:40:18 - INFO - trainer -     T-F1 = 0.9903309274138635 | 50/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:40:18 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:40:18 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:40:18 - INFO - trainer -     T-F1(O) = 0.9962567135029566
10/09/2021 10:40:18 - INFO - trainer -     T-F1(P) = 0.9959136018680678
10/09/2021 10:40:18 - INFO - trainer -     T-F1(S) = 0.9903270382312298
10/09/2021 10:40:18 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/09/2021 10:40:18 - INFO - trainer -     U-F1(A) = 0.7135678391959799
10/09/2021 10:40:18 - INFO - trainer -     U-F1(E) = 0.7542213883677298
10/09/2021 10:40:18 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/09/2021 10:40:18 - INFO - trainer -     U-F1(O) = 0.9650008706251089
10/09/2021 10:40:18 - INFO - trainer -     intent_acc = 0.9352363413136894
10/09/2021 10:40:18 - INFO - trainer -     loss = 0.4661247866849105
10/09/2021 10:40:18 - INFO - trainer -     semantic_frame_acc = 0.9171270718232044
10/09/2021 10:40:18 - INFO - trainer -     slot_f1 = 0.9901100431815016
10/09/2021 10:40:18 - INFO - trainer -     slot_precision = 0.9877709838799333
10/09/2021 10:40:18 - INFO - trainer -     slot_recall = 0.9924602066461882

10/09/2021 10:40:18 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:40:18 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:40:18 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:40:18 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:40:18 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:40:18 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:40:18 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:40:18 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:40:18 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:40:18 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:40:18 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:40:18 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:40:18 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:40:18 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:40:18 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:40:18 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:40:18 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.06it/s]
Epoch:  55%|                                 | 11/20 [23:35<19:19, 128.84s/it]10/09/2021 10:40:42 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 101/918 [00:12<01:38,  8.27it/s]
10/09/2021 10:40:42 - INFO - trainer -     Num examples = 3258
10/09/2021 10:40:42 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:40:46 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:40:46 - INFO - trainer -     T-F1 = 0.9884338005170771 | 50/51 [00:03<00:00, 13.92it/s]
10/09/2021 10:40:46 - INFO - trainer -     T-F1(C) = 0.9729729729729729
10/09/2021 10:40:46 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/09/2021 10:40:46 - INFO - trainer -     T-F1(O) = 0.9954957399468173
10/09/2021 10:40:46 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:40:46 - INFO - trainer -     T-F1(S) = 0.9893960350391886
10/09/2021 10:40:46 - INFO - trainer -     T-F1(T) = 0.9653259361997226
10/09/2021 10:40:46 - INFO - trainer -     U-F1(A) = 0.693069306930693
10/09/2021 10:40:46 - INFO - trainer -     U-F1(E) = 0.7433962264150943
10/09/2021 10:40:46 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/09/2021 10:40:46 - INFO - trainer -     U-F1(O) = 0.9646649260226283
10/09/2021 10:40:46 - INFO - trainer -     intent_acc = 0.9337016574585635
10/09/2021 10:40:46 - INFO - trainer -     loss = 0.4890780973376012
10/09/2021 10:40:46 - INFO - trainer -     semantic_frame_acc = 0.9112952731737262
10/09/2021 10:40:46 - INFO - trainer -     slot_f1 = 0.9881697981906751
10/09/2021 10:40:46 - INFO - trainer -     slot_precision = 0.9850166481687015
10/09/2021 10:40:46 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:40:46 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:40:46 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:40:46 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:40:46 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:40:46 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:40:46 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:40:46 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:40:46 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:40:46 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:40:46 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:40:46 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:40:46 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:40:46 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:40:46 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:40:46 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:40:46 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:40:46 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:41:10 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 301/918 [00:40<01:14,  8.27it/s]
10/09/2021 10:41:10 - INFO - trainer -     Num examples = 3258
10/09/2021 10:41:10 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.02it/s]
10/09/2021 10:41:14 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:41:14 - INFO - trainer -     T-F1 = 0.9896598639455783 | 50/51 [00:03<00:00, 13.91it/s]
10/09/2021 10:41:14 - INFO - trainer -     T-F1(C) = 0.9729729729729729
10/09/2021 10:41:14 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:41:14 - INFO - trainer -     T-F1(O) = 0.9959839357429718
10/09/2021 10:41:14 - INFO - trainer -     T-F1(P) = 0.9959136018680678
10/09/2021 10:41:14 - INFO - trainer -     T-F1(S) = 0.9907749077490775
10/09/2021 10:41:14 - INFO - trainer -     T-F1(T) = 0.9721448467966575
10/09/2021 10:41:14 - INFO - trainer -     U-F1(A) = 0.611111111111111
10/09/2021 10:41:14 - INFO - trainer -     U-F1(E) = 0.6940451745379878
10/09/2021 10:41:14 - INFO - trainer -     U-F1(I) = 0.1818181818181818
10/09/2021 10:41:14 - INFO - trainer -     U-F1(O) = 0.9601100412654747
10/09/2021 10:41:14 - INFO - trainer -     intent_acc = 0.9266421117249847
10/09/2021 10:41:14 - INFO - trainer -     loss = 0.5286265478706828
10/09/2021 10:41:14 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:41:14 - INFO - trainer -     slot_f1 = 0.989423879766212
10/09/2021 10:41:14 - INFO - trainer -     slot_precision = 0.986130374479889
10/09/2021 10:41:14 - INFO - trainer -     slot_recall = 0.9927394582518849

10/09/2021 10:41:14 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:41:14 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:41:14 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:41:14 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:41:14 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:41:14 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:41:14 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:41:14 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:41:14 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:41:14 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:41:14 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:41:14 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:41:14 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:41:14 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:41:14 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:41:14 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:41:14 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:41:38 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 501/918 [01:08<00:49,  8.34it/s]
10/09/2021 10:41:38 - INFO - trainer -     Num examples = 3258
10/09/2021 10:41:38 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.06it/s]
10/09/2021 10:41:42 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:41:42 - INFO - trainer -     T-F1 = 0.990042286181967 | 50/51 [00:03<00:00, 13.94it/s]
10/09/2021 10:41:42 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/09/2021 10:41:42 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:41:42 - INFO - trainer -     T-F1(O) = 0.996150718351857
10/09/2021 10:41:42 - INFO - trainer -     T-F1(P) = 0.9967826849956127
10/09/2021 10:41:42 - INFO - trainer -     T-F1(S) = 0.9902912621359223
10/09/2021 10:41:42 - INFO - trainer -     T-F1(T) = 0.9789621318373072
10/09/2021 10:41:42 - INFO - trainer -     U-F1(A) = 0.7142857142857143
10/09/2021 10:41:42 - INFO - trainer -     U-F1(E) = 0.7617260787992495
10/09/2021 10:41:42 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:41:42 - INFO - trainer -     U-F1(O) = 0.965205288796103
10/09/2021 10:41:42 - INFO - trainer -     intent_acc = 0.9361571516267649
10/09/2021 10:41:42 - INFO - trainer -     loss = 0.4619639049850258
10/09/2021 10:41:42 - INFO - trainer -     semantic_frame_acc = 0.9171270718232044
10/09/2021 10:41:42 - INFO - trainer -     slot_f1 = 0.9898144272359426
10/09/2021 10:41:42 - INFO - trainer -     slot_precision = 0.9891243725599553
10/09/2021 10:41:42 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:41:42 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:41:42 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:41:42 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:41:42 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:41:42 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:41:42 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:41:42 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:41:42 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:41:42 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:41:42 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:41:42 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:41:42 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:41:42 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:41:42 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:41:42 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:41:42 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:41:42 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:42:07 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 701/918 [01:36<00:25,  8.35it/s]
10/09/2021 10:42:07 - INFO - trainer -     Num examples = 3258
10/09/2021 10:42:07 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:42:11 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:42:11 - INFO - trainer -     T-F1 = 0.9899100081810744 | 50/51 [00:03<00:00, 13.92it/s]
10/09/2021 10:42:11 - INFO - trainer -     T-F1(C) = 0.9685714285714285
10/09/2021 10:42:11 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:42:11 - INFO - trainer -     T-F1(O) = 0.9960958681271013
10/09/2021 10:42:11 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/09/2021 10:42:11 - INFO - trainer -     T-F1(S) = 0.9907493061979649
10/09/2021 10:42:11 - INFO - trainer -     T-F1(T) = 0.9735006973500697
10/09/2021 10:42:11 - INFO - trainer -     U-F1(A) = 0.7015706806282722
10/09/2021 10:42:11 - INFO - trainer -     U-F1(E) = 0.7627737226277371
10/09/2021 10:42:11 - INFO - trainer -     U-F1(I) = 0.14285714285714285
10/09/2021 10:42:11 - INFO - trainer -     U-F1(O) = 0.9639058413251961
10/09/2021 10:42:11 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:42:11 - INFO - trainer -     loss = 0.4749208475444831
10/09/2021 10:42:11 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:42:11 - INFO - trainer -     slot_f1 = 0.989679218967922
10/09/2021 10:42:11 - INFO - trainer -     slot_precision = 0.9885762050710505
10/09/2021 10:42:11 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:42:11 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:42:11 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:42:11 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:42:11 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:42:11 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:42:11 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:42:11 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:42:11 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:42:11 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:42:11 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:42:11 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:42:11 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:42:11 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:42:11 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:42:11 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:42:11 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:42:11 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:42:36 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 901/918 [02:05<00:02,  7.89it/s]
10/09/2021 10:42:36 - INFO - trainer -     Num examples = 3258
10/09/2021 10:42:36 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.05it/s]
10/09/2021 10:42:40 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:42:40 - INFO - trainer -     T-F1 = 0.9889660809154066 | 50/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:42:40 - INFO - trainer -     T-F1(C) = 0.9699570815450644
10/09/2021 10:42:40 - INFO - trainer -     T-F1(L) = 0.9850746268656716
10/09/2021 10:42:40 - INFO - trainer -     T-F1(O) = 0.9957146731760239
10/09/2021 10:42:40 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/09/2021 10:42:40 - INFO - trainer -     T-F1(S) = 0.9898523985239852
10/09/2021 10:42:40 - INFO - trainer -     T-F1(T) = 0.9707927677329623
10/09/2021 10:42:40 - INFO - trainer -     U-F1(A) = 0.7040816326530611
10/09/2021 10:42:40 - INFO - trainer -     U-F1(E) = 0.7759562841530055
10/09/2021 10:42:40 - INFO - trainer -     U-F1(I) = 0.2
10/09/2021 10:42:40 - INFO - trainer -     U-F1(O) = 0.9656255452800557
10/09/2021 10:42:40 - INFO - trainer -     intent_acc = 0.9370779619398404
10/09/2021 10:42:40 - INFO - trainer -     loss = 0.4640964655680399
10/09/2021 10:42:40 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:42:40 - INFO - trainer -     slot_f1 = 0.9887139473317542
10/09/2021 10:42:40 - INFO - trainer -     slot_precision = 0.9866518353726362
10/09/2021 10:42:40 - INFO - trainer -     slot_recall = 0.9907846970120078

10/09/2021 10:42:40 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:42:40 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:42:40 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:42:40 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:42:40 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:42:40 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:42:40 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:42:40 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:42:40 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:42:40 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:42:40 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:42:40 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:42:40 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:42:40 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:42:40 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:42:40 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:42:40 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:11<00:00,  6.96it/s]
Epoch:  60%|                             | 12/20 [25:47<17:18, 129.78s/it]10/09/2021 10:43:04 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 183/918 [00:22<01:27,  8.36it/s]
10/09/2021 10:43:04 - INFO - trainer -     Num examples = 3258
10/09/2021 10:43:04 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.35it/s]
10/09/2021 10:43:08 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:43:08 - INFO - trainer -     T-F1 = 0.9893790849673203 | 50/51 [00:03<00:00, 13.98it/s]
10/09/2021 10:43:08 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/09/2021 10:43:08 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:43:08 - INFO - trainer -     T-F1(O) = 0.9958767361111112
10/09/2021 10:43:08 - INFO - trainer -     T-F1(P) = 0.9953271028037383
10/09/2021 10:43:08 - INFO - trainer -     T-F1(S) = 0.9903091832025841
10/09/2021 10:43:08 - INFO - trainer -     T-F1(T) = 0.9762237762237763
10/09/2021 10:43:08 - INFO - trainer -     U-F1(A) = 0.6918918918918919
10/09/2021 10:43:08 - INFO - trainer -     U-F1(E) = 0.75
10/09/2021 10:43:08 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:43:08 - INFO - trainer -     U-F1(O) = 0.9643104643104643
10/09/2021 10:43:08 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:43:08 - INFO - trainer -     loss = 0.483757417429896
10/09/2021 10:43:08 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/09/2021 10:43:08 - INFO - trainer -     slot_f1 = 0.9891364902506964
10/09/2021 10:43:08 - INFO - trainer -     slot_precision = 0.9866629619338705
10/09/2021 10:43:08 - INFO - trainer -     slot_recall = 0.991622451829098

10/09/2021 10:43:08 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:43:08 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:43:08 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:43:08 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:43:08 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:43:08 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:43:08 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:43:08 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:43:08 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:43:08 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:43:08 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:43:08 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:43:08 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:43:08 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:43:08 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:43:08 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:43:08 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:43:32 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 383/918 [00:50<01:04,  8.32it/s]
10/09/2021 10:43:32 - INFO - trainer -     Num examples = 3258
10/09/2021 10:43:32 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.97it/s]
10/09/2021 10:43:36 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:43:36 - INFO - trainer -     T-F1 = 0.9900504293307891 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:43:36 - INFO - trainer -     T-F1(C) = 0.9713467048710601
10/09/2021 10:43:36 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:43:36 - INFO - trainer -     T-F1(O) = 0.9961494658061716
10/09/2021 10:43:36 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/09/2021 10:43:36 - INFO - trainer -     T-F1(S) = 0.9907749077490775
10/09/2021 10:43:36 - INFO - trainer -     T-F1(T) = 0.9748603351955307
10/09/2021 10:43:36 - INFO - trainer -     U-F1(A) = 0.6907216494845361
10/09/2021 10:43:36 - INFO - trainer -     U-F1(E) = 0.7658802177858439
10/09/2021 10:43:36 - INFO - trainer -     U-F1(I) = 0.22222222222222224
10/09/2021 10:43:36 - INFO - trainer -     U-F1(O) = 0.963674467341949
10/09/2021 10:43:36 - INFO - trainer -     intent_acc = 0.9337016574585635
10/09/2021 10:43:36 - INFO - trainer -     loss = 0.4869991494525297
10/09/2021 10:43:36 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:43:36 - INFO - trainer -     slot_f1 = 0.9898229471629723
10/09/2021 10:43:36 - INFO - trainer -     slot_precision = 0.9883073496659243
10/09/2021 10:43:36 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:43:36 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:43:36 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:43:36 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:43:36 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:43:36 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:43:36 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:43:36 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:43:36 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:43:36 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:43:36 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:43:36 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:43:36 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:43:36 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:43:36 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:43:36 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:43:36 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:43:36 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:44:00 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 583/918 [01:18<00:40,  8.32it/s]
10/09/2021 10:44:00 - INFO - trainer -     Num examples = 3258
10/09/2021 10:44:00 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:44:04 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:44:04 - INFO - trainer -     T-F1 = 0.9899127589967284 | 50/51 [00:03<00:00, 13.90it/s]
10/09/2021 10:44:04 - INFO - trainer -     T-F1(C) = 0.9701280227596016
10/09/2021 10:44:04 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:44:04 - INFO - trainer -     T-F1(O) = 0.9960954446854664
10/09/2021 10:44:04 - INFO - trainer -     T-F1(P) = 0.996780801872988
10/09/2021 10:44:04 - INFO - trainer -     T-F1(S) = 0.9912240184757506
10/09/2021 10:44:04 - INFO - trainer -     T-F1(T) = 0.9735006973500697
10/09/2021 10:44:04 - INFO - trainer -     U-F1(A) = 0.7128712871287128
10/09/2021 10:44:04 - INFO - trainer -     U-F1(E) = 0.7615526802218114
10/09/2021 10:44:04 - INFO - trainer -     U-F1(I) = 0.14285714285714285
10/09/2021 10:44:04 - INFO - trainer -     U-F1(O) = 0.9642296283371139
10/09/2021 10:44:04 - INFO - trainer -     intent_acc = 0.9343155310006138
10/09/2021 10:44:04 - INFO - trainer -     loss = 0.48943069504172193
10/09/2021 10:44:04 - INFO - trainer -     semantic_frame_acc = 0.9149785144260283
10/09/2021 10:44:04 - INFO - trainer -     slot_f1 = 0.9896820970440602
10/09/2021 10:44:04 - INFO - trainer -     slot_precision = 0.9883040935672515
10/09/2021 10:44:04 - INFO - trainer -     slot_recall = 0.9910639486177045

10/09/2021 10:44:04 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:44:04 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:44:04 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:44:04 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:44:04 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:44:04 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:44:04 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:44:04 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:44:04 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:44:04 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:44:04 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:44:04 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:44:04 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:44:04 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:44:04 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:44:04 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:44:04 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:44:28 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 783/918 [01:46<00:16,  8.36it/s]
10/09/2021 10:44:28 - INFO - trainer -     Num examples = 3258
10/09/2021 10:44:28 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:44:32 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:44:32 - INFO - trainer -     T-F1 = 0.9900477164280845 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:44:32 - INFO - trainer -     T-F1(C) = 0.972818311874106
10/09/2021 10:44:32 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:44:32 - INFO - trainer -     T-F1(O) = 0.9961498834119625
10/09/2021 10:44:32 - INFO - trainer -     T-F1(P) = 0.996780801872988
10/09/2021 10:44:32 - INFO - trainer -     T-F1(S) = 0.9898430286241922
10/09/2021 10:44:32 - INFO - trainer -     T-F1(T) = 0.9735006973500697
10/09/2021 10:44:32 - INFO - trainer -     U-F1(A) = 0.6806282722513088
10/09/2021 10:44:32 - INFO - trainer -     U-F1(E) = 0.7726432532347505
10/09/2021 10:44:32 - INFO - trainer -     U-F1(I) = 0.20833333333333334
10/09/2021 10:44:32 - INFO - trainer -     U-F1(O) = 0.9640864714086472
10/09/2021 10:44:32 - INFO - trainer -     intent_acc = 0.9343155310006138
10/09/2021 10:44:32 - INFO - trainer -     loss = 0.49412376141431286
10/09/2021 10:44:32 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/09/2021 10:44:32 - INFO - trainer -     slot_f1 = 0.9898201087714406
10/09/2021 10:44:32 - INFO - trainer -     slot_precision = 0.9885793871866295
10/09/2021 10:44:32 - INFO - trainer -     slot_recall = 0.9910639486177045

10/09/2021 10:44:32 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:44:32 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:44:32 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:44:32 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:44:32 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:44:32 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:44:32 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:44:32 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:44:32 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:44:32 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:44:32 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:44:32 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:44:32 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:44:32 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:44:32 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:44:32 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:44:32 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.24it/s]
Epoch:  65%|                          | 13/20 [27:53<15:01, 128.85s/it]10/09/2021 10:44:56 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 65/918 [00:07<01:41,  8.37it/s]
10/09/2021 10:44:56 - INFO - trainer -     Num examples = 3258
10/09/2021 10:44:56 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:45:00 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:45:00 - INFO - trainer -     T-F1 = 0.9907407407407407 | 50/51 [00:03<00:00, 13.94it/s]
10/09/2021 10:45:00 - INFO - trainer -     T-F1(C) = 0.9743589743589743
10/09/2021 10:45:00 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:45:00 - INFO - trainer -     T-F1(O) = 0.9964192708333334
10/09/2021 10:45:00 - INFO - trainer -     T-F1(P) = 0.9973676513600468
10/09/2021 10:45:00 - INFO - trainer -     T-F1(S) = 0.9930715935334873
10/09/2021 10:45:00 - INFO - trainer -     T-F1(T) = 0.9667590027700832
10/09/2021 10:45:00 - INFO - trainer -     U-F1(A) = 0.6448087431693988
10/09/2021 10:45:00 - INFO - trainer -     U-F1(E) = 0.7518248175182483
10/09/2021 10:45:00 - INFO - trainer -     U-F1(I) = 0.2
10/09/2021 10:45:00 - INFO - trainer -     U-F1(O) = 0.9618798955613578
10/09/2021 10:45:00 - INFO - trainer -     intent_acc = 0.9306322897483118
10/09/2021 10:45:00 - INFO - trainer -     loss = 0.5213823821030411
10/09/2021 10:45:00 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/09/2021 10:45:00 - INFO - trainer -     slot_f1 = 0.9905292479108635
10/09/2021 10:45:00 - INFO - trainer -     slot_precision = 0.9880522367324257
10/09/2021 10:45:00 - INFO - trainer -     slot_recall = 0.9930187098575817

10/09/2021 10:45:00 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:45:00 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:45:00 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:45:00 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:45:00 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:45:00 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:45:00 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:45:00 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:45:00 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:45:00 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:45:00 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:45:00 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:45:00 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:45:00 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:45:00 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:45:00 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:45:00 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:45:24 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 265/918 [00:35<01:18,  8.37it/s]
10/09/2021 10:45:24 - INFO - trainer -     Num examples = 3258
10/09/2021 10:45:24 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.02it/s]
10/09/2021 10:45:28 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:45:28 - INFO - trainer -     T-F1 = 0.9899264906071332 | 50/51 [00:03<00:00, 13.90it/s]
10/09/2021 10:45:28 - INFO - trainer -     T-F1(C) = 0.9743589743589743
10/09/2021 10:45:28 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:45:28 - INFO - trainer -     T-F1(O) = 0.9960933260987519
10/09/2021 10:45:28 - INFO - trainer -     T-F1(P) = 0.9970760233918129
10/09/2021 10:45:28 - INFO - trainer -     T-F1(S) = 0.9907834101382489
10/09/2021 10:45:28 - INFO - trainer -     T-F1(T) = 0.9680111265646731
10/09/2021 10:45:28 - INFO - trainer -     U-F1(A) = 0.6214689265536723
10/09/2021 10:45:28 - INFO - trainer -     U-F1(E) = 0.75
10/09/2021 10:45:28 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/09/2021 10:45:28 - INFO - trainer -     U-F1(O) = 0.9626168224299065
10/09/2021 10:45:28 - INFO - trainer -     intent_acc = 0.9315531000613874
10/09/2021 10:45:28 - INFO - trainer -     loss = 0.5354539115054935
10/09/2021 10:45:28 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/09/2021 10:45:28 - INFO - trainer -     slot_f1 = 0.9896964633806739
10/09/2021 10:45:28 - INFO - trainer -     slot_precision = 0.986948069980561
10/09/2021 10:45:28 - INFO - trainer -     slot_recall = 0.9924602066461882

10/09/2021 10:45:28 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:45:28 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:45:28 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:45:28 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:45:28 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:45:28 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:45:28 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:45:28 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:45:28 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:45:28 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:45:28 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:45:28 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:45:28 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:45:28 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:45:28 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:45:28 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:45:28 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:45:52 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 465/918 [01:03<00:54,  8.39it/s]
10/09/2021 10:45:52 - INFO - trainer -     Num examples = 3258
10/09/2021 10:45:52 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:45:56 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:45:56 - INFO - trainer -     T-F1 = 0.9888556673008969 | 50/51 [00:03<00:00, 13.93it/s]
10/09/2021 10:45:56 - INFO - trainer -     T-F1(C) = 0.9729729729729729
10/09/2021 10:45:56 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:45:56 - INFO - trainer -     T-F1(O) = 0.9956564230643936
10/09/2021 10:45:56 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/09/2021 10:45:56 - INFO - trainer -     T-F1(S) = 0.9885057471264368
10/09/2021 10:45:56 - INFO - trainer -     T-F1(T) = 0.9655172413793105
10/09/2021 10:45:56 - INFO - trainer -     U-F1(A) = 0.627027027027027
10/09/2021 10:45:56 - INFO - trainer -     U-F1(E) = 0.7441860465116279
10/09/2021 10:45:56 - INFO - trainer -     U-F1(I) = 0.15
10/09/2021 10:45:56 - INFO - trainer -     U-F1(O) = 0.9624242424242424
10/09/2021 10:45:56 - INFO - trainer -     intent_acc = 0.9306322897483118
10/09/2021 10:45:56 - INFO - trainer -     loss = 0.5006912948161948
10/09/2021 10:45:56 - INFO - trainer -     semantic_frame_acc = 0.90914671577655
10/09/2021 10:45:56 - INFO - trainer -     slot_f1 = 0.9884610037536494
10/09/2021 10:45:56 - INFO - trainer -     slot_precision = 0.9842192691029901
10/09/2021 10:45:56 - INFO - trainer -     slot_recall = 0.9927394582518849

10/09/2021 10:45:56 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:45:56 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:45:56 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:45:56 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:45:56 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:45:56 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:45:56 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:45:56 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:45:56 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:45:56 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:45:56 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:45:56 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:45:56 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:45:56 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:45:56 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:45:56 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:45:56 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:46:20 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 665/918 [01:31<00:30,  8.33it/s]
10/09/2021 10:46:20 - INFO - trainer -     Num examples = 3258
10/09/2021 10:46:20 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:46:24 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:46:24 - INFO - trainer -     T-F1 = 0.9905827760338475 | 50/51 [00:03<00:00, 13.96it/s]
10/09/2021 10:46:24 - INFO - trainer -     T-F1(C) = 0.9684813753581661
10/09/2021 10:46:24 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:46:24 - INFO - trainer -     T-F1(O) = 0.9963683668491518
10/09/2021 10:46:24 - INFO - trainer -     T-F1(P) = 0.9970743124634289
10/09/2021 10:46:24 - INFO - trainer -     T-F1(S) = 0.9921405455386038
10/09/2021 10:46:24 - INFO - trainer -     T-F1(T) = 0.9775910364145658
10/09/2021 10:46:24 - INFO - trainer -     U-F1(A) = 0.6734693877551021
10/09/2021 10:46:24 - INFO - trainer -     U-F1(E) = 0.7509578544061303
10/09/2021 10:46:24 - INFO - trainer -     U-F1(I) = 0.13953488372093023
10/09/2021 10:46:24 - INFO - trainer -     U-F1(O) = 0.9640312771503041
10/09/2021 10:46:24 - INFO - trainer -     intent_acc = 0.932780847145488
10/09/2021 10:46:24 - INFO - trainer -     loss = 0.5269720626666266
10/09/2021 10:46:24 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/09/2021 10:46:24 - INFO - trainer -     slot_f1 = 0.9903671645958397
10/09/2021 10:46:24 - INFO - trainer -     slot_precision = 0.9902289223897264
10/09/2021 10:46:24 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:46:24 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:46:24 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:46:24 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:46:24 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:46:24 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:46:24 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:46:24 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:46:24 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:46:24 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:46:24 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:46:24 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:46:24 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:46:24 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:46:24 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:46:24 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:46:24 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:46:24 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:46:48 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 865/918 [01:59<00:06,  8.29it/s]
10/09/2021 10:46:48 - INFO - trainer -     Num examples = 3258
10/09/2021 10:46:48 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.04it/s]
10/09/2021 10:46:52 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:46:52 - INFO - trainer -     T-F1 = 0.9904554131442597 | 50/51 [00:03<00:00, 13.92it/s]
10/09/2021 10:46:52 - INFO - trainer -     T-F1(C) = 0.9754689754689755
10/09/2021 10:46:52 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:46:52 - INFO - trainer -     T-F1(O) = 0.9963127643422622
10/09/2021 10:46:52 - INFO - trainer -     T-F1(P) = 0.9964953271028038
10/09/2021 10:46:52 - INFO - trainer -     T-F1(S) = 0.9921622867680959
10/09/2021 10:46:52 - INFO - trainer -     T-F1(T) = 0.9719887955182073
10/09/2021 10:46:52 - INFO - trainer -     U-F1(A) = 0.7064676616915423
10/09/2021 10:46:52 - INFO - trainer -     U-F1(E) = 0.7723132969034608
10/09/2021 10:46:52 - INFO - trainer -     U-F1(I) = 0.10256410256410256
10/09/2021 10:46:52 - INFO - trainer -     U-F1(O) = 0.965601536581107
10/09/2021 10:46:52 - INFO - trainer -     intent_acc = 0.9361571516267649
10/09/2021 10:46:52 - INFO - trainer -     loss = 0.5031445317116439
10/09/2021 10:46:52 - INFO - trainer -     semantic_frame_acc = 0.91804788213628
10/09/2021 10:46:52 - INFO - trainer -     slot_f1 = 0.9902370990237099
10/09/2021 10:46:52 - INFO - trainer -     slot_precision = 0.9891334633602675
10/09/2021 10:46:52 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:46:52 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:46:52 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:46:52 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:46:52 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:46:52 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:46:52 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:46:52 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:46:52 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:46:52 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:46:52 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:46:52 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:46:52 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:46:52 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:46:52 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:46:52 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:46:52 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:46:52 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.06it/s]
Epoch:  70%|                      | 14/20 [30:03<12:55, 129.21s/it]10/09/2021 10:47:16 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 147/918 [00:17<01:32,  8.31it/s]
10/09/2021 10:47:16 - INFO - trainer -     Num examples = 3258
10/09/2021 10:47:16 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:47:20 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:47:20 - INFO - trainer -     T-F1 = 0.9886936384688735 | 50/51 [00:03<00:00, 13.91it/s]
10/09/2021 10:47:20 - INFO - trainer -     T-F1(C) = 0.9685714285714285
10/09/2021 10:47:20 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:47:20 - INFO - trainer -     T-F1(O) = 0.9956061838893409
10/09/2021 10:47:20 - INFO - trainer -     T-F1(P) = 0.9953271028037383
10/09/2021 10:47:20 - INFO - trainer -     T-F1(S) = 0.9916974169741698
10/09/2021 10:47:20 - INFO - trainer -     T-F1(T) = 0.9678321678321679
10/09/2021 10:47:20 - INFO - trainer -     U-F1(A) = 0.7035175879396984
10/09/2021 10:47:20 - INFO - trainer -     U-F1(E) = 0.7638376383763837
10/09/2021 10:47:20 - INFO - trainer -     U-F1(I) = 0.18604651162790697
10/09/2021 10:47:20 - INFO - trainer -     U-F1(O) = 0.9654570830425679
10/09/2021 10:47:20 - INFO - trainer -     intent_acc = 0.9355432780847146
10/09/2021 10:47:20 - INFO - trainer -     loss = 0.5226778902259528
10/09/2021 10:47:20 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/09/2021 10:47:20 - INFO - trainer -     slot_f1 = 0.988435279364637
10/09/2021 10:47:20 - INFO - trainer -     slot_precision = 0.9863737486095662
10/09/2021 10:47:20 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:47:20 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:47:20 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:47:20 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:47:20 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:47:20 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:47:20 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:47:20 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:47:20 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:47:20 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:47:20 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:47:20 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:47:20 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:47:20 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:47:20 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:47:20 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:47:20 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:47:20 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:47:44 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 347/918 [00:45<01:08,  8.36it/s]
10/09/2021 10:47:44 - INFO - trainer -     Num examples = 3258
10/09/2021 10:47:44 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:47:48 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:47:48 - INFO - trainer -     T-F1 = 0.988036976617727 | 50/51 [00:03<00:00, 13.89it/s]
10/09/2021 10:47:48 - INFO - trainer -     T-F1(C) = 0.9618104667609618
10/09/2021 10:47:48 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:47:48 - INFO - trainer -     T-F1(O) = 0.9952225841476656
10/09/2021 10:47:48 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/09/2021 10:47:48 - INFO - trainer -     T-F1(S) = 0.9908003679852806
10/09/2021 10:47:48 - INFO - trainer -     T-F1(T) = 0.9653259361997226
10/09/2021 10:47:48 - INFO - trainer -     U-F1(A) = 0.7000000000000001
10/09/2021 10:47:48 - INFO - trainer -     U-F1(E) = 0.7431906614785991
10/09/2021 10:47:48 - INFO - trainer -     U-F1(I) = 0.15
10/09/2021 10:47:48 - INFO - trainer -     U-F1(O) = 0.9645956265185699
10/09/2021 10:47:48 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:47:48 - INFO - trainer -     loss = 0.5390403509578284
10/09/2021 10:47:48 - INFO - trainer -     semantic_frame_acc = 0.9100675260896255
10/09/2021 10:47:48 - INFO - trainer -     slot_f1 = 0.9877641824249166
10/09/2021 10:47:48 - INFO - trainer -     slot_precision = 0.9836610357241762
10/09/2021 10:47:48 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:47:48 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:47:48 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:47:48 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:47:48 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:47:48 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:47:48 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:47:48 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:47:48 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:47:48 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:47:48 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:47:48 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:47:48 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:47:48 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:47:48 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:47:48 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:47:48 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:47:48 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:48:12 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 547/918 [01:13<00:44,  8.32it/s]
10/09/2021 10:48:12 - INFO - trainer -     Num examples = 3258
10/09/2021 10:48:12 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.01it/s]
10/09/2021 10:48:16 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:48:16 - INFO - trainer -     T-F1 = 0.9897638869933124 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:48:16 - INFO - trainer -     T-F1(C) = 0.9713467048710601
10/09/2021 10:48:16 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:48:16 - INFO - trainer -     T-F1(O) = 0.9960431459699711
10/09/2021 10:48:16 - INFO - trainer -     T-F1(P) = 0.9956127522667446
10/09/2021 10:48:16 - INFO - trainer -     T-F1(S) = 0.9925994449583718
10/09/2021 10:48:16 - INFO - trainer -     T-F1(T) = 0.9719887955182073
10/09/2021 10:48:16 - INFO - trainer -     U-F1(A) = 0.6868686868686869
10/09/2021 10:48:16 - INFO - trainer -     U-F1(E) = 0.7514450867052023
10/09/2021 10:48:16 - INFO - trainer -     U-F1(I) = 0.1818181818181818
10/09/2021 10:48:16 - INFO - trainer -     U-F1(O) = 0.9647263249348392
10/09/2021 10:48:16 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:48:16 - INFO - trainer -     loss = 0.5427750726686973
10/09/2021 10:48:16 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/09/2021 10:48:16 - INFO - trainer -     slot_f1 = 0.9895295267346085
10/09/2021 10:48:16 - INFO - trainer -     slot_precision = 0.989391401451703
10/09/2021 10:48:16 - INFO - trainer -     slot_recall = 0.9896676905892209

10/09/2021 10:48:16 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:48:16 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:48:16 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:48:16 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:48:16 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:48:16 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:48:16 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:48:16 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:48:16 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:48:16 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:48:16 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:48:16 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:48:16 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:48:16 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:48:16 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:48:16 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:48:16 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:48:40 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 747/918 [01:41<00:20,  8.35it/s]
10/09/2021 10:48:40 - INFO - trainer -     Num examples = 3258
10/09/2021 10:48:40 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:48:44 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:48:44 - INFO - trainer -     T-F1 = 0.9900504293307891 | 50/51 [00:03<00:00, 13.89it/s]
10/09/2021 10:48:44 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/09/2021 10:48:44 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:48:44 - INFO - trainer -     T-F1(O) = 0.9961494658061716
10/09/2021 10:48:44 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/09/2021 10:48:44 - INFO - trainer -     T-F1(S) = 0.9917050691244239
10/09/2021 10:48:44 - INFO - trainer -     T-F1(T) = 0.9733520336605891
10/09/2021 10:48:44 - INFO - trainer -     U-F1(A) = 0.7219512195121951
10/09/2021 10:48:44 - INFO - trainer -     U-F1(E) = 0.7533460803059274
10/09/2021 10:48:44 - INFO - trainer -     U-F1(I) = 0.16216216216216217
10/09/2021 10:48:44 - INFO - trainer -     U-F1(O) = 0.9657450878108155
10/09/2021 10:48:44 - INFO - trainer -     intent_acc = 0.93646408839779
10/09/2021 10:48:44 - INFO - trainer -     loss = 0.5315733494273588
10/09/2021 10:48:44 - INFO - trainer -     semantic_frame_acc = 0.9171270718232044
10/09/2021 10:48:44 - INFO - trainer -     slot_f1 = 0.9898229471629723
10/09/2021 10:48:44 - INFO - trainer -     slot_precision = 0.9883073496659243
10/09/2021 10:48:44 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:48:44 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:48:44 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:48:44 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:48:44 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:48:44 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:48:44 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:48:44 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:48:44 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:48:44 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:48:44 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:48:44 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:48:44 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:48:44 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:48:44 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:48:44 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:48:44 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:48:44 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.28it/s]
Epoch:  75%|                  | 15/20 [32:09<10:41, 128.28s/it]10/09/2021 10:49:08 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 29/918 [00:03<01:46,  8.32it/s]
10/09/2021 10:49:08 - INFO - trainer -     Num examples = 3258
10/09/2021 10:49:08 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.00it/s]
10/09/2021 10:49:12 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:49:12 - INFO - trainer -     T-F1 = 0.990988530857455 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:49:12 - INFO - trainer -     T-F1(C) = 0.9741379310344828
10/09/2021 10:49:12 - INFO - trainer -     T-F1(L) = 0.9880239520958084
10/09/2021 10:49:12 - INFO - trainer -     T-F1(O) = 0.9965315412963365
10/09/2021 10:49:12 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/09/2021 10:49:12 - INFO - trainer -     T-F1(S) = 0.9935125115848007
10/09/2021 10:49:12 - INFO - trainer -     T-F1(T) = 0.9747899159663864
10/09/2021 10:49:12 - INFO - trainer -     U-F1(A) = 0.6839378238341969
10/09/2021 10:49:12 - INFO - trainer -     U-F1(E) = 0.7518796992481204
10/09/2021 10:49:12 - INFO - trainer -     U-F1(I) = 0.1702127659574468
10/09/2021 10:49:12 - INFO - trainer -     U-F1(O) = 0.9627437325905293
10/09/2021 10:49:12 - INFO - trainer -     intent_acc = 0.9315531000613874
10/09/2021 10:49:12 - INFO - trainer -     loss = 0.5370095517194154
10/09/2021 10:49:12 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/09/2021 10:49:12 - INFO - trainer -     slot_f1 = 0.990782122905028
10/09/2021 10:49:12 - INFO - trainer -     slot_precision = 0.9910589550153674
10/09/2021 10:49:12 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:49:12 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:49:12 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:49:12 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:49:12 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:49:12 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:49:12 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:49:12 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:49:12 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:49:12 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:49:12 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:49:12 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:49:12 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:49:12 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:49:12 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:49:12 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:49:12 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:49:12 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:49:36 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 229/918 [00:31<01:22,  8.36it/s]
10/09/2021 10:49:36 - INFO - trainer -     Num examples = 3258
10/09/2021 10:49:36 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.03it/s]
10/09/2021 10:49:40 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:49:40 - INFO - trainer -     T-F1 = 0.9905930470347649 | 50/51 [00:03<00:00, 13.93it/s]
10/09/2021 10:49:40 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:49:40 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:49:40 - INFO - trainer -     T-F1(O) = 0.9963667913887534
10/09/2021 10:49:40 - INFO - trainer -     T-F1(P) = 0.9964912280701754
10/09/2021 10:49:40 - INFO - trainer -     T-F1(S) = 0.9921550530687586
10/09/2021 10:49:40 - INFO - trainer -     T-F1(T) = 0.9733520336605891
10/09/2021 10:49:40 - INFO - trainer -     U-F1(A) = 0.6868686868686869
10/09/2021 10:49:40 - INFO - trainer -     U-F1(E) = 0.7354085603112841
10/09/2021 10:49:40 - INFO - trainer -     U-F1(I) = 0.14634146341463417
10/09/2021 10:49:40 - INFO - trainer -     U-F1(O) = 0.9630400832899532
10/09/2021 10:49:40 - INFO - trainer -     intent_acc = 0.9315531000613874
10/09/2021 10:49:40 - INFO - trainer -     loss = 0.5603273823565128
10/09/2021 10:49:40 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/09/2021 10:49:40 - INFO - trainer -     slot_f1 = 0.9903779110305397
10/09/2021 10:49:40 - INFO - trainer -     slot_precision = 0.9891364902506964
10/09/2021 10:49:40 - INFO - trainer -     slot_recall = 0.991622451829098

10/09/2021 10:49:40 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:49:40 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:49:40 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:49:40 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:49:40 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:49:40 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:49:40 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:49:40 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:49:40 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:49:40 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:49:40 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:49:40 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:49:40 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:49:40 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:49:40 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:49:40 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:49:40 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:50:04 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 429/918 [00:59<00:59,  8.21it/s]
10/09/2021 10:50:04 - INFO - trainer -     Num examples = 3258
10/09/2021 10:50:04 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.85it/s]
10/09/2021 10:50:08 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:50:08 - INFO - trainer -     T-F1 = 0.9896514161220045 | 50/51 [00:03<00:00, 13.55it/s]
10/09/2021 10:50:08 - INFO - trainer -     T-F1(C) = 0.972818311874106
10/09/2021 10:50:08 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:50:08 - INFO - trainer -     T-F1(O) = 0.9959852430555555
10/09/2021 10:50:08 - INFO - trainer -     T-F1(P) = 0.9961999415375621
10/09/2021 10:50:08 - INFO - trainer -     T-F1(S) = 0.9903181189488244
10/09/2021 10:50:08 - INFO - trainer -     T-F1(T) = 0.9721448467966575
10/09/2021 10:50:08 - INFO - trainer -     U-F1(A) = 0.6162162162162163
10/09/2021 10:50:08 - INFO - trainer -     U-F1(E) = 0.7408829174664108
10/09/2021 10:50:08 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:50:08 - INFO - trainer -     U-F1(O) = 0.962051637497834
10/09/2021 10:50:08 - INFO - trainer -     intent_acc = 0.9297114794352364
10/09/2021 10:50:08 - INFO - trainer -     loss = 0.5502389469275287
10/09/2021 10:50:08 - INFO - trainer -     semantic_frame_acc = 0.9097605893186004
10/09/2021 10:50:08 - INFO - trainer -     slot_f1 = 0.9894150417827299
10/09/2021 10:50:08 - INFO - trainer -     slot_precision = 0.9869408168935816
10/09/2021 10:50:08 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:50:08 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:50:08 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:50:08 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:50:08 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:50:08 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:50:08 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:50:08 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:50:08 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:50:08 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:50:08 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:50:08 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:50:08 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:50:08 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:50:08 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:50:08 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:50:08 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:50:08 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:50:32 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 629/918 [01:27<00:35,  8.24it/s]
10/09/2021 10:50:32 - INFO - trainer -     Num examples = 3258
10/09/2021 10:50:32 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.87it/s]
10/09/2021 10:50:36 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:50:36 - INFO - trainer -     T-F1 = 0.9904580152671756 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:50:36 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:50:36 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:50:36 - INFO - trainer -     T-F1(O) = 0.9962039045553145
10/09/2021 10:50:36 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:50:36 - INFO - trainer -     T-F1(S) = 0.9907834101382489
10/09/2021 10:50:36 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/09/2021 10:50:36 - INFO - trainer -     U-F1(A) = 0.6597938144329897
10/09/2021 10:50:36 - INFO - trainer -     U-F1(E) = 0.7466666666666667
10/09/2021 10:50:36 - INFO - trainer -     U-F1(I) = 0.10526315789473684
10/09/2021 10:50:36 - INFO - trainer -     U-F1(O) = 0.9630144122243445
10/09/2021 10:50:36 - INFO - trainer -     intent_acc = 0.9315531000613874
10/09/2021 10:50:36 - INFO - trainer -     loss = 0.5643183574372647
10/09/2021 10:50:36 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/09/2021 10:50:36 - INFO - trainer -     slot_f1 = 0.990239821528165
10/09/2021 10:50:36 - INFO - trainer -     slot_precision = 0.9888610414926204
10/09/2021 10:50:36 - INFO - trainer -     slot_recall = 0.991622451829098

10/09/2021 10:50:36 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:50:36 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:50:36 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:50:36 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:50:36 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:50:36 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:50:36 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:50:36 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:50:36 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:50:36 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:50:36 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:50:36 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:50:36 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:50:36 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:50:36 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:50:36 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:50:36 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:51:01 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 829/918 [01:56<00:10,  8.24it/s]
10/09/2021 10:51:01 - INFO - trainer -     Num examples = 3258
10/09/2021 10:51:01 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.82it/s]
10/09/2021 10:51:05 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:51:05 - INFO - trainer -     T-F1 = 0.990865712338105 | 50/51 [00:03<00:00, 13.67it/s]
10/09/2021 10:51:05 - INFO - trainer -     T-F1(C) = 0.9756097560975611
10/09/2021 10:51:05 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:51:05 - INFO - trainer -     T-F1(O) = 0.9963667913887534
10/09/2021 10:51:05 - INFO - trainer -     T-F1(P) = 0.9964932787843368
10/09/2021 10:51:05 - INFO - trainer -     T-F1(S) = 0.9912402028584602
10/09/2021 10:51:05 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/09/2021 10:51:05 - INFO - trainer -     U-F1(A) = 0.6934673366834171
10/09/2021 10:51:05 - INFO - trainer -     U-F1(E) = 0.766355140186916
10/09/2021 10:51:05 - INFO - trainer -     U-F1(I) = 0.20512820512820512
10/09/2021 10:51:05 - INFO - trainer -     U-F1(O) = 0.9656973707121714
10/09/2021 10:51:05 - INFO - trainer -     intent_acc = 0.93646408839779
10/09/2021 10:51:05 - INFO - trainer -     loss = 0.5465949494756904
10/09/2021 10:51:05 - INFO - trainer -     semantic_frame_acc = 0.9186617556783303
10/09/2021 10:51:05 - INFO - trainer -     slot_f1 = 0.9906568121600893
10/09/2021 10:51:05 - INFO - trainer -     slot_precision = 0.9894150417827298
10/09/2021 10:51:05 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:51:05 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:51:05 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:51:05 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:51:05 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:51:05 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:51:05 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:51:05 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:51:05 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:51:05 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:51:05 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:51:05 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:51:05 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:51:05 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:51:05 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:51:05 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:51:05 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:51:05 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.02it/s]
Epoch:  80%|              | 16/20 [34:20<08:36, 129.04s/it]10/09/2021 10:51:29 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 111/918 [00:13<01:36,  8.40it/s]
10/09/2021 10:51:29 - INFO - trainer -     Num examples = 3258
10/09/2021 10:51:29 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:51:33 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:51:33 - INFO - trainer -     T-F1 = 0.9911287020608708 | 50/51 [00:03<00:00, 13.52it/s]
10/09/2021 10:51:33 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:51:33 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:51:33 - INFO - trainer -     T-F1(O) = 0.9964767738088784
10/09/2021 10:51:33 - INFO - trainer -     T-F1(P) = 0.9961999415375621
10/09/2021 10:51:33 - INFO - trainer -     T-F1(S) = 0.993058769088385
10/09/2021 10:51:33 - INFO - trainer -     T-F1(T) = 0.9775280898876404
10/09/2021 10:51:33 - INFO - trainer -     U-F1(A) = 0.7024390243902437
10/09/2021 10:51:33 - INFO - trainer -     U-F1(E) = 0.7387033398821218
10/09/2021 10:51:33 - INFO - trainer -     U-F1(I) = 0.18604651162790697
10/09/2021 10:51:33 - INFO - trainer -     U-F1(O) = 0.9644035422816462
10/09/2021 10:51:33 - INFO - trainer -     intent_acc = 0.9333947206875384
10/09/2021 10:51:33 - INFO - trainer -     loss = 0.5669296519721255
10/09/2021 10:51:33 - INFO - trainer -     semantic_frame_acc = 0.916206261510129
10/09/2021 10:51:33 - INFO - trainer -     slot_f1 = 0.9909255898366607
10/09/2021 10:51:33 - INFO - trainer -     slot_precision = 0.990787269681742
10/09/2021 10:51:33 - INFO - trainer -     slot_recall = 0.9910639486177045

10/09/2021 10:51:33 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:51:33 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:51:33 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:51:33 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:51:33 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:51:33 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:51:33 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:51:33 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:51:33 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:51:33 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:51:33 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:51:33 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:51:33 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:51:33 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:51:33 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:51:33 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:51:33 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:51:57 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 311/918 [00:41<01:12,  8.36it/s]
10/09/2021 10:51:57 - INFO - trainer -     Num examples = 3258
10/09/2021 10:51:57 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:52:01 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:52:01 - INFO - trainer -     T-F1 = 0.9904449904449905 | 50/51 [00:03<00:00, 13.74it/s]
10/09/2021 10:52:01 - INFO - trainer -     T-F1(C) = 0.9756097560975611
10/09/2021 10:52:01 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/09/2021 10:52:01 - INFO - trainer -     T-F1(O) = 0.9963143631436314
10/09/2021 10:52:01 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:52:01 - INFO - trainer -     T-F1(S) = 0.9921259842519685
10/09/2021 10:52:01 - INFO - trainer -     T-F1(T) = 0.9747899159663864
10/09/2021 10:52:01 - INFO - trainer -     U-F1(A) = 0.6666666666666666
10/09/2021 10:52:01 - INFO - trainer -     U-F1(E) = 0.7407407407407407
10/09/2021 10:52:01 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/09/2021 10:52:01 - INFO - trainer -     U-F1(O) = 0.9631041053178591
10/09/2021 10:52:01 - INFO - trainer -     intent_acc = 0.9321669736034377
10/09/2021 10:52:01 - INFO - trainer -     loss = 0.5561632743726174
10/09/2021 10:52:01 - INFO - trainer -     semantic_frame_acc = 0.9140577041129527
10/09/2021 10:52:01 - INFO - trainer -     slot_f1 = 0.9902261938006144
10/09/2021 10:52:01 - INFO - trainer -     slot_precision = 0.9902261938006144
10/09/2021 10:52:01 - INFO - trainer -     slot_recall = 0.9902261938006144

10/09/2021 10:52:01 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:52:01 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:52:01 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:52:01 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:52:01 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:52:01 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:52:01 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:52:01 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:52:01 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:52:01 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:52:01 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:52:01 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:52:01 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:52:01 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:52:01 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:52:01 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:52:01 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:52:25 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 511/918 [01:09<00:48,  8.35it/s]
10/09/2021 10:52:25 - INFO - trainer -     Num examples = 3258
10/09/2021 10:52:25 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:52:29 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:52:29 - INFO - trainer -     T-F1 = 0.9899127589967284 | 50/51 [00:03<00:00, 13.78it/s]
10/09/2021 10:52:29 - INFO - trainer -     T-F1(C) = 0.96875
10/09/2021 10:52:29 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/09/2021 10:52:29 - INFO - trainer -     T-F1(O) = 0.9959869848156183
10/09/2021 10:52:29 - INFO - trainer -     T-F1(P) = 0.9961977186311787
10/09/2021 10:52:29 - INFO - trainer -     T-F1(S) = 0.9916820702402956
10/09/2021 10:52:29 - INFO - trainer -     T-F1(T) = 0.9761570827489481
10/09/2021 10:52:29 - INFO - trainer -     U-F1(A) = 0.69
10/09/2021 10:52:29 - INFO - trainer -     U-F1(E) = 0.7437379576107901
10/09/2021 10:52:29 - INFO - trainer -     U-F1(I) = 0.19047619047619047
10/09/2021 10:52:29 - INFO - trainer -     U-F1(O) = 0.9640312771503041
10/09/2021 10:52:29 - INFO - trainer -     intent_acc = 0.9330877839165131
10/09/2021 10:52:29 - INFO - trainer -     loss = 0.5798963150192126
10/09/2021 10:52:29 - INFO - trainer -     semantic_frame_acc = 0.9137507673419276
10/09/2021 10:52:29 - INFO - trainer -     slot_f1 = 0.9896820970440602
10/09/2021 10:52:29 - INFO - trainer -     slot_precision = 0.9883040935672515
10/09/2021 10:52:29 - INFO - trainer -     slot_recall = 0.9910639486177045

10/09/2021 10:52:29 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:52:29 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:52:29 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:52:29 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:52:29 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:52:29 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:52:29 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:52:29 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:52:29 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:52:29 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:52:29 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:52:29 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:52:29 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:52:29 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:52:29 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:52:29 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:52:29 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:52:53 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 711/918 [01:37<00:24,  8.34it/s]
10/09/2021 10:52:53 - INFO - trainer -     Num examples = 3258
10/09/2021 10:52:53 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.96it/s]
10/09/2021 10:52:57 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:52:57 - INFO - trainer -     T-F1 = 0.9890978468247479 | 50/51 [00:03<00:00, 13.90it/s]
10/09/2021 10:52:57 - INFO - trainer -     T-F1(C) = 0.9672830725462305
10/09/2021 10:52:57 - INFO - trainer -     T-F1(L) = 0.9880952380952381
10/09/2021 10:52:57 - INFO - trainer -     T-F1(O) = 0.9957696062479661
10/09/2021 10:52:57 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:52:57 - INFO - trainer -     T-F1(S) = 0.9907578558225508
10/09/2021 10:52:57 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:52:57 - INFO - trainer -     U-F1(A) = 0.6596858638743456
10/09/2021 10:52:57 - INFO - trainer -     U-F1(E) = 0.7480916030534351
10/09/2021 10:52:57 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:52:57 - INFO - trainer -     U-F1(O) = 0.9632072197153766
10/09/2021 10:52:57 - INFO - trainer -     intent_acc = 0.9321669736034377
10/09/2021 10:52:57 - INFO - trainer -     loss = 0.5712479100215668
10/09/2021 10:52:57 - INFO - trainer -     semantic_frame_acc = 0.9112952731737262
10/09/2021 10:52:57 - INFO - trainer -     slot_f1 = 0.988848620016727
10/09/2021 10:52:57 - INFO - trainer -     slot_precision = 0.9871973281380462
10/09/2021 10:52:57 - INFO - trainer -     slot_recall = 0.9905054454063111

10/09/2021 10:52:57 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:52:57 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:52:57 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:52:57 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:52:57 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:52:57 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:52:57 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:52:57 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:52:57 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:52:57 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:52:57 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:52:57 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:52:57 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:52:57 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:52:57 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:52:57 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:52:57 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:53:21 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 911/918 [02:05<00:00,  8.35it/s]
10/09/2021 10:53:21 - INFO - trainer -     Num examples = 3258
10/09/2021 10:53:21 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.97it/s]
10/09/2021 10:53:25 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:53:25 - INFO - trainer -     T-F1 = 0.9904658131299373 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:53:25 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/09/2021 10:53:25 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:53:25 - INFO - trainer -     T-F1(O) = 0.9962026689812303
10/09/2021 10:53:25 - INFO - trainer -     T-F1(P) = 0.9964953271028038
10/09/2021 10:53:25 - INFO - trainer -     T-F1(S) = 0.9916897506925207
10/09/2021 10:53:25 - INFO - trainer -     T-F1(T) = 0.9775910364145658
10/09/2021 10:53:25 - INFO - trainer -     U-F1(A) = 0.6735751295336788
10/09/2021 10:53:25 - INFO - trainer -     U-F1(E) = 0.7485380116959063
10/09/2021 10:53:25 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:53:25 - INFO - trainer -     U-F1(O) = 0.9641309998267198
10/09/2021 10:53:25 - INFO - trainer -     intent_acc = 0.9337016574585635
10/09/2021 10:53:25 - INFO - trainer -     loss = 0.5591785970929206
10/09/2021 10:53:25 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:53:25 - INFO - trainer -     slot_f1 = 0.9902479799387016
10/09/2021 10:53:25 - INFO - trainer -     slot_precision = 0.9880455935501807
10/09/2021 10:53:25 - INFO - trainer -     slot_recall = 0.9924602066461882

10/09/2021 10:53:25 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:53:25 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:53:25 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:53:25 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:53:25 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:53:25 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:53:25 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:53:25 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:53:25 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:53:25 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:53:25 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:53:25 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:53:25 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:53:25 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:53:25 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:53:25 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:53:25 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.03it/s]
Epoch:  85%|           | 17/20 [36:31<06:28, 129.51s/it]10/09/2021 10:53:49 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 193/918 [00:23<01:27,  8.33it/s]
10/09/2021 10:53:49 - INFO - trainer -     Num examples = 3258
10/09/2021 10:53:49 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.97it/s]
10/09/2021 10:53:53 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:53:53 - INFO - trainer -     T-F1 = 0.9901853871319519 | 50/51 [00:03<00:00, 13.91it/s]
10/09/2021 10:53:53 - INFO - trainer -     T-F1(C) = 0.9700427960057061
10/09/2021 10:53:53 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:53:53 - INFO - trainer -     T-F1(O) = 0.9960954446854664
10/09/2021 10:53:53 - INFO - trainer -     T-F1(P) = 0.9959064327485381
10/09/2021 10:53:53 - INFO - trainer -     T-F1(S) = 0.9916820702402956
10/09/2021 10:53:53 - INFO - trainer -     T-F1(T) = 0.9762237762237763
10/09/2021 10:53:53 - INFO - trainer -     U-F1(A) = 0.6965174129353233
10/09/2021 10:53:53 - INFO - trainer -     U-F1(E) = 0.7556390977443609
10/09/2021 10:53:53 - INFO - trainer -     U-F1(I) = 0.18604651162790697
10/09/2021 10:53:53 - INFO - trainer -     U-F1(O) = 0.9644599303135888
10/09/2021 10:53:53 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:53:53 - INFO - trainer -     loss = 0.5702795080986678
10/09/2021 10:53:53 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:53:53 - INFO - trainer -     slot_f1 = 0.9899609592861127
10/09/2021 10:53:53 - INFO - trainer -     slot_precision = 0.9885825675299359
10/09/2021 10:53:53 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:53:53 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:53:53 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:53:53 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:53:53 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:53:53 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:53:53 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:53:53 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:53:53 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:53:53 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:53:53 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:53:53 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:53:53 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:53:53 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:53:53 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:53:53 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:53:53 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:53:53 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:54:17 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 393/918 [00:51<01:02,  8.40it/s]
10/09/2021 10:54:17 - INFO - trainer -     Num examples = 3258
10/09/2021 10:54:17 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.97it/s]
10/09/2021 10:54:21 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:54:21 - INFO - trainer -     T-F1 = 0.9888465723612623 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:54:21 - INFO - trainer -     T-F1(C) = 0.9631728045325779
10/09/2021 10:54:21 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:54:21 - INFO - trainer -     T-F1(O) = 0.9955492835432045
10/09/2021 10:54:21 - INFO - trainer -     T-F1(P) = 0.9962021618463338
10/09/2021 10:54:21 - INFO - trainer -     T-F1(S) = 0.9903270382312298
10/09/2021 10:54:21 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:54:21 - INFO - trainer -     U-F1(A) = 0.6767676767676767
10/09/2021 10:54:21 - INFO - trainer -     U-F1(E) = 0.7611940298507464
10/09/2021 10:54:21 - INFO - trainer -     U-F1(I) = 0.19047619047619047
10/09/2021 10:54:21 - INFO - trainer -     U-F1(O) = 0.9644599303135888
10/09/2021 10:54:21 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:54:21 - INFO - trainer -     loss = 0.5546000020615026
10/09/2021 10:54:21 - INFO - trainer -     semantic_frame_acc = 0.9122160834868017
10/09/2021 10:54:21 - INFO - trainer -     slot_f1 = 0.9885920979410128
10/09/2021 10:54:21 - INFO - trainer -     slot_precision = 0.9850291100637649
10/09/2021 10:54:21 - INFO - trainer -     slot_recall = 0.9921809550404915

10/09/2021 10:54:21 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:54:21 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:54:21 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:54:21 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:54:21 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:54:21 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:54:21 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:54:21 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:54:21 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:54:21 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:54:21 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:54:21 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:54:21 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:54:21 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:54:21 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:54:21 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:54:21 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:54:45 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 593/918 [01:19<00:39,  8.30it/s]
10/09/2021 10:54:45 - INFO - trainer -     Num examples = 3258
10/09/2021 10:54:45 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:54:49 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:54:49 - INFO - trainer -     T-F1 = 0.9901853871319519 | 50/51 [00:03<00:00, 13.86it/s]
10/09/2021 10:54:49 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/09/2021 10:54:49 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:54:49 - INFO - trainer -     T-F1(O) = 0.9960954446854664
10/09/2021 10:54:49 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/09/2021 10:54:49 - INFO - trainer -     T-F1(S) = 0.9935245143385754
10/09/2021 10:54:49 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:54:49 - INFO - trainer -     U-F1(A) = 0.6734693877551021
10/09/2021 10:54:49 - INFO - trainer -     U-F1(E) = 0.7537878787878789
10/09/2021 10:54:49 - INFO - trainer -     U-F1(I) = 0.2
10/09/2021 10:54:49 - INFO - trainer -     U-F1(O) = 0.9645340751043116
10/09/2021 10:54:49 - INFO - trainer -     intent_acc = 0.9340085942295887
10/09/2021 10:54:49 - INFO - trainer -     loss = 0.5608500205710822
10/09/2021 10:54:49 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:54:49 - INFO - trainer -     slot_f1 = 0.9899609592861127
10/09/2021 10:54:49 - INFO - trainer -     slot_precision = 0.9885825675299359
10/09/2021 10:54:49 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:54:49 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:54:49 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:54:49 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:54:49 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:54:49 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:54:49 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:54:49 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:54:49 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:54:49 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:54:49 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:54:49 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:54:49 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:54:49 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:54:49 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:54:49 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:54:49 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:54:49 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:55:13 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 793/918 [01:47<00:15,  8.31it/s]
10/09/2021 10:55:13 - INFO - trainer -     Num examples = 3258
10/09/2021 10:55:13 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.00it/s]
10/09/2021 10:55:17 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:55:17 - INFO - trainer -     T-F1 = 0.989788972089857 | 50/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:55:17 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/09/2021 10:55:17 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:55:17 - INFO - trainer -     T-F1(O) = 0.9959307688134122
10/09/2021 10:55:17 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/09/2021 10:55:17 - INFO - trainer -     T-F1(S) = 0.9917050691244239
10/09/2021 10:55:17 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:55:17 - INFO - trainer -     U-F1(A) = 0.6834170854271356
10/09/2021 10:55:17 - INFO - trainer -     U-F1(E) = 0.7514231499051233
10/09/2021 10:55:17 - INFO - trainer -     U-F1(I) = 0.1818181818181818
10/09/2021 10:55:17 - INFO - trainer -     U-F1(O) = 0.9638009049773757
10/09/2021 10:55:17 - INFO - trainer -     intent_acc = 0.932780847145488
10/09/2021 10:55:17 - INFO - trainer -     loss = 0.5717619981105421
10/09/2021 10:55:17 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/09/2021 10:55:17 - INFO - trainer -     slot_f1 = 0.9895557721765771
10/09/2021 10:55:17 - INFO - trainer -     slot_precision = 0.9869444444444444
10/09/2021 10:55:17 - INFO - trainer -     slot_recall = 0.9921809550404915

10/09/2021 10:55:17 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:55:17 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:55:17 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:55:17 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:55:17 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:55:17 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:55:17 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:55:17 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:55:17 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:55:17 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:55:17 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:55:17 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:55:17 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:55:17 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:55:17 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:55:17 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:55:17 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.28it/s]
Epoch:  90%|       | 18/20 [38:37<04:17, 128.51s/it]10/09/2021 10:55:41 - INFO - trainer -   ***** Running evaluation on dev dataset ***** | 75/918 [00:08<01:40,  8.39it/s]
10/09/2021 10:55:41 - INFO - trainer -     Num examples = 3258
10/09/2021 10:55:41 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.00it/s]
10/09/2021 10:55:45 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:55:45 - INFO - trainer -     T-F1 = 0.9907306434023991 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:55:45 - INFO - trainer -     T-F1(C) = 0.9742120343839542
10/09/2021 10:55:45 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:55:45 - INFO - trainer -     T-F1(O) = 0.9963123644251627
10/09/2021 10:55:45 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:55:45 - INFO - trainer -     T-F1(S) = 0.9935364727608494
10/09/2021 10:55:45 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:55:45 - INFO - trainer -     U-F1(A) = 0.6868686868686869
10/09/2021 10:55:45 - INFO - trainer -     U-F1(E) = 0.7466666666666667
10/09/2021 10:55:45 - INFO - trainer -     U-F1(I) = 0.1818181818181818
10/09/2021 10:55:45 - INFO - trainer -     U-F1(O) = 0.9636458514524264
10/09/2021 10:55:45 - INFO - trainer -     intent_acc = 0.9324739103744628
10/09/2021 10:55:45 - INFO - trainer -     loss = 0.5634503751701
10/09/2021 10:55:45 - INFO - trainer -     semantic_frame_acc = 0.9146715776550031
10/09/2021 10:55:45 - INFO - trainer -     slot_f1 = 0.9905186837702175
10/09/2021 10:55:45 - INFO - trainer -     slot_precision = 0.9891395154553049
10/09/2021 10:55:45 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:55:45 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:55:45 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:55:45 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:55:45 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:55:45 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:55:45 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:55:45 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:55:45 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:55:45 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:55:45 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:55:45 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:55:45 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:55:45 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:55:45 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:55:45 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:55:45 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:55:45 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:56:09 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 275/918 [00:36<01:16,  8.42it/s]
10/09/2021 10:56:09 - INFO - trainer -     Num examples = 3258
10/09/2021 10:56:09 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:56:13 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:56:13 - INFO - trainer -     T-F1 = 0.990182710662667 | 50/51 [00:03<00:00, 13.92it/s]
10/09/2021 10:56:13 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/09/2021 10:56:13 - INFO - trainer -     T-F1(L) = 0.991044776119403
10/09/2021 10:56:13 - INFO - trainer -     T-F1(O) = 0.9960958681271013
10/09/2021 10:56:13 - INFO - trainer -     T-F1(P) = 0.9953243717124488
10/09/2021 10:56:13 - INFO - trainer -     T-F1(S) = 0.9926131117266852
10/09/2021 10:56:13 - INFO - trainer -     T-F1(T) = 0.9747899159663864
10/09/2021 10:56:13 - INFO - trainer -     U-F1(A) = 0.6965174129353233
10/09/2021 10:56:13 - INFO - trainer -     U-F1(E) = 0.7575757575757577
10/09/2021 10:56:13 - INFO - trainer -     U-F1(I) = 0.19047619047619047
10/09/2021 10:56:13 - INFO - trainer -     U-F1(O) = 0.9646649260226283
10/09/2021 10:56:13 - INFO - trainer -     intent_acc = 0.934622467771639
10/09/2021 10:56:13 - INFO - trainer -     loss = 0.5691118989797199
10/09/2021 10:56:13 - INFO - trainer -     semantic_frame_acc = 0.9152854511970534
10/09/2021 10:56:13 - INFO - trainer -     slot_f1 = 0.9899581589958159
10/09/2021 10:56:13 - INFO - trainer -     slot_precision = 0.988854834215659
10/09/2021 10:56:13 - INFO - trainer -     slot_recall = 0.9910639486177045

10/09/2021 10:56:13 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:56:13 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:56:13 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:56:13 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:56:13 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:56:13 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:56:13 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:56:13 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:56:13 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:56:13 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:56:13 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:56:13 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:56:13 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:56:13 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:56:13 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:56:13 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:56:13 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:56:37 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 475/918 [01:05<00:53,  8.31it/s]
10/09/2021 10:56:37 - INFO - trainer -     Num examples = 3258
10/09/2021 10:56:37 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:56:41 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:56:41 - INFO - trainer -     T-F1 = 0.9901853871319519 | 50/51 [00:03<00:00, 13.90it/s]
10/09/2021 10:56:41 - INFO - trainer -     T-F1(C) = 0.9727403156384504
10/09/2021 10:56:41 - INFO - trainer -     T-F1(L) = 0.9940476190476191
10/09/2021 10:56:41 - INFO - trainer -     T-F1(O) = 0.9960954446854664
10/09/2021 10:56:41 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:56:41 - INFO - trainer -     T-F1(S) = 0.9921550530687586
10/09/2021 10:56:41 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:56:41 - INFO - trainer -     U-F1(A) = 0.6767676767676767
10/09/2021 10:56:41 - INFO - trainer -     U-F1(E) = 0.75
10/09/2021 10:56:41 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:56:41 - INFO - trainer -     U-F1(O) = 0.9640062597809077
10/09/2021 10:56:41 - INFO - trainer -     intent_acc = 0.9330877839165131
10/09/2021 10:56:41 - INFO - trainer -     loss = 0.5717384832746842
10/09/2021 10:56:41 - INFO - trainer -     semantic_frame_acc = 0.914364640883978
10/09/2021 10:56:41 - INFO - trainer -     slot_f1 = 0.9899609592861127
10/09/2021 10:56:41 - INFO - trainer -     slot_precision = 0.9885825675299359
10/09/2021 10:56:41 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:56:41 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:56:41 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:56:41 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:56:41 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:56:41 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:56:41 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:56:41 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:56:41 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:56:41 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:56:41 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:56:41 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:56:41 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:56:41 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:56:41 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:56:41 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:56:41 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:56:41 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:57:05 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 675/918 [01:33<00:29,  8.30it/s]
10/09/2021 10:57:05 - INFO - trainer -     Num examples = 3258
10/09/2021 10:57:05 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.98it/s]
10/09/2021 10:57:09 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:57:09 - INFO - trainer -     T-F1 = 0.9887182275383988 | 50/51 [00:03<00:00, 13.80it/s]
10/09/2021 10:57:09 - INFO - trainer -     T-F1(C) = 0.9672830725462305
10/09/2021 10:57:09 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:57:09 - INFO - trainer -     T-F1(O) = 0.9954937835930289
10/09/2021 10:57:09 - INFO - trainer -     T-F1(P) = 0.9956178790534619
10/09/2021 10:57:09 - INFO - trainer -     T-F1(S) = 0.98989898989899
10/09/2021 10:57:09 - INFO - trainer -     T-F1(T) = 0.9720670391061452
10/09/2021 10:57:09 - INFO - trainer -     U-F1(A) = 0.6632124352331606
10/09/2021 10:57:09 - INFO - trainer -     U-F1(E) = 0.744721689059501
10/09/2021 10:57:09 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:57:09 - INFO - trainer -     U-F1(O) = 0.963387124761409
10/09/2021 10:57:09 - INFO - trainer -     intent_acc = 0.9321669736034377
10/09/2021 10:57:09 - INFO - trainer -     loss = 0.5665818815137825
10/09/2021 10:57:09 - INFO - trainer -     semantic_frame_acc = 0.9103744628606507
10/09/2021 10:57:09 - INFO - trainer -     slot_f1 = 0.9884610037536494
10/09/2021 10:57:09 - INFO - trainer -     slot_precision = 0.9842192691029901
10/09/2021 10:57:09 - INFO - trainer -     slot_recall = 0.9927394582518849

10/09/2021 10:57:09 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:57:09 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:57:09 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:57:09 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:57:09 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:57:09 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:57:09 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:57:09 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:57:09 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:57:09 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:57:09 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:57:09 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:57:09 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:57:09 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:57:09 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:57:09 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:57:09 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:57:33 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 875/918 [02:01<00:05,  8.33it/s]
10/09/2021 10:57:33 - INFO - trainer -     Num examples = 3258
10/09/2021 10:57:33 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.98it/s]
10/09/2021 10:57:37 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:57:37 - INFO - trainer -     T-F1 = 0.9899264906071332 | 50/51 [00:03<00:00, 13.92it/s]
10/09/2021 10:57:37 - INFO - trainer -     T-F1(C) = 0.9672830725462305
10/09/2021 10:57:37 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:57:37 - INFO - trainer -     T-F1(O) = 0.995984807379273
10/09/2021 10:57:37 - INFO - trainer -     T-F1(P) = 0.9959088252483926
10/09/2021 10:57:37 - INFO - trainer -     T-F1(S) = 0.9930843706777317
10/09/2021 10:57:37 - INFO - trainer -     T-F1(T) = 0.9734265734265735
10/09/2021 10:57:37 - INFO - trainer -     U-F1(A) = 0.6564102564102565
10/09/2021 10:57:37 - INFO - trainer -     U-F1(E) = 0.7528517110266157
10/09/2021 10:57:37 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/09/2021 10:57:37 - INFO - trainer -     U-F1(O) = 0.9638512339242266
10/09/2021 10:57:37 - INFO - trainer -     intent_acc = 0.932780847145488
10/09/2021 10:57:37 - INFO - trainer -     loss = 0.5641010788579782
10/09/2021 10:57:37 - INFO - trainer -     semantic_frame_acc = 0.9131368937998773
10/09/2021 10:57:37 - INFO - trainer -     slot_f1 = 0.9896964633806739
10/09/2021 10:57:37 - INFO - trainer -     slot_precision = 0.986948069980561
10/09/2021 10:57:37 - INFO - trainer -     slot_recall = 0.9924602066461882

10/09/2021 10:57:37 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:57:37 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:57:37 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:57:37 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:57:37 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:57:37 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:57:37 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:57:37 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:57:37 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:57:37 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:57:37 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:57:37 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:57:37 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:57:37 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:57:37 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:57:37 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:57:37 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:10<00:00,  7.05it/s]
Epoch:  95%|   | 19/20 [40:47<02:09, 129.01s/it]10/09/2021 10:58:01 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 157/918 [00:18<01:31,  8.30it/s]
10/09/2021 10:58:01 - INFO - trainer -     Num examples = 3258
10/09/2021 10:58:01 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.95it/s]
10/09/2021 10:58:05 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:58:05 - INFO - trainer -     T-F1 = 0.989510965808473 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:58:05 - INFO - trainer -     T-F1(C) = 0.9672830725462305
10/09/2021 10:58:05 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:58:05 - INFO - trainer -     T-F1(O) = 0.9958231624627067
10/09/2021 10:58:05 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:58:05 - INFO - trainer -     T-F1(S) = 0.9926131117266852
10/09/2021 10:58:05 - INFO - trainer -     T-F1(T) = 0.9719887955182073
10/09/2021 10:58:05 - INFO - trainer -     U-F1(A) = 0.6455026455026455
10/09/2021 10:58:05 - INFO - trainer -     U-F1(E) = 0.7504761904761904
10/09/2021 10:58:05 - INFO - trainer -     U-F1(I) = 0.15384615384615383
10/09/2021 10:58:05 - INFO - trainer -     U-F1(O) = 0.9630400832899532
10/09/2021 10:58:05 - INFO - trainer -     intent_acc = 0.9318600368324125
10/09/2021 10:58:05 - INFO - trainer -     loss = 0.5663188171445155
10/09/2021 10:58:05 - INFO - trainer -     semantic_frame_acc = 0.9112952731737262
10/09/2021 10:58:05 - INFO - trainer -     slot_f1 = 0.9892712832659887
10/09/2021 10:58:05 - INFO - trainer -     slot_precision = 0.9872080088987765
10/09/2021 10:58:05 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:58:05 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:58:05 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:58:05 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:58:05 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:58:05 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:58:05 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:58:05 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:58:05 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:58:05 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:58:05 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:58:05 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:58:05 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:58:05 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:58:05 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:58:05 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:58:05 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:58:05 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:58:29 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 357/918 [00:47<01:07,  8.30it/s]
10/09/2021 10:58:29 - INFO - trainer -     Num examples = 3258
10/09/2021 10:58:29 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 14.00it/s]
10/09/2021 10:58:33 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:58:33 - INFO - trainer -     T-F1 = 0.9896514161220045 | 50/51 [00:03<00:00, 13.90it/s]
10/09/2021 10:58:33 - INFO - trainer -     T-F1(C) = 0.9672830725462305
10/09/2021 10:58:33 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:58:33 - INFO - trainer -     T-F1(O) = 0.9958767361111112
10/09/2021 10:58:33 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:58:33 - INFO - trainer -     T-F1(S) = 0.9930843706777317
10/09/2021 10:58:33 - INFO - trainer -     T-F1(T) = 0.9719887955182073
10/09/2021 10:58:33 - INFO - trainer -     U-F1(A) = 0.663265306122449
10/09/2021 10:58:33 - INFO - trainer -     U-F1(E) = 0.7547169811320754
10/09/2021 10:58:33 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/09/2021 10:58:33 - INFO - trainer -     U-F1(O) = 0.9639937380413985
10/09/2021 10:58:33 - INFO - trainer -     intent_acc = 0.9330877839165131
10/09/2021 10:58:33 - INFO - trainer -     loss = 0.5645396893515306
10/09/2021 10:58:33 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/09/2021 10:58:33 - INFO - trainer -     slot_f1 = 0.9894150417827299
10/09/2021 10:58:33 - INFO - trainer -     slot_precision = 0.9869408168935816
10/09/2021 10:58:33 - INFO - trainer -     slot_recall = 0.9919017034347948

10/09/2021 10:58:33 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:58:33 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:58:33 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:58:33 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:58:33 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:58:33 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:58:33 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:58:33 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:58:33 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:58:33 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:58:33 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:58:33 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:58:33 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:58:33 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:58:33 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:58:33 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:58:33 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:58:57 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 557/918 [01:15<00:43,  8.35it/s]
10/09/2021 10:58:57 - INFO - trainer -     Num examples = 3258
10/09/2021 10:58:57 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.96it/s]
10/09/2021 10:59:02 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:59:02 - INFO - trainer -     T-F1 = 0.9896485971125034 | 50/51 [00:03<00:00, 13.82it/s]
10/09/2021 10:59:02 - INFO - trainer -     T-F1(C) = 0.9686609686609687
10/09/2021 10:59:02 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:59:02 - INFO - trainer -     T-F1(O) = 0.9958771834653358
10/09/2021 10:59:02 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:59:02 - INFO - trainer -     T-F1(S) = 0.9926199261992621
10/09/2021 10:59:02 - INFO - trainer -     T-F1(T) = 0.9719887955182073
10/09/2021 10:59:02 - INFO - trainer -     U-F1(A) = 0.663265306122449
10/09/2021 10:59:02 - INFO - trainer -     U-F1(E) = 0.7575757575757577
10/09/2021 10:59:02 - INFO - trainer -     U-F1(I) = 0.1951219512195122
10/09/2021 10:59:02 - INFO - trainer -     U-F1(O) = 0.9643540253868892
10/09/2021 10:59:02 - INFO - trainer -     intent_acc = 0.9337016574585635
10/09/2021 10:59:02 - INFO - trainer -     loss = 0.5654915785234348
10/09/2021 10:59:02 - INFO - trainer -     semantic_frame_acc = 0.9134438305709024
10/09/2021 10:59:02 - INFO - trainer -     slot_f1 = 0.989412092504876
10/09/2021 10:59:02 - INFO - trainer -     slot_precision = 0.9872115651932166
10/09/2021 10:59:02 - INFO - trainer -     slot_recall = 0.991622451829098

10/09/2021 10:59:02 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:59:02 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:59:02 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:59:02 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:59:02 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:59:02 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:59:02 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:59:02 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:59:02 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:59:02 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:59:02 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:59:02 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:59:02 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:59:02 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:59:02 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:59:02 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:59:02 - INFO - trainer -     slot_recall = 0.9907846970120078
                                                                                                                       10/09/2021 10:59:26 - INFO - trainer -   ***** Running evaluation on dev dataset *****| 757/918 [01:43<00:19,  8.36it/s]
10/09/2021 10:59:26 - INFO - trainer -     Num examples = 3258
10/09/2021 10:59:26 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 51/51 [00:03<00:00, 13.99it/s]
10/09/2021 10:59:30 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:59:30 - INFO - trainer -     T-F1 = 0.9893761917733588 | 50/51 [00:03<00:00, 13.88it/s]
10/09/2021 10:59:30 - INFO - trainer -     T-F1(C) = 0.9672830725462305
10/09/2021 10:59:30 - INFO - trainer -     T-F1(L) = 0.9910979228486648
10/09/2021 10:59:30 - INFO - trainer -     T-F1(O) = 0.995768688293371
10/09/2021 10:59:30 - INFO - trainer -     T-F1(P) = 0.9956153171587255
10/09/2021 10:59:30 - INFO - trainer -     T-F1(S) = 0.9921550530687586
10/09/2021 10:59:30 - INFO - trainer -     T-F1(T) = 0.9719887955182073
10/09/2021 10:59:30 - INFO - trainer -     U-F1(A) = 0.663265306122449
10/09/2021 10:59:30 - INFO - trainer -     U-F1(E) = 0.7528517110266157
10/09/2021 10:59:30 - INFO - trainer -     U-F1(I) = 0.2
10/09/2021 10:59:30 - INFO - trainer -     U-F1(O) = 0.9641988182134168
10/09/2021 10:59:30 - INFO - trainer -     intent_acc = 0.9333947206875384
10/09/2021 10:59:30 - INFO - trainer -     loss = 0.5660604634413532
10/09/2021 10:59:30 - INFO - trainer -     semantic_frame_acc = 0.9128299570288521
10/09/2021 10:59:30 - INFO - trainer -     slot_f1 = 0.9891334633602674
10/09/2021 10:59:30 - INFO - trainer -     slot_precision = 0.9869335557408951
10/09/2021 10:59:30 - INFO - trainer -     slot_recall = 0.9913432002234013

10/09/2021 10:59:30 - INFO - trainer -   ***** Current best eval results based on U-F1(I) *****
10/09/2021 10:59:30 - INFO - trainer -     T-F1 = 0.987892803700177
10/09/2021 10:59:30 - INFO - trainer -     T-F1(C) = 0.9644381223328592
10/09/2021 10:59:30 - INFO - trainer -     T-F1(L) = 0.9851632047477745
10/09/2021 10:59:30 - INFO - trainer -     T-F1(O) = 0.99516960651289
10/09/2021 10:59:30 - INFO - trainer -     T-F1(P) = 0.9967789165446559
10/09/2021 10:59:30 - INFO - trainer -     T-F1(S) = 0.9871794871794872
10/09/2021 10:59:30 - INFO - trainer -     T-F1(T) = 0.9719101123595506
10/09/2021 10:59:30 - INFO - trainer -     U-F1(A) = 0.6440677966101696
10/09/2021 10:59:30 - INFO - trainer -     U-F1(E) = 0.7673179396092362
10/09/2021 10:59:30 - INFO - trainer -     U-F1(I) = 0.3548387096774193
10/09/2021 10:59:30 - INFO - trainer -     U-F1(O) = 0.9614980749037453
10/09/2021 10:59:30 - INFO - trainer -     intent_acc = 0.9303253529772867
10/09/2021 10:59:30 - INFO - trainer -     semantic_frame_acc = 0.9069981583793738
10/09/2021 10:59:30 - INFO - trainer -     slot_f1 = 0.9873382496173647
10/09/2021 10:59:30 - INFO - trainer -     slot_precision = 0.9839156960621187
10/09/2021 10:59:30 - INFO - trainer -     slot_recall = 0.9907846970120078
Iteration: 100%|| 918/918 [02:06<00:00,  7.26it/s]
Epoch: 100%|| 20/20 [42:54<00:00, 128.71s/it]
10/09/2021 10:59:49 - INFO - transformers.configuration_utils -   loading configuration file final_low_distilbert_de_model\config.json
10/09/2021 10:59:49 - INFO - transformers.configuration_utils -   Model config DistilBertConfig {
  "activation": "gelu",
  "architectures": [
    "JointDistilBERT"
  ],
  "attention_dropout": 0.1,
  "dim": 768,
  "dropout": 0.1,
  "finetuning_task": "low",
  "hidden_dim": 3072,
  "initializer_range": 0.02,
  "max_position_embeddings": 512,
  "model_type": "distilbert",
  "n_heads": 12,
  "n_layers": 6,
  "pad_token_id": 0,
  "qa_dropout": 0.1,
  "seq_classif_dropout": 0.2,
  "sinusoidal_pos_embds": false,
  "tie_weights_": true,
  "vocab_size": 30522
}

10/09/2021 10:59:49 - INFO - transformers.modeling_utils -   loading weights file final_low_distilbert_de_model\pytorch_model.bin
10/09/2021 10:59:49 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing JointDistilBERT.

10/09/2021 10:59:49 - INFO - transformers.modeling_utils -   All the weights of JointDistilBERT were initialized from the model checkpoint at final_low_distilbert_de_model.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use JointDistilBERT for predictions without further training.
10/09/2021 10:59:50 - INFO - trainer -   ***** Model Loaded *****
10/09/2021 10:59:50 - INFO - trainer -   ***** Running evaluation on test dataset *****
10/09/2021 10:59:50 - INFO - trainer -     Num examples = 3628
10/09/2021 10:59:50 - INFO - trainer -     Batch size = 64
Evaluating: 100%|| 57/57 [00:04<00:00, 14.00it/s]
10/09/2021 10:59:54 - INFO - trainer -   ***** Eval results *****
10/09/2021 10:59:54 - INFO - trainer -     T-F1 = 0.9850270004909181
10/09/2021 10:59:54 - INFO - trainer -     T-F1(C) = 0.9559228650137741
10/09/2021 10:59:54 - INFO - trainer -     T-F1(L) = 0.972972972972973
10/09/2021 10:59:54 - INFO - trainer -     T-F1(O) = 0.99418773612322
10/09/2021 10:59:54 - INFO - trainer -     T-F1(P) = 0.9955509029049987
10/09/2021 10:59:54 - INFO - trainer -     T-F1(S) = 0.9838509316770186
10/09/2021 10:59:54 - INFO - trainer -     T-F1(T) = 0.9703504043126685
10/09/2021 10:59:54 - INFO - trainer -     U-F1(A) = 0.611764705882353
10/09/2021 10:59:54 - INFO - trainer -     U-F1(E) = 0.7431693989071039
10/09/2021 10:59:54 - INFO - trainer -     U-F1(I) = 0.380952380952381
10/09/2021 10:59:54 - INFO - trainer -     U-F1(O) = 0.9650911337658326
10/09/2021 10:59:54 - INFO - trainer -     intent_acc = 0.9349503858875413
10/09/2021 10:59:54 - INFO - trainer -     loss = 0.23206980787871176
10/09/2021 10:59:54 - INFO - trainer -     semantic_frame_acc = 0.906284454244763
10/09/2021 10:59:54 - INFO - trainer -     slot_f1 = 0.9842311088684242
10/09/2021 10:59:54 - INFO - trainer -     slot_precision = 0.9784298971657888
10/09/2021 10:59:54 - INFO - trainer -     slot_recall = 0.9901015228426396
